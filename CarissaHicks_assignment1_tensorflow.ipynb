{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "5. Use Tensorflow for automatic differentiation"
      ],
      "metadata": {
        "id": "fY0oXIgQ24kg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv3OkkhO2l28"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter initialization\n",
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set tensorflow global random seed\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    #parameters defined as tensorflow variables instead of numpy arrays\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer. Note that weights and biases are defined as tensorflow variables instead of numpy arrays\n",
        "    W1=tf.Variable(tf.random.uniform(shape=(nh,nx), minval=-0.01, maxval=0.01), name=\"W1\")\n",
        "    b1=tf.Variable(tf.zeros(shape=(nh,1),name=\"b1\" ))\n",
        "    W2=tf.Variable(tf.random.uniform(shape=(ny,nh), minval=-0.01, maxval=0.01), name=\"W2\")\n",
        "    b2=tf.Variable(tf.zeros(shape=(ny,1), name=\"b2\"))\n",
        "   \n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "zGgqsoTS3H7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(parameters,X):\n",
        "    #the input is read as an integer, use tf.cast to cast it to float before using it in fowrard pass computation.\n",
        "    X= tf.cast(X, tf.float32)  \n",
        "    Z1= tf.matmul(parameters[\"W1\"],X)+ parameters[\"b1\"] # b1 is broadcasted n times before it is added to \n",
        "    A1=tf.nn.relu(Z1)\n",
        "    Z2=tf.matmul(parameters[\"W2\"],A1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "    \n",
        "    Yhat = Z2 #remove sigmoid activation since this is a regression problem\n",
        "       \n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "XpXYafhe3KXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(Y, Yhat):\n",
        "  per_sample_losses = (Y - Yhat)**2\n",
        "  loss = tf.reduce_mean(per_sample_losses)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "FW6JirHX3sZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients= tape.gradient(loss,parameters)\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "Qkh5mmXGAH5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"].assign_sub(learning_rate*gradients[\"W1\"])\n",
        "    parameters[\"W2\"].assign_sub(learning_rate*gradients[\"W2\"])\n",
        "    parameters[\"b1\"].assign_sub(learning_rate*gradients[\"b1\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate*gradients[\"b2\"])\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "CTanplBGBC3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\" \n",
        "    \n",
        "    \n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "    \n",
        "    # We want to use this network for binary classification, so we have only one neuron in the output layer with a sigmoid activation\n",
        "    ny=1\n",
        "    \n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "    \n",
        "    \n",
        "    #initialize lists to store the training and valideation losses. \n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "    \n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "      \n",
        "      \"\"\"\n",
        "        run forward pass and compute the loss function on training and validation data. \n",
        "        Note that the forward pass and loss computations on the training data are enclosed inside the gradient tape context in order to build the computational graph.\n",
        "        The gradients are only computed on the training data and used to update the parameter. Validation data is not used for training and updating the parameters.\n",
        "        \"\"\"\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "        #run the forward pass on train_X\n",
        "        train_Yhat=forward_pass(parameters,train_X)\n",
        "        #compute the train_loss\n",
        "        train_loss=compute_loss(train_Y,train_Yhat)\n",
        "\n",
        "\n",
        "       #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "      \n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "       # append the train and validation loss for the current iteration to the train_losses and val_losses \n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "     \n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"    \n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "    \n",
        "    \n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "        \n",
        "        \n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n",
        "        "
      ],
      "metadata": {
        "id": "JZqT0zgSBiDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get predictions\n",
        "def predict(parameters,X):\n",
        "    Yhat=forward_pass(parameters, X)\n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "iuxwc-iGBxyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to calculate the MAPE\n",
        "def MAPE(observedY, predictedY):\n",
        "    per_sample_loss = abs(observedY - predictedY)/observedY\n",
        "    mape = tf.reduce_mean(per_sample_loss)\n",
        "    return float(mape)"
      ],
      "metadata": {
        "id": "JiZ33ocOCCIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "df = pd.read_csv(\"sample_data/california_housing_train.csv\")"
      ],
      "metadata": {
        "id": "pN23nQtMCuGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split training data into 80% training and 20% validation\n",
        "train = df.sample(frac=0.8, random_state=123)\n",
        "val = df.drop(train.index)"
      ],
      "metadata": {
        "id": "u3FaxvMvC2a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the input datasets train.csv and validation.csv and store them into numpy arrays\n",
        "train = train.to_numpy()\n",
        "test = pd.read_csv('sample_data/california_housing_test.csv').to_numpy()\n",
        "val = val.to_numpy()"
      ],
      "metadata": {
        "id": "8C2pyRYGC4Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separate the features from the target variable (median_house_value) in train, val and test\n",
        "train_X = train[:,:-1]\n",
        "train_Y = train[...,-1] #labels\n",
        "\n",
        "test_X = test[:,:-1]\n",
        "test_Y = test[...,-1] #labels\n",
        "\n",
        "val_X = val[:,:-1]\n",
        "val_Y = val[...,-1] #labels"
      ],
      "metadata": {
        "id": "_EyA9gxIC6-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "print(val_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_Y.shape)\n",
        "print(val_Y.shape)\n",
        "print(test_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw8B_cfMC7lF",
        "outputId": "fcf5f022-0ba8-4811-b96e-f3021a1673dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13600, 8)\n",
            "(3000, 8)\n",
            "(3400, 8)\n",
            "(13600,)\n",
            "(3000,)\n",
            "(3400,)\n",
            "[344700. 176500. 270500. ...  62000. 162500. 500001.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the data: subtract mean of each feature and divide by the std, so that the feature is centered around 0 and has a unit std\n",
        "train_norm = (train_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "test_norm = (test_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "val_norm = (val_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "\n",
        "print(train_norm.shape)\n",
        "print(test_norm.shape)\n",
        "print(val_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6RDGd7YC_KR",
        "outputId": "b2d94aab-4ff9-4862-e1f6-c31dda397b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13600, 8)\n",
            "(3000, 8)\n",
            "(3400, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#divide the median_house_values by 100k to scale them down\n",
        "train_Y = train_Y/100000\n",
        "test_Y = test_Y/100000\n",
        "val_Y = val_Y/100000"
      ],
      "metadata": {
        "id": "nI0XzzUgDBWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transpose feature matricies for train,test,val and reshape target vectors to 2D arrays\n",
        "train_norm = train_norm.transpose()\n",
        "test_norm = test_norm.transpose()\n",
        "val_norm = val_norm.transpose()"
      ],
      "metadata": {
        "id": "iEy227ZwDB-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y=np.reshape(train_Y, (1, train_Y.size))\n",
        "test_Y=np.reshape(test_Y, (1, test_Y.size))\n",
        "val_Y=np.reshape(val_Y, (1, val_Y.size))"
      ],
      "metadata": {
        "id": "AYrrGcafDE3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_norm.shape)\n",
        "print(train_Y.shape)\n",
        "print(val_norm.shape)\n",
        "print(val_Y.shape)\n",
        "print(test_norm.shape)\n",
        "print(test_Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "triBJshZDGYz",
        "outputId": "f3c6d2c8-03f1-40e4-845d-9a3ef2c440ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 13600)\n",
            "(1, 13600)\n",
            "(8, 3400)\n",
            "(1, 3400)\n",
            "(8, 3000)\n",
            "(1, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations= 2000\n",
        "parameters, history=create_nn_model(train_norm,train_Y,128, val_norm, val_Y, iterations, 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV07f_ddDJSf",
        "outputId": "0faf1c95-ea5c-4d9a-f721-08bf50b5a7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.630237102508545 val_loss5.691183567047119\n",
            "iteration 1 :train_loss:5.61295747756958 val_loss5.673861503601074\n",
            "iteration 2 :train_loss:5.595747470855713 val_loss5.656608581542969\n",
            "iteration 3 :train_loss:5.578606605529785 val_loss5.639425754547119\n",
            "iteration 4 :train_loss:5.561534881591797 val_loss5.622311115264893\n",
            "iteration 5 :train_loss:5.544531345367432 val_loss5.6052656173706055\n",
            "iteration 6 :train_loss:5.527596473693848 val_loss5.588288307189941\n",
            "iteration 7 :train_loss:5.51072883605957 val_loss5.571379661560059\n",
            "iteration 8 :train_loss:5.493929862976074 val_loss5.554538249969482\n",
            "iteration 9 :train_loss:5.477199077606201 val_loss5.5377655029296875\n",
            "iteration 10 :train_loss:5.46053409576416 val_loss5.521059513092041\n",
            "iteration 11 :train_loss:5.443937301635742 val_loss5.504420280456543\n",
            "iteration 12 :train_loss:5.427405834197998 val_loss5.487848281860352\n",
            "iteration 13 :train_loss:5.410942077636719 val_loss5.47134256362915\n",
            "iteration 14 :train_loss:5.3945441246032715 val_loss5.4549031257629395\n",
            "iteration 15 :train_loss:5.378211975097656 val_loss5.4385294914245605\n",
            "iteration 16 :train_loss:5.361945152282715 val_loss5.422222137451172\n",
            "iteration 17 :train_loss:5.345744609832764 val_loss5.405978679656982\n",
            "iteration 18 :train_loss:5.32960844039917 val_loss5.3898024559021\n",
            "iteration 19 :train_loss:5.313536167144775 val_loss5.373690128326416\n",
            "iteration 20 :train_loss:5.297529697418213 val_loss5.357642650604248\n",
            "iteration 21 :train_loss:5.281587600708008 val_loss5.341659069061279\n",
            "iteration 22 :train_loss:5.265708923339844 val_loss5.325739860534668\n",
            "iteration 23 :train_loss:5.249894142150879 val_loss5.309885025024414\n",
            "iteration 24 :train_loss:5.234142780303955 val_loss5.294093132019043\n",
            "iteration 25 :train_loss:5.218454837799072 val_loss5.278365135192871\n",
            "iteration 26 :train_loss:5.202829360961914 val_loss5.262698650360107\n",
            "iteration 27 :train_loss:5.187267303466797 val_loss5.247096538543701\n",
            "iteration 28 :train_loss:5.171767711639404 val_loss5.2315568923950195\n",
            "iteration 29 :train_loss:5.15632963180542 val_loss5.216078758239746\n",
            "iteration 30 :train_loss:5.14095401763916 val_loss5.200663089752197\n",
            "iteration 31 :train_loss:5.125639915466309 val_loss5.185308933258057\n",
            "iteration 32 :train_loss:5.110387325286865 val_loss5.170017242431641\n",
            "iteration 33 :train_loss:5.09519624710083 val_loss5.154785633087158\n",
            "iteration 34 :train_loss:5.0800652503967285 val_loss5.139615058898926\n",
            "iteration 35 :train_loss:5.064995288848877 val_loss5.124505996704102\n",
            "iteration 36 :train_loss:5.049986362457275 val_loss5.109457492828369\n",
            "iteration 37 :train_loss:5.035036563873291 val_loss5.09446907043457\n",
            "iteration 38 :train_loss:5.020148277282715 val_loss5.079540252685547\n",
            "iteration 39 :train_loss:5.0053181648254395 val_loss5.064671993255615\n",
            "iteration 40 :train_loss:4.990548610687256 val_loss5.049861907958984\n",
            "iteration 41 :train_loss:4.9758381843566895 val_loss5.035112380981445\n",
            "iteration 42 :train_loss:4.961186408996582 val_loss5.020421981811523\n",
            "iteration 43 :train_loss:4.946592807769775 val_loss5.0057902336120605\n",
            "iteration 44 :train_loss:4.932058811187744 val_loss4.991216659545898\n",
            "iteration 45 :train_loss:4.917582035064697 val_loss4.976701736450195\n",
            "iteration 46 :train_loss:4.903163909912109 val_loss4.962244987487793\n",
            "iteration 47 :train_loss:4.888803482055664 val_loss4.947845935821533\n",
            "iteration 48 :train_loss:4.874500274658203 val_loss4.933504104614258\n",
            "iteration 49 :train_loss:4.860255241394043 val_loss4.919219970703125\n",
            "iteration 50 :train_loss:4.846065998077393 val_loss4.904993057250977\n",
            "iteration 51 :train_loss:4.831934928894043 val_loss4.8908233642578125\n",
            "iteration 52 :train_loss:4.817859172821045 val_loss4.876709461212158\n",
            "iteration 53 :train_loss:4.803840637207031 val_loss4.862652778625488\n",
            "iteration 54 :train_loss:4.7898783683776855 val_loss4.848652362823486\n",
            "iteration 55 :train_loss:4.77597188949585 val_loss4.834708213806152\n",
            "iteration 56 :train_loss:4.762120246887207 val_loss4.82081937789917\n",
            "iteration 57 :train_loss:4.748325347900391 val_loss4.806985855102539\n",
            "iteration 58 :train_loss:4.734584808349609 val_loss4.793208122253418\n",
            "iteration 59 :train_loss:4.72089958190918 val_loss4.77948522567749\n",
            "iteration 60 :train_loss:4.707269668579102 val_loss4.765817642211914\n",
            "iteration 61 :train_loss:4.6936936378479 val_loss4.752203941345215\n",
            "iteration 62 :train_loss:4.680172443389893 val_loss4.738645076751709\n",
            "iteration 63 :train_loss:4.666705131530762 val_loss4.72514009475708\n",
            "iteration 64 :train_loss:4.65329122543335 val_loss4.7116899490356445\n",
            "iteration 65 :train_loss:4.639931678771973 val_loss4.698293209075928\n",
            "iteration 66 :train_loss:4.6266255378723145 val_loss4.68494987487793\n",
            "iteration 67 :train_loss:4.613372802734375 val_loss4.671659469604492\n",
            "iteration 68 :train_loss:4.600172519683838 val_loss4.658422470092773\n",
            "iteration 69 :train_loss:4.587025165557861 val_loss4.645238876342773\n",
            "iteration 70 :train_loss:4.573930740356445 val_loss4.632107734680176\n",
            "iteration 71 :train_loss:4.560888767242432 val_loss4.619028568267822\n",
            "iteration 72 :train_loss:4.547898769378662 val_loss4.606001377105713\n",
            "iteration 73 :train_loss:4.534960746765137 val_loss4.593027114868164\n",
            "iteration 74 :train_loss:4.522074222564697 val_loss4.580104351043701\n",
            "iteration 75 :train_loss:4.509239673614502 val_loss4.567233562469482\n",
            "iteration 76 :train_loss:4.496456623077393 val_loss4.554413795471191\n",
            "iteration 77 :train_loss:4.483724594116211 val_loss4.541645050048828\n",
            "iteration 78 :train_loss:4.471043109893799 val_loss4.528928279876709\n",
            "iteration 79 :train_loss:4.4584126472473145 val_loss4.516261100769043\n",
            "iteration 80 :train_loss:4.4458327293396 val_loss4.503644943237305\n",
            "iteration 81 :train_loss:4.433302402496338 val_loss4.491078853607178\n",
            "iteration 82 :train_loss:4.420823097229004 val_loss4.4785637855529785\n",
            "iteration 83 :train_loss:4.408393383026123 val_loss4.466097831726074\n",
            "iteration 84 :train_loss:4.3960137367248535 val_loss4.453681468963623\n",
            "iteration 85 :train_loss:4.383683204650879 val_loss4.4413161277771\n",
            "iteration 86 :train_loss:4.371401786804199 val_loss4.428999423980713\n",
            "iteration 87 :train_loss:4.3591694831848145 val_loss4.416730880737305\n",
            "iteration 88 :train_loss:4.346986293792725 val_loss4.404512405395508\n",
            "iteration 89 :train_loss:4.3348517417907715 val_loss4.392342567443848\n",
            "iteration 90 :train_loss:4.322765350341797 val_loss4.380220890045166\n",
            "iteration 91 :train_loss:4.310727596282959 val_loss4.368147850036621\n",
            "iteration 92 :train_loss:4.298738479614258 val_loss4.356122970581055\n",
            "iteration 93 :train_loss:4.2867960929870605 val_loss4.344146251678467\n",
            "iteration 94 :train_loss:4.274902820587158 val_loss4.332216739654541\n",
            "iteration 95 :train_loss:4.26305627822876 val_loss4.320335388183594\n",
            "iteration 96 :train_loss:4.251256942749023 val_loss4.308501243591309\n",
            "iteration 97 :train_loss:4.239504814147949 val_loss4.2967143058776855\n",
            "iteration 98 :train_loss:4.227799892425537 val_loss4.284974098205566\n",
            "iteration 99 :train_loss:4.216141700744629 val_loss4.273281097412109\n",
            "iteration 100 :train_loss:4.204530239105225 val_loss4.261634826660156\n",
            "iteration 101 :train_loss:4.192964553833008 val_loss4.250034332275391\n",
            "iteration 102 :train_loss:4.181445121765137 val_loss4.238480567932129\n",
            "iteration 103 :train_loss:4.1699724197387695 val_loss4.226973056793213\n",
            "iteration 104 :train_loss:4.158545017242432 val_loss4.215511322021484\n",
            "iteration 105 :train_loss:4.147163391113281 val_loss4.204095363616943\n",
            "iteration 106 :train_loss:4.13582706451416 val_loss4.192724704742432\n",
            "iteration 107 :train_loss:4.124536037445068 val_loss4.181398868560791\n",
            "iteration 108 :train_loss:4.113289833068848 val_loss4.170118808746338\n",
            "iteration 109 :train_loss:4.102088928222656 val_loss4.158884048461914\n",
            "iteration 110 :train_loss:4.090932846069336 val_loss4.147693157196045\n",
            "iteration 111 :train_loss:4.07982063293457 val_loss4.136547565460205\n",
            "iteration 112 :train_loss:4.068753242492676 val_loss4.12544584274292\n",
            "iteration 113 :train_loss:4.057730197906494 val_loss4.114388942718506\n",
            "iteration 114 :train_loss:4.046751022338867 val_loss4.1033759117126465\n",
            "iteration 115 :train_loss:4.035815715789795 val_loss4.092406749725342\n",
            "iteration 116 :train_loss:4.024923801422119 val_loss4.081480979919434\n",
            "iteration 117 :train_loss:4.01407527923584 val_loss4.070599555969238\n",
            "iteration 118 :train_loss:4.003270149230957 val_loss4.059760570526123\n",
            "iteration 119 :train_loss:3.9925084114074707 val_loss4.0489654541015625\n",
            "iteration 120 :train_loss:3.9817895889282227 val_loss4.038212776184082\n",
            "iteration 121 :train_loss:3.9711132049560547 val_loss4.02750301361084\n",
            "iteration 122 :train_loss:3.960479736328125 val_loss4.016836166381836\n",
            "iteration 123 :train_loss:3.949888229370117 val_loss4.006211280822754\n",
            "iteration 124 :train_loss:3.9393393993377686 val_loss3.99562931060791\n",
            "iteration 125 :train_loss:3.928832769393921 val_loss3.9850895404815674\n",
            "iteration 126 :train_loss:3.918367624282837 val_loss3.974591016769409\n",
            "iteration 127 :train_loss:3.9079439640045166 val_loss3.964134931564331\n",
            "iteration 128 :train_loss:3.8975625038146973 val_loss3.9537200927734375\n",
            "iteration 129 :train_loss:3.8872222900390625 val_loss3.9433467388153076\n",
            "iteration 130 :train_loss:3.8769233226776123 val_loss3.9330153465270996\n",
            "iteration 131 :train_loss:3.8666653633117676 val_loss3.9227240085601807\n",
            "iteration 132 :train_loss:3.856448173522949 val_loss3.9124746322631836\n",
            "iteration 133 :train_loss:3.8462722301483154 val_loss3.9022655487060547\n",
            "iteration 134 :train_loss:3.8361363410949707 val_loss3.892096996307373\n",
            "iteration 135 :train_loss:3.8260409832000732 val_loss3.8819692134857178\n",
            "iteration 136 :train_loss:3.815986394882202 val_loss3.8718812465667725\n",
            "iteration 137 :train_loss:3.8059709072113037 val_loss3.8618338108062744\n",
            "iteration 138 :train_loss:3.7959954738616943 val_loss3.8518266677856445\n",
            "iteration 139 :train_loss:3.786060333251953 val_loss3.8418588638305664\n",
            "iteration 140 :train_loss:3.7761647701263428 val_loss3.8319311141967773\n",
            "iteration 141 :train_loss:3.766308307647705 val_loss3.822042226791382\n",
            "iteration 142 :train_loss:3.756491184234619 val_loss3.8121931552886963\n",
            "iteration 143 :train_loss:3.746713638305664 val_loss3.8023831844329834\n",
            "iteration 144 :train_loss:3.7369747161865234 val_loss3.7926125526428223\n",
            "iteration 145 :train_loss:3.7272748947143555 val_loss3.7828800678253174\n",
            "iteration 146 :train_loss:3.7176132202148438 val_loss3.773186683654785\n",
            "iteration 147 :train_loss:3.7079904079437256 val_loss3.763531446456909\n",
            "iteration 148 :train_loss:3.6984052658081055 val_loss3.753915548324585\n",
            "iteration 149 :train_loss:3.688859224319458 val_loss3.7443370819091797\n",
            "iteration 150 :train_loss:3.6793506145477295 val_loss3.7347967624664307\n",
            "iteration 151 :train_loss:3.6698801517486572 val_loss3.725294589996338\n",
            "iteration 152 :train_loss:3.660447120666504 val_loss3.715830087661743\n",
            "iteration 153 :train_loss:3.6510517597198486 val_loss3.7064030170440674\n",
            "iteration 154 :train_loss:3.6416940689086914 val_loss3.6970136165618896\n",
            "iteration 155 :train_loss:3.632373094558716 val_loss3.687661647796631\n",
            "iteration 156 :train_loss:3.62308931350708 val_loss3.6783463954925537\n",
            "iteration 157 :train_loss:3.6138429641723633 val_loss3.6690688133239746\n",
            "iteration 158 :train_loss:3.6046335697174072 val_loss3.659827709197998\n",
            "iteration 159 :train_loss:3.5954604148864746 val_loss3.650623321533203\n",
            "iteration 160 :train_loss:3.5863237380981445 val_loss3.64145565032959\n",
            "iteration 161 :train_loss:3.577223777770996 val_loss3.63232421875\n",
            "iteration 162 :train_loss:3.568159580230713 val_loss3.6232292652130127\n",
            "iteration 163 :train_loss:3.559131622314453 val_loss3.6141703128814697\n",
            "iteration 164 :train_loss:3.5501396656036377 val_loss3.605147123336792\n",
            "iteration 165 :train_loss:3.5411834716796875 val_loss3.596160411834717\n",
            "iteration 166 :train_loss:3.5322628021240234 val_loss3.5872087478637695\n",
            "iteration 167 :train_loss:3.5233776569366455 val_loss3.5782928466796875\n",
            "iteration 168 :train_loss:3.5145280361175537 val_loss3.5694122314453125\n",
            "iteration 169 :train_loss:3.50571346282959 val_loss3.560567617416382\n",
            "iteration 170 :train_loss:3.496934175491333 val_loss3.551757335662842\n",
            "iteration 171 :train_loss:3.488189220428467 val_loss3.5429818630218506\n",
            "iteration 172 :train_loss:3.479480028152466 val_loss3.5342416763305664\n",
            "iteration 173 :train_loss:3.4708046913146973 val_loss3.525536298751831\n",
            "iteration 174 :train_loss:3.4621646404266357 val_loss3.5168652534484863\n",
            "iteration 175 :train_loss:3.4535582065582275 val_loss3.5082287788391113\n",
            "iteration 176 :train_loss:3.44498610496521 val_loss3.499626636505127\n",
            "iteration 177 :train_loss:3.436448097229004 val_loss3.491058111190796\n",
            "iteration 178 :train_loss:3.4279446601867676 val_loss3.4825239181518555\n",
            "iteration 179 :train_loss:3.4194743633270264 val_loss3.4740233421325684\n",
            "iteration 180 :train_loss:3.4110374450683594 val_loss3.4655568599700928\n",
            "iteration 181 :train_loss:3.402634382247925 val_loss3.4571237564086914\n",
            "iteration 182 :train_loss:3.3942651748657227 val_loss3.4487240314483643\n",
            "iteration 183 :train_loss:3.3859288692474365 val_loss3.4403584003448486\n",
            "iteration 184 :train_loss:3.3776257038116455 val_loss3.4320249557495117\n",
            "iteration 185 :train_loss:3.3693556785583496 val_loss3.42372465133667\n",
            "iteration 186 :train_loss:3.3611185550689697 val_loss3.4154574871063232\n",
            "iteration 187 :train_loss:3.3529136180877686 val_loss3.4072234630584717\n",
            "iteration 188 :train_loss:3.3447420597076416 val_loss3.399021625518799\n",
            "iteration 189 :train_loss:3.3366024494171143 val_loss3.390852451324463\n",
            "iteration 190 :train_loss:3.328495502471924 val_loss3.382715940475464\n",
            "iteration 191 :train_loss:3.320420503616333 val_loss3.3746113777160645\n",
            "iteration 192 :train_loss:3.312377691268921 val_loss3.3665390014648438\n",
            "iteration 193 :train_loss:3.3043670654296875 val_loss3.3584983348846436\n",
            "iteration 194 :train_loss:3.2963879108428955 val_loss3.3504903316497803\n",
            "iteration 195 :train_loss:3.288440465927124 val_loss3.3425133228302\n",
            "iteration 196 :train_loss:3.280524492263794 val_loss3.334568500518799\n",
            "iteration 197 :train_loss:3.2726407051086426 val_loss3.3266549110412598\n",
            "iteration 198 :train_loss:3.264787435531616 val_loss3.318772315979004\n",
            "iteration 199 :train_loss:3.2569658756256104 val_loss3.3109216690063477\n",
            "iteration 200 :train_loss:3.2491748332977295 val_loss3.3031015396118164\n",
            "iteration 201 :train_loss:3.241414785385132 val_loss3.2953124046325684\n",
            "iteration 202 :train_loss:3.2336859703063965 val_loss3.2875545024871826\n",
            "iteration 203 :train_loss:3.225987434387207 val_loss3.279827117919922\n",
            "iteration 204 :train_loss:3.2183196544647217 val_loss3.272130012512207\n",
            "iteration 205 :train_loss:3.2106823921203613 val_loss3.2644641399383545\n",
            "iteration 206 :train_loss:3.203075647354126 val_loss3.2568280696868896\n",
            "iteration 207 :train_loss:3.1954987049102783 val_loss3.249222755432129\n",
            "iteration 208 :train_loss:3.1879520416259766 val_loss3.2416470050811768\n",
            "iteration 209 :train_loss:3.1804354190826416 val_loss3.2341015338897705\n",
            "iteration 210 :train_loss:3.1729483604431152 val_loss3.226585865020752\n",
            "iteration 211 :train_loss:3.1654908657073975 val_loss3.219099760055542\n",
            "iteration 212 :train_loss:3.1580631732940674 val_loss3.211643695831299\n",
            "iteration 213 :train_loss:3.150664806365967 val_loss3.204216957092285\n",
            "iteration 214 :train_loss:3.143296241760254 val_loss3.19681978225708\n",
            "iteration 215 :train_loss:3.1359565258026123 val_loss3.1894516944885254\n",
            "iteration 216 :train_loss:3.1286461353302 val_loss3.1821129322052\n",
            "iteration 217 :train_loss:3.1213643550872803 val_loss3.1748030185699463\n",
            "iteration 218 :train_loss:3.11411190032959 val_loss3.1675221920013428\n",
            "iteration 219 :train_loss:3.1068882942199707 val_loss3.1602699756622314\n",
            "iteration 220 :train_loss:3.0996925830841064 val_loss3.1530466079711914\n",
            "iteration 221 :train_loss:3.0925259590148926 val_loss3.1458516120910645\n",
            "iteration 222 :train_loss:3.085387706756592 val_loss3.1386849880218506\n",
            "iteration 223 :train_loss:3.078277826309204 val_loss3.131547212600708\n",
            "iteration 224 :train_loss:3.0711963176727295 val_loss3.1244373321533203\n",
            "iteration 225 :train_loss:3.064142942428589 val_loss3.1173558235168457\n",
            "iteration 226 :train_loss:3.057116746902466 val_loss3.110302209854126\n",
            "iteration 227 :train_loss:3.050118923187256 val_loss3.103276252746582\n",
            "iteration 228 :train_loss:3.04314923286438 val_loss3.096278190612793\n",
            "iteration 229 :train_loss:3.036206007003784 val_loss3.0893077850341797\n",
            "iteration 230 :train_loss:3.0292911529541016 val_loss3.082365036010742\n",
            "iteration 231 :train_loss:3.0224037170410156 val_loss3.0754497051239014\n",
            "iteration 232 :train_loss:3.0155434608459473 val_loss3.0685622692108154\n",
            "iteration 233 :train_loss:3.0087103843688965 val_loss3.061701536178589\n",
            "iteration 234 :train_loss:3.001904249191284 val_loss3.0548672676086426\n",
            "iteration 235 :train_loss:2.995124578475952 val_loss3.048060894012451\n",
            "iteration 236 :train_loss:2.988372802734375 val_loss3.04128098487854\n",
            "iteration 237 :train_loss:2.98164701461792 val_loss3.0345284938812256\n",
            "iteration 238 :train_loss:2.9749484062194824 val_loss3.027801513671875\n",
            "iteration 239 :train_loss:2.968276023864746 val_loss3.0211021900177\n",
            "iteration 240 :train_loss:2.961630344390869 val_loss3.0144286155700684\n",
            "iteration 241 :train_loss:2.955010414123535 val_loss3.007781982421875\n",
            "iteration 242 :train_loss:2.9484174251556396 val_loss3.0011613368988037\n",
            "iteration 243 :train_loss:2.941849708557129 val_loss2.9945669174194336\n",
            "iteration 244 :train_loss:2.9353084564208984 val_loss2.9879982471466064\n",
            "iteration 245 :train_loss:2.92879319190979 val_loss2.9814555644989014\n",
            "iteration 246 :train_loss:2.9223034381866455 val_loss2.9749391078948975\n",
            "iteration 247 :train_loss:2.915839195251465 val_loss2.9684479236602783\n",
            "iteration 248 :train_loss:2.9094011783599854 val_loss2.961982488632202\n",
            "iteration 249 :train_loss:2.9029881954193115 val_loss2.9555423259735107\n",
            "iteration 250 :train_loss:2.8966007232666016 val_loss2.9491279125213623\n",
            "iteration 251 :train_loss:2.8902387619018555 val_loss2.9427390098571777\n",
            "iteration 252 :train_loss:2.883901596069336 val_loss2.936375141143799\n",
            "iteration 253 :train_loss:2.877589702606201 val_loss2.9300363063812256\n",
            "iteration 254 :train_loss:2.871302604675293 val_loss2.923722505569458\n",
            "iteration 255 :train_loss:2.8650403022766113 val_loss2.917433738708496\n",
            "iteration 256 :train_loss:2.8588027954101562 val_loss2.9111695289611816\n",
            "iteration 257 :train_loss:2.8525900840759277 val_loss2.9049298763275146\n",
            "iteration 258 :train_loss:2.846402168273926 val_loss2.8987154960632324\n",
            "iteration 259 :train_loss:2.840238332748413 val_loss2.8925247192382812\n",
            "iteration 260 :train_loss:2.834099054336548 val_loss2.886359214782715\n",
            "iteration 261 :train_loss:2.827983856201172 val_loss2.8802173137664795\n",
            "iteration 262 :train_loss:2.821892738342285 val_loss2.8741002082824707\n",
            "iteration 263 :train_loss:2.815825939178467 val_loss2.868006706237793\n",
            "iteration 264 :train_loss:2.8097832202911377 val_loss2.8619375228881836\n",
            "iteration 265 :train_loss:2.8037643432617188 val_loss2.8558921813964844\n",
            "iteration 266 :train_loss:2.79776930809021 val_loss2.8498706817626953\n",
            "iteration 267 :train_loss:2.791797399520874 val_loss2.843872547149658\n",
            "iteration 268 :train_loss:2.7858502864837646 val_loss2.8378987312316895\n",
            "iteration 269 :train_loss:2.7799253463745117 val_loss2.8319480419158936\n",
            "iteration 270 :train_loss:2.774024486541748 val_loss2.8260207176208496\n",
            "iteration 271 :train_loss:2.7681467533111572 val_loss2.8201165199279785\n",
            "iteration 272 :train_loss:2.7622921466827393 val_loss2.8142359256744385\n",
            "iteration 273 :train_loss:2.7564609050750732 val_loss2.8083789348602295\n",
            "iteration 274 :train_loss:2.750652551651001 val_loss2.802544593811035\n",
            "iteration 275 :train_loss:2.7448673248291016 val_loss2.7967329025268555\n",
            "iteration 276 :train_loss:2.739104986190796 val_loss2.7909443378448486\n",
            "iteration 277 :train_loss:2.7333648204803467 val_loss2.7851784229278564\n",
            "iteration 278 :train_loss:2.727647542953491 val_loss2.779435396194458\n",
            "iteration 279 :train_loss:2.7219531536102295 val_loss2.773714780807495\n",
            "iteration 280 :train_loss:2.716280937194824 val_loss2.768017053604126\n",
            "iteration 281 :train_loss:2.7106313705444336 val_loss2.762341260910034\n",
            "iteration 282 :train_loss:2.7050037384033203 val_loss2.756688117980957\n",
            "iteration 283 :train_loss:2.6993985176086426 val_loss2.7510569095611572\n",
            "iteration 284 :train_loss:2.6938154697418213 val_loss2.745448112487793\n",
            "iteration 285 :train_loss:2.6882545948028564 val_loss2.739861488342285\n",
            "iteration 286 :train_loss:2.68271541595459 val_loss2.7342963218688965\n",
            "iteration 287 :train_loss:2.6771979331970215 val_loss2.7287533283233643\n",
            "iteration 288 :train_loss:2.6717026233673096 val_loss2.7232325077056885\n",
            "iteration 289 :train_loss:2.666228771209717 val_loss2.7177329063415527\n",
            "iteration 290 :train_loss:2.660776376724243 val_loss2.712254762649536\n",
            "iteration 291 :train_loss:2.655345916748047 val_loss2.706798553466797\n",
            "iteration 292 :train_loss:2.6499361991882324 val_loss2.7013638019561768\n",
            "iteration 293 :train_loss:2.644547939300537 val_loss2.6959500312805176\n",
            "iteration 294 :train_loss:2.63918137550354 val_loss2.6905574798583984\n",
            "iteration 295 :train_loss:2.633835554122925 val_loss2.6851863861083984\n",
            "iteration 296 :train_loss:2.6285107135772705 val_loss2.6798365116119385\n",
            "iteration 297 :train_loss:2.6232070922851562 val_loss2.6745073795318604\n",
            "iteration 298 :train_loss:2.617924213409424 val_loss2.6691994667053223\n",
            "iteration 299 :train_loss:2.6126625537872314 val_loss2.663912057876587\n",
            "iteration 300 :train_loss:2.607421398162842 val_loss2.6586453914642334\n",
            "iteration 301 :train_loss:2.602200746536255 val_loss2.6533994674682617\n",
            "iteration 302 :train_loss:2.59700083732605 val_loss2.64817476272583\n",
            "iteration 303 :train_loss:2.5918209552764893 val_loss2.642969846725464\n",
            "iteration 304 :train_loss:2.5866618156433105 val_loss2.6377854347229004\n",
            "iteration 305 :train_loss:2.5815229415893555 val_loss2.6326217651367188\n",
            "iteration 306 :train_loss:2.576404333114624 val_loss2.6274778842926025\n",
            "iteration 307 :train_loss:2.571305990219116 val_loss2.622354030609131\n",
            "iteration 308 :train_loss:2.566227674484253 val_loss2.617250919342041\n",
            "iteration 309 :train_loss:2.561168909072876 val_loss2.6121673583984375\n",
            "iteration 310 :train_loss:2.5561304092407227 val_loss2.6071043014526367\n",
            "iteration 311 :train_loss:2.551111936569214 val_loss2.602060556411743\n",
            "iteration 312 :train_loss:2.5461127758026123 val_loss2.597036361694336\n",
            "iteration 313 :train_loss:2.5411336421966553 val_loss2.5920324325561523\n",
            "iteration 314 :train_loss:2.5361740589141846 val_loss2.587047815322876\n",
            "iteration 315 :train_loss:2.531233549118042 val_loss2.582082986831665\n",
            "iteration 316 :train_loss:2.526312828063965 val_loss2.577136993408203\n",
            "iteration 317 :train_loss:2.521411418914795 val_loss2.5722110271453857\n",
            "iteration 318 :train_loss:2.516528844833374 val_loss2.5673041343688965\n",
            "iteration 319 :train_loss:2.5116658210754395 val_loss2.5624163150787354\n",
            "iteration 320 :train_loss:2.506822109222412 val_loss2.5575475692749023\n",
            "iteration 321 :train_loss:2.5019969940185547 val_loss2.5526981353759766\n",
            "iteration 322 :train_loss:2.4971909523010254 val_loss2.5478675365448\n",
            "iteration 323 :train_loss:2.4924044609069824 val_loss2.543056011199951\n",
            "iteration 324 :train_loss:2.487635612487793 val_loss2.5382635593414307\n",
            "iteration 325 :train_loss:2.48288631439209 val_loss2.533489227294922\n",
            "iteration 326 :train_loss:2.4781553745269775 val_loss2.528733968734741\n",
            "iteration 327 :train_loss:2.4734432697296143 val_loss2.5239970684051514\n",
            "iteration 328 :train_loss:2.468749523162842 val_loss2.5192787647247314\n",
            "iteration 329 :train_loss:2.464073419570923 val_loss2.5145785808563232\n",
            "iteration 330 :train_loss:2.459416627883911 val_loss2.509897232055664\n",
            "iteration 331 :train_loss:2.454777717590332 val_loss2.5052340030670166\n",
            "iteration 332 :train_loss:2.450157403945923 val_loss2.500589370727539\n",
            "iteration 333 :train_loss:2.4455549716949463 val_loss2.495962381362915\n",
            "iteration 334 :train_loss:2.4409704208374023 val_loss2.491353988647461\n",
            "iteration 335 :train_loss:2.43640398979187 val_loss2.4867630004882812\n",
            "iteration 336 :train_loss:2.4318552017211914 val_loss2.4821903705596924\n",
            "iteration 337 :train_loss:2.4273247718811035 val_loss2.477635622024536\n",
            "iteration 338 :train_loss:2.4228122234344482 val_loss2.4730985164642334\n",
            "iteration 339 :train_loss:2.4183168411254883 val_loss2.468579053878784\n",
            "iteration 340 :train_loss:2.413839340209961 val_loss2.4640774726867676\n",
            "iteration 341 :train_loss:2.409379005432129 val_loss2.4595932960510254\n",
            "iteration 342 :train_loss:2.4049365520477295 val_loss2.4551262855529785\n",
            "iteration 343 :train_loss:2.4005115032196045 val_loss2.4506771564483643\n",
            "iteration 344 :train_loss:2.396103620529175 val_loss2.4462454319000244\n",
            "iteration 345 :train_loss:2.3917126655578613 val_loss2.44183087348938\n",
            "iteration 346 :train_loss:2.3873395919799805 val_loss2.4374334812164307\n",
            "iteration 347 :train_loss:2.3829829692840576 val_loss2.4330532550811768\n",
            "iteration 348 :train_loss:2.3786439895629883 val_loss2.428690195083618\n",
            "iteration 349 :train_loss:2.374321937561035 val_loss2.424344301223755\n",
            "iteration 350 :train_loss:2.3700168132781982 val_loss2.4200148582458496\n",
            "iteration 351 :train_loss:2.3657283782958984 val_loss2.4157025814056396\n",
            "iteration 352 :train_loss:2.3614566326141357 val_loss2.411407470703125\n",
            "iteration 353 :train_loss:2.35720157623291 val_loss2.4071285724639893\n",
            "iteration 354 :train_loss:2.3529632091522217 val_loss2.4028661251068115\n",
            "iteration 355 :train_loss:2.3487417697906494 val_loss2.398621082305908\n",
            "iteration 356 :train_loss:2.344536304473877 val_loss2.3943917751312256\n",
            "iteration 357 :train_loss:2.3403475284576416 val_loss2.390179395675659\n",
            "iteration 358 :train_loss:2.3361752033233643 val_loss2.3859832286834717\n",
            "iteration 359 :train_loss:2.332019090652466 val_loss2.381803274154663\n",
            "iteration 360 :train_loss:2.3278794288635254 val_loss2.3776400089263916\n",
            "iteration 361 :train_loss:2.3237555027008057 val_loss2.37349271774292\n",
            "iteration 362 :train_loss:2.319647789001465 val_loss2.369361162185669\n",
            "iteration 363 :train_loss:2.315556287765503 val_loss2.365246534347534\n",
            "iteration 364 :train_loss:2.3114805221557617 val_loss2.361147165298462\n",
            "iteration 365 :train_loss:2.3074209690093994 val_loss2.3570642471313477\n",
            "iteration 366 :train_loss:2.303377151489258 val_loss2.352996826171875\n",
            "iteration 367 :train_loss:2.299349069595337 val_loss2.348945379257202\n",
            "iteration 368 :train_loss:2.2953367233276367 val_loss2.344909429550171\n",
            "iteration 369 :train_loss:2.2913401126861572 val_loss2.3408894538879395\n",
            "iteration 370 :train_loss:2.2873589992523193 val_loss2.3368849754333496\n",
            "iteration 371 :train_loss:2.283393383026123 val_loss2.3328959941864014\n",
            "iteration 372 :train_loss:2.2794432640075684 val_loss2.3289225101470947\n",
            "iteration 373 :train_loss:2.2755086421966553 val_loss2.3249642848968506\n",
            "iteration 374 :train_loss:2.2715890407562256 val_loss2.321021556854248\n",
            "iteration 375 :train_loss:2.2676851749420166 val_loss2.317094326019287\n",
            "iteration 376 :train_loss:2.263796091079712 val_loss2.3131821155548096\n",
            "iteration 377 :train_loss:2.2599222660064697 val_loss2.3092851638793945\n",
            "iteration 378 :train_loss:2.25606369972229 val_loss2.305403232574463\n",
            "iteration 379 :train_loss:2.2522199153900146 val_loss2.3015363216400146\n",
            "iteration 380 :train_loss:2.2483911514282227 val_loss2.297684669494629\n",
            "iteration 381 :train_loss:2.244577407836914 val_loss2.2938475608825684\n",
            "iteration 382 :train_loss:2.2407784461975098 val_loss2.290025472640991\n",
            "iteration 383 :train_loss:2.2369942665100098 val_loss2.2862181663513184\n",
            "iteration 384 :train_loss:2.233224868774414 val_loss2.282425880432129\n",
            "iteration 385 :train_loss:2.2294700145721436 val_loss2.2786481380462646\n",
            "iteration 386 :train_loss:2.2257301807403564 val_loss2.2748849391937256\n",
            "iteration 387 :train_loss:2.2220046520233154 val_loss2.2711362838745117\n",
            "iteration 388 :train_loss:2.2182934284210205 val_loss2.267402172088623\n",
            "iteration 389 :train_loss:2.214596748352051 val_loss2.2636826038360596\n",
            "iteration 390 :train_loss:2.2109146118164062 val_loss2.259977340698242\n",
            "iteration 391 :train_loss:2.207246780395508 val_loss2.256286382675171\n",
            "iteration 392 :train_loss:2.2035930156707764 val_loss2.252609968185425\n",
            "iteration 393 :train_loss:2.199953079223633 val_loss2.2489476203918457\n",
            "iteration 394 :train_loss:2.1963281631469727 val_loss2.2452993392944336\n",
            "iteration 395 :train_loss:2.192716598510742 val_loss2.2416653633117676\n",
            "iteration 396 :train_loss:2.189119577407837 val_loss2.2380452156066895\n",
            "iteration 397 :train_loss:2.1855361461639404 val_loss2.234438896179199\n",
            "iteration 398 :train_loss:2.181966543197632 val_loss2.230846643447876\n",
            "iteration 399 :train_loss:2.1784112453460693 val_loss2.2272684574127197\n",
            "iteration 400 :train_loss:2.1748695373535156 val_loss2.2237040996551514\n",
            "iteration 401 :train_loss:2.1713414192199707 val_loss2.220153331756592\n",
            "iteration 402 :train_loss:2.1678271293640137 val_loss2.21661639213562\n",
            "iteration 403 :train_loss:2.1643264293670654 val_loss2.213092803955078\n",
            "iteration 404 :train_loss:2.160839319229126 val_loss2.209583044052124\n",
            "iteration 405 :train_loss:2.157365560531616 val_loss2.2060868740081787\n",
            "iteration 406 :train_loss:2.1539053916931152 val_loss2.202604055404663\n",
            "iteration 407 :train_loss:2.150458335876465 val_loss2.199134349822998\n",
            "iteration 408 :train_loss:2.1470251083374023 val_loss2.195678472518921\n",
            "iteration 409 :train_loss:2.1436049938201904 val_loss2.1922357082366943\n",
            "iteration 410 :train_loss:2.140198230743408 val_loss2.1888065338134766\n",
            "iteration 411 :train_loss:2.1368043422698975 val_loss2.1853902339935303\n",
            "iteration 412 :train_loss:2.1334238052368164 val_loss2.1819872856140137\n",
            "iteration 413 :train_loss:2.130056381225586 val_loss2.1785972118377686\n",
            "iteration 414 :train_loss:2.126701831817627 val_loss2.175220251083374\n",
            "iteration 415 :train_loss:2.1233603954315186 val_loss2.17185640335083\n",
            "iteration 416 :train_loss:2.1200318336486816 val_loss2.1685051918029785\n",
            "iteration 417 :train_loss:2.116715908050537 val_loss2.1651673316955566\n",
            "iteration 418 :train_loss:2.1134133338928223 val_loss2.161841869354248\n",
            "iteration 419 :train_loss:2.1101229190826416 val_loss2.158529758453369\n",
            "iteration 420 :train_loss:2.1068453788757324 val_loss2.1552298069000244\n",
            "iteration 421 :train_loss:2.1035804748535156 val_loss2.151942729949951\n",
            "iteration 422 :train_loss:2.1003286838531494 val_loss2.148668050765991\n",
            "iteration 423 :train_loss:2.0970890522003174 val_loss2.1454062461853027\n",
            "iteration 424 :train_loss:2.0938618183135986 val_loss2.1421570777893066\n",
            "iteration 425 :train_loss:2.0906472206115723 val_loss2.1389200687408447\n",
            "iteration 426 :train_loss:2.08744478225708 val_loss2.135695457458496\n",
            "iteration 427 :train_loss:2.084254741668701 val_loss2.1324832439422607\n",
            "iteration 428 :train_loss:2.0810770988464355 val_loss2.1292834281921387\n",
            "iteration 429 :train_loss:2.077911615371704 val_loss2.12609601020813\n",
            "iteration 430 :train_loss:2.074758529663086 val_loss2.1229207515716553\n",
            "iteration 431 :train_loss:2.071617603302002 val_loss2.1197574138641357\n",
            "iteration 432 :train_loss:2.068488597869873 val_loss2.1166062355041504\n",
            "iteration 433 :train_loss:2.0653717517852783 val_loss2.1134674549102783\n",
            "iteration 434 :train_loss:2.0622665882110596 val_loss2.110340118408203\n",
            "iteration 435 :train_loss:2.059173583984375 val_loss2.107224941253662\n",
            "iteration 436 :train_loss:2.0560922622680664 val_loss2.1041219234466553\n",
            "iteration 437 :train_loss:2.053023099899292 val_loss2.1010305881500244\n",
            "iteration 438 :train_loss:2.0499658584594727 val_loss2.0979509353637695\n",
            "iteration 439 :train_loss:2.04692006111145 val_loss2.094883441925049\n",
            "iteration 440 :train_loss:2.0438859462738037 val_loss2.091827392578125\n",
            "iteration 441 :train_loss:2.0408637523651123 val_loss2.088782787322998\n",
            "iteration 442 :train_loss:2.0378527641296387 val_loss2.0857503414154053\n",
            "iteration 443 :train_loss:2.03485369682312 val_loss2.0827291011810303\n",
            "iteration 444 :train_loss:2.0318660736083984 val_loss2.079719305038452\n",
            "iteration 445 :train_loss:2.0288896560668945 val_loss2.076721429824829\n",
            "iteration 446 :train_loss:2.0259249210357666 val_loss2.0737345218658447\n",
            "iteration 447 :train_loss:2.0229713916778564 val_loss2.070758819580078\n",
            "iteration 448 :train_loss:2.020029067993164 val_loss2.0677950382232666\n",
            "iteration 449 :train_loss:2.0170984268188477 val_loss2.0648422241210938\n",
            "iteration 450 :train_loss:2.01417875289917 val_loss2.0619008541107178\n",
            "iteration 451 :train_loss:2.011270046234131 val_loss2.0589702129364014\n",
            "iteration 452 :train_loss:2.0083727836608887 val_loss2.056051254272461\n",
            "iteration 453 :train_loss:2.005486488342285 val_loss2.053143262863159\n",
            "iteration 454 :train_loss:2.0026113986968994 val_loss2.050246238708496\n",
            "iteration 455 :train_loss:1.9997472763061523 val_loss2.0473601818084717\n",
            "iteration 456 :train_loss:1.9968940019607544 val_loss2.044485330581665\n",
            "iteration 457 :train_loss:1.9940516948699951 val_loss2.041621446609497\n",
            "iteration 458 :train_loss:1.991220235824585 val_loss2.0387680530548096\n",
            "iteration 459 :train_loss:1.988399624824524 val_loss2.035926103591919\n",
            "iteration 460 :train_loss:1.985589623451233 val_loss2.0330941677093506\n",
            "iteration 461 :train_loss:1.9827907085418701 val_loss2.030273199081421\n",
            "iteration 462 :train_loss:1.9800022840499878 val_loss2.02746319770813\n",
            "iteration 463 :train_loss:1.977224349975586 val_loss2.0246636867523193\n",
            "iteration 464 :train_loss:1.9744571447372437 val_loss2.0218749046325684\n",
            "iteration 465 :train_loss:1.9717005491256714 val_loss2.019096612930298\n",
            "iteration 466 :train_loss:1.96895432472229 val_loss2.016328811645508\n",
            "iteration 467 :train_loss:1.9662187099456787 val_loss2.013571262359619\n",
            "iteration 468 :train_loss:1.9634932279586792 val_loss2.01082444190979\n",
            "iteration 469 :train_loss:1.9607784748077393 val_loss2.0080878734588623\n",
            "iteration 470 :train_loss:1.9580738544464111 val_loss2.005361795425415\n",
            "iteration 471 :train_loss:1.9553797245025635 val_loss2.002645969390869\n",
            "iteration 472 :train_loss:1.9526958465576172 val_loss1.9999403953552246\n",
            "iteration 473 :train_loss:1.9500219821929932 val_loss1.997245192527771\n",
            "iteration 474 :train_loss:1.9473583698272705 val_loss1.99455988407135\n",
            "iteration 475 :train_loss:1.9447050094604492 val_loss1.9918850660324097\n",
            "iteration 476 :train_loss:1.942061424255371 val_loss1.9892199039459229\n",
            "iteration 477 :train_loss:1.9394280910491943 val_loss1.986565113067627\n",
            "iteration 478 :train_loss:1.9368047714233398 val_loss1.9839203357696533\n",
            "iteration 479 :train_loss:1.9341914653778076 val_loss1.9812853336334229\n",
            "iteration 480 :train_loss:1.931587815284729 val_loss1.9786603450775146\n",
            "iteration 481 :train_loss:1.9289944171905518 val_loss1.9760452508926392\n",
            "iteration 482 :train_loss:1.9264105558395386 val_loss1.9734400510787964\n",
            "iteration 483 :train_loss:1.9238364696502686 val_loss1.9708443880081177\n",
            "iteration 484 :train_loss:1.9212721586227417 val_loss1.9682587385177612\n",
            "iteration 485 :train_loss:1.9187175035476685 val_loss1.9656827449798584\n",
            "iteration 486 :train_loss:1.916172742843628 val_loss1.9631165266036987\n",
            "iteration 487 :train_loss:1.913637638092041 val_loss1.9605600833892822\n",
            "iteration 488 :train_loss:1.911111831665039 val_loss1.9580128192901611\n",
            "iteration 489 :train_loss:1.9085955619812012 val_loss1.9554753303527832\n",
            "iteration 490 :train_loss:1.906089186668396 val_loss1.9529474973678589\n",
            "iteration 491 :train_loss:1.9035919904708862 val_loss1.95042884349823\n",
            "iteration 492 :train_loss:1.9011045694351196 val_loss1.9479197263717651\n",
            "iteration 493 :train_loss:1.8986259698867798 val_loss1.945420265197754\n",
            "iteration 494 :train_loss:1.8961570262908936 val_loss1.942929983139038\n",
            "iteration 495 :train_loss:1.8936973810195923 val_loss1.9404487609863281\n",
            "iteration 496 :train_loss:1.8912471532821655 val_loss1.9379771947860718\n",
            "iteration 497 :train_loss:1.8888057470321655 val_loss1.9355146884918213\n",
            "iteration 498 :train_loss:1.8863739967346191 val_loss1.9330614805221558\n",
            "iteration 499 :train_loss:1.883951187133789 val_loss1.930617332458496\n",
            "iteration 500 :train_loss:1.8815375566482544 val_loss1.9281822443008423\n",
            "iteration 501 :train_loss:1.8791329860687256 val_loss1.925756573677063\n",
            "iteration 502 :train_loss:1.8767375946044922 val_loss1.92333984375\n",
            "iteration 503 :train_loss:1.874351143836975 val_loss1.9209320545196533\n",
            "iteration 504 :train_loss:1.8719735145568848 val_loss1.918533444404602\n",
            "iteration 505 :train_loss:1.8696049451828003 val_loss1.9161434173583984\n",
            "iteration 506 :train_loss:1.8672451972961426 val_loss1.9137623310089111\n",
            "iteration 507 :train_loss:1.8648942708969116 val_loss1.9113903045654297\n",
            "iteration 508 :train_loss:1.862552285194397 val_loss1.9090272188186646\n",
            "iteration 509 :train_loss:1.860219120979309 val_loss1.906672477722168\n",
            "iteration 510 :train_loss:1.857894778251648 val_loss1.9043267965316772\n",
            "iteration 511 :train_loss:1.8555787801742554 val_loss1.9019899368286133\n",
            "iteration 512 :train_loss:1.853271722793579 val_loss1.8996615409851074\n",
            "iteration 513 :train_loss:1.8509732484817505 val_loss1.8973418474197388\n",
            "iteration 514 :train_loss:1.8486833572387695 val_loss1.8950307369232178\n",
            "iteration 515 :train_loss:1.8464019298553467 val_loss1.892728328704834\n",
            "iteration 516 :train_loss:1.8441293239593506 val_loss1.8904342651367188\n",
            "iteration 517 :train_loss:1.8418649435043335 val_loss1.8881487846374512\n",
            "iteration 518 :train_loss:1.839609146118164 val_loss1.8858718872070312\n",
            "iteration 519 :train_loss:1.8373618125915527 val_loss1.8836030960083008\n",
            "iteration 520 :train_loss:1.8351225852966309 val_loss1.8813430070877075\n",
            "iteration 521 :train_loss:1.8328920602798462 val_loss1.8790912628173828\n",
            "iteration 522 :train_loss:1.830669641494751 val_loss1.876847743988037\n",
            "iteration 523 :train_loss:1.8284556865692139 val_loss1.87461256980896\n",
            "iteration 524 :train_loss:1.8262497186660767 val_loss1.8723857402801514\n",
            "iteration 525 :train_loss:1.8240519762039185 val_loss1.870166540145874\n",
            "iteration 526 :train_loss:1.821862816810608 val_loss1.8679561614990234\n",
            "iteration 527 :train_loss:1.8196815252304077 val_loss1.8657536506652832\n",
            "iteration 528 :train_loss:1.817508339881897 val_loss1.8635594844818115\n",
            "iteration 529 :train_loss:1.8153432607650757 val_loss1.8613730669021606\n",
            "iteration 530 :train_loss:1.8131861686706543 val_loss1.8591951131820679\n",
            "iteration 531 :train_loss:1.8110368251800537 val_loss1.8570247888565063\n",
            "iteration 532 :train_loss:1.8088959455490112 val_loss1.8548624515533447\n",
            "iteration 533 :train_loss:1.8067626953125 val_loss1.8527082204818726\n",
            "iteration 534 :train_loss:1.8046371936798096 val_loss1.8505618572235107\n",
            "iteration 535 :train_loss:1.8025201559066772 val_loss1.8484232425689697\n",
            "iteration 536 :train_loss:1.8004103899002075 val_loss1.8462926149368286\n",
            "iteration 537 :train_loss:1.7983087301254272 val_loss1.8441699743270874\n",
            "iteration 538 :train_loss:1.7962148189544678 val_loss1.842054843902588\n",
            "iteration 539 :train_loss:1.7941285371780396 val_loss1.8399474620819092\n",
            "iteration 540 :train_loss:1.7920498847961426 val_loss1.8378478288650513\n",
            "iteration 541 :train_loss:1.7899789810180664 val_loss1.8357559442520142\n",
            "iteration 542 :train_loss:1.7879159450531006 val_loss1.8336716890335083\n",
            "iteration 543 :train_loss:1.7858601808547974 val_loss1.8315950632095337\n",
            "iteration 544 :train_loss:1.783812165260315 val_loss1.8295258283615112\n",
            "iteration 545 :train_loss:1.7817715406417847 val_loss1.8274643421173096\n",
            "iteration 546 :train_loss:1.7797387838363647 val_loss1.8254106044769287\n",
            "iteration 547 :train_loss:1.7777132987976074 val_loss1.823364019393921\n",
            "iteration 548 :train_loss:1.7756953239440918 val_loss1.8213250637054443\n",
            "iteration 549 :train_loss:1.7736848592758179 val_loss1.8192936182022095\n",
            "iteration 550 :train_loss:1.771681547164917 val_loss1.8172693252563477\n",
            "iteration 551 :train_loss:1.7696857452392578 val_loss1.815252423286438\n",
            "iteration 552 :train_loss:1.7676969766616821 val_loss1.813242793083191\n",
            "iteration 553 :train_loss:1.7657157182693481 val_loss1.811240553855896\n",
            "iteration 554 :train_loss:1.7637416124343872 val_loss1.8092454671859741\n",
            "iteration 555 :train_loss:1.7617747783660889 val_loss1.8072575330734253\n",
            "iteration 556 :train_loss:1.7598152160644531 val_loss1.8052772283554077\n",
            "iteration 557 :train_loss:1.7578628063201904 val_loss1.803303599357605\n",
            "iteration 558 :train_loss:1.7559176683425903 val_loss1.801337480545044\n",
            "iteration 559 :train_loss:1.7539794445037842 val_loss1.7993782758712769\n",
            "iteration 560 :train_loss:1.752048373222351 val_loss1.7974262237548828\n",
            "iteration 561 :train_loss:1.750124216079712 val_loss1.7954809665679932\n",
            "iteration 562 :train_loss:1.7482070922851562 val_loss1.7935431003570557\n",
            "iteration 563 :train_loss:1.7462971210479736 val_loss1.7916120290756226\n",
            "iteration 564 :train_loss:1.744394063949585 val_loss1.7896881103515625\n",
            "iteration 565 :train_loss:1.7424980401992798 val_loss1.7877708673477173\n",
            "iteration 566 :train_loss:1.7406085729599 val_loss1.785860538482666\n",
            "iteration 567 :train_loss:1.7387261390686035 val_loss1.7839571237564087\n",
            "iteration 568 :train_loss:1.7368507385253906 val_loss1.7820605039596558\n",
            "iteration 569 :train_loss:1.734981894493103 val_loss1.7801709175109863\n",
            "iteration 570 :train_loss:1.7331199645996094 val_loss1.7782878875732422\n",
            "iteration 571 :train_loss:1.731264591217041 val_loss1.7764116525650024\n",
            "iteration 572 :train_loss:1.7294158935546875 val_loss1.774542212486267\n",
            "iteration 573 :train_loss:1.727574110031128 val_loss1.7726795673370361\n",
            "iteration 574 :train_loss:1.7257390022277832 val_loss1.770823359489441\n",
            "iteration 575 :train_loss:1.7239106893539429 val_loss1.7689740657806396\n",
            "iteration 576 :train_loss:1.7220889329910278 val_loss1.7671313285827637\n",
            "iteration 577 :train_loss:1.720273494720459 val_loss1.7652950286865234\n",
            "iteration 578 :train_loss:1.718464970588684 val_loss1.763465404510498\n",
            "iteration 579 :train_loss:1.716662883758545 val_loss1.761642336845398\n",
            "iteration 580 :train_loss:1.7148669958114624 val_loss1.7598258256912231\n",
            "iteration 581 :train_loss:1.7130779027938843 val_loss1.758015513420105\n",
            "iteration 582 :train_loss:1.7112951278686523 val_loss1.7562116384506226\n",
            "iteration 583 :train_loss:1.7095187902450562 val_loss1.754414677619934\n",
            "iteration 584 :train_loss:1.7077488899230957 val_loss1.7526236772537231\n",
            "iteration 585 :train_loss:1.705985188484192 val_loss1.7508389949798584\n",
            "iteration 586 :train_loss:1.7042279243469238 val_loss1.7490606307983398\n",
            "iteration 587 :train_loss:1.7024767398834229 val_loss1.747288703918457\n",
            "iteration 588 :train_loss:1.7007321119308472 val_loss1.74552321434021\n",
            "iteration 589 :train_loss:1.6989936828613281 val_loss1.74376380443573\n",
            "iteration 590 :train_loss:1.6972615718841553 val_loss1.742010474205017\n",
            "iteration 591 :train_loss:1.6955351829528809 val_loss1.7402633428573608\n",
            "iteration 592 :train_loss:1.6938155889511108 val_loss1.7385226488113403\n",
            "iteration 593 :train_loss:1.6921017169952393 val_loss1.7367879152297974\n",
            "iteration 594 :train_loss:1.6903941631317139 val_loss1.7350592613220215\n",
            "iteration 595 :train_loss:1.688692569732666 val_loss1.7333368062973022\n",
            "iteration 596 :train_loss:1.6869971752166748 val_loss1.7316205501556396\n",
            "iteration 597 :train_loss:1.6853077411651611 val_loss1.729910135269165\n",
            "iteration 598 :train_loss:1.683624505996704 val_loss1.7282055616378784\n",
            "iteration 599 :train_loss:1.6819467544555664 val_loss1.7265070676803589\n",
            "iteration 600 :train_loss:1.680275321006775 val_loss1.7248146533966064\n",
            "iteration 601 :train_loss:1.678609848022461 val_loss1.7231281995773315\n",
            "iteration 602 :train_loss:1.6769499778747559 val_loss1.7214473485946655\n",
            "iteration 603 :train_loss:1.6752961874008179 val_loss1.7197725772857666\n",
            "iteration 604 :train_loss:1.6736483573913574 val_loss1.7181037664413452\n",
            "iteration 605 :train_loss:1.6720061302185059 val_loss1.7164404392242432\n",
            "iteration 606 :train_loss:1.6703699827194214 val_loss1.7147834300994873\n",
            "iteration 607 :train_loss:1.6687393188476562 val_loss1.7131319046020508\n",
            "iteration 608 :train_loss:1.667114496231079 val_loss1.7114861011505127\n",
            "iteration 609 :train_loss:1.6654956340789795 val_loss1.709846019744873\n",
            "iteration 610 :train_loss:1.6638821363449097 val_loss1.7082117795944214\n",
            "iteration 611 :train_loss:1.6622744798660278 val_loss1.7065831422805786\n",
            "iteration 612 :train_loss:1.660672664642334 val_loss1.7049602270126343\n",
            "iteration 613 :train_loss:1.6590763330459595 val_loss1.7033430337905884\n",
            "iteration 614 :train_loss:1.6574853658676147 val_loss1.7017310857772827\n",
            "iteration 615 :train_loss:1.6559003591537476 val_loss1.700124979019165\n",
            "iteration 616 :train_loss:1.6543207168579102 val_loss1.6985244750976562\n",
            "iteration 617 :train_loss:1.6527469158172607 val_loss1.6969294548034668\n",
            "iteration 618 :train_loss:1.651178240776062 val_loss1.6953400373458862\n",
            "iteration 619 :train_loss:1.6496151685714722 val_loss1.6937559843063354\n",
            "iteration 620 :train_loss:1.648057460784912 val_loss1.6921772956848145\n",
            "iteration 621 :train_loss:1.646505355834961 val_loss1.6906042098999023\n",
            "iteration 622 :train_loss:1.6449586153030396 val_loss1.6890363693237305\n",
            "iteration 623 :train_loss:1.6434171199798584 val_loss1.6874738931655884\n",
            "iteration 624 :train_loss:1.6418813467025757 val_loss1.6859166622161865\n",
            "iteration 625 :train_loss:1.640350580215454 val_loss1.6843650341033936\n",
            "iteration 626 :train_loss:1.6388252973556519 val_loss1.6828186511993408\n",
            "iteration 627 :train_loss:1.6373051404953003 val_loss1.6812776327133179\n",
            "iteration 628 :train_loss:1.635790467262268 val_loss1.679741621017456\n",
            "iteration 629 :train_loss:1.634280800819397 val_loss1.6782112121582031\n",
            "iteration 630 :train_loss:1.6327767372131348 val_loss1.6766856908798218\n",
            "iteration 631 :train_loss:1.6312776803970337 val_loss1.6751654148101807\n",
            "iteration 632 :train_loss:1.6297837495803833 val_loss1.6736505031585693\n",
            "iteration 633 :train_loss:1.6282950639724731 val_loss1.6721407175064087\n",
            "iteration 634 :train_loss:1.6268112659454346 val_loss1.6706359386444092\n",
            "iteration 635 :train_loss:1.6253327131271362 val_loss1.6691362857818604\n",
            "iteration 636 :train_loss:1.6238594055175781 val_loss1.6676417589187622\n",
            "iteration 637 :train_loss:1.6223909854888916 val_loss1.6661522388458252\n",
            "iteration 638 :train_loss:1.6209275722503662 val_loss1.6646678447723389\n",
            "iteration 639 :train_loss:1.619469404220581 val_loss1.6631882190704346\n",
            "iteration 640 :train_loss:1.6180158853530884 val_loss1.6617138385772705\n",
            "iteration 641 :train_loss:1.616567850112915 val_loss1.6602444648742676\n",
            "iteration 642 :train_loss:1.6151247024536133 val_loss1.6587798595428467\n",
            "iteration 643 :train_loss:1.6136860847473145 val_loss1.657320499420166\n",
            "iteration 644 :train_loss:1.6122527122497559 val_loss1.6558657884597778\n",
            "iteration 645 :train_loss:1.6108239889144897 val_loss1.6544160842895508\n",
            "iteration 646 :train_loss:1.6094002723693848 val_loss1.6529711484909058\n",
            "iteration 647 :train_loss:1.6079812049865723 val_loss1.6515311002731323\n",
            "iteration 648 :train_loss:1.6065672636032104 val_loss1.6500959396362305\n",
            "iteration 649 :train_loss:1.6051580905914307 val_loss1.6486655473709106\n",
            "iteration 650 :train_loss:1.603753685951233 val_loss1.6472398042678833\n",
            "iteration 651 :train_loss:1.6023539304733276 val_loss1.645818829536438\n",
            "iteration 652 :train_loss:1.600959062576294 val_loss1.6444026231765747\n",
            "iteration 653 :train_loss:1.5995688438415527 val_loss1.642991304397583\n",
            "iteration 654 :train_loss:1.598183274269104 val_loss1.6415842771530151\n",
            "iteration 655 :train_loss:1.5968023538589478 val_loss1.640182375907898\n",
            "iteration 656 :train_loss:1.595426082611084 val_loss1.6387847661972046\n",
            "iteration 657 :train_loss:1.5940544605255127 val_loss1.6373918056488037\n",
            "iteration 658 :train_loss:1.5926876068115234 val_loss1.636003851890564\n",
            "iteration 659 :train_loss:1.591325283050537 val_loss1.634619951248169\n",
            "iteration 660 :train_loss:1.5899672508239746 val_loss1.6332409381866455\n",
            "iteration 661 :train_loss:1.5886141061782837 val_loss1.6318663358688354\n",
            "iteration 662 :train_loss:1.5872653722763062 val_loss1.6304965019226074\n",
            "iteration 663 :train_loss:1.5859214067459106 val_loss1.6291309595108032\n",
            "iteration 664 :train_loss:1.5845816135406494 val_loss1.627769947052002\n",
            "iteration 665 :train_loss:1.5832462310791016 val_loss1.626413345336914\n",
            "iteration 666 :train_loss:1.5819154977798462 val_loss1.6250611543655396\n",
            "iteration 667 :train_loss:1.5805892944335938 val_loss1.623713493347168\n",
            "iteration 668 :train_loss:1.5792672634124756 val_loss1.6223701238632202\n",
            "iteration 669 :train_loss:1.5779496431350708 val_loss1.621031403541565\n",
            "iteration 670 :train_loss:1.576636552810669 val_loss1.619696855545044\n",
            "iteration 671 :train_loss:1.575327754020691 val_loss1.6183665990829468\n",
            "iteration 672 :train_loss:1.5740231275558472 val_loss1.617040753364563\n",
            "iteration 673 :train_loss:1.5727230310440063 val_loss1.6157190799713135\n",
            "iteration 674 :train_loss:1.5714272260665894 val_loss1.6144016981124878\n",
            "iteration 675 :train_loss:1.5701355934143066 val_loss1.613088846206665\n",
            "iteration 676 :train_loss:1.5688482522964478 val_loss1.611780047416687\n",
            "iteration 677 :train_loss:1.5675652027130127 val_loss1.6104756593704224\n",
            "iteration 678 :train_loss:1.566286325454712 val_loss1.6091750860214233\n",
            "iteration 679 :train_loss:1.5650116205215454 val_loss1.6078788042068481\n",
            "iteration 680 :train_loss:1.5637410879135132 val_loss1.6065869331359863\n",
            "iteration 681 :train_loss:1.5624747276306152 val_loss1.6052989959716797\n",
            "iteration 682 :train_loss:1.561212420463562 val_loss1.6040152311325073\n",
            "iteration 683 :train_loss:1.559954285621643 val_loss1.6027355194091797\n",
            "iteration 684 :train_loss:1.5587003231048584 val_loss1.6014599800109863\n",
            "iteration 685 :train_loss:1.557450294494629 val_loss1.6001883745193481\n",
            "iteration 686 :train_loss:1.5562046766281128 val_loss1.5989210605621338\n",
            "iteration 687 :train_loss:1.5549626350402832 val_loss1.597657561302185\n",
            "iteration 688 :train_loss:1.553724765777588 val_loss1.596398115158081\n",
            "iteration 689 :train_loss:1.5524909496307373 val_loss1.5951427221298218\n",
            "iteration 690 :train_loss:1.5512611865997314 val_loss1.5938911437988281\n",
            "iteration 691 :train_loss:1.5500353574752808 val_loss1.5926436185836792\n",
            "iteration 692 :train_loss:1.5488132238388062 val_loss1.5914002656936646\n",
            "iteration 693 :train_loss:1.5475952625274658 val_loss1.5901603698730469\n",
            "iteration 694 :train_loss:1.5463812351226807 val_loss1.588924765586853\n",
            "iteration 695 :train_loss:1.5451711416244507 val_loss1.5876928567886353\n",
            "iteration 696 :train_loss:1.5439648628234863 val_loss1.5864648818969727\n",
            "iteration 697 :train_loss:1.5427625179290771 val_loss1.5852408409118652\n",
            "iteration 698 :train_loss:1.5415637493133545 val_loss1.5840203762054443\n",
            "iteration 699 :train_loss:1.540368914604187 val_loss1.5828040838241577\n",
            "iteration 700 :train_loss:1.5391780138015747 val_loss1.581591248512268\n",
            "iteration 701 :train_loss:1.5379908084869385 val_loss1.5803823471069336\n",
            "iteration 702 :train_loss:1.5368074178695679 val_loss1.5791771411895752\n",
            "iteration 703 :train_loss:1.5356277227401733 val_loss1.5779756307601929\n",
            "iteration 704 :train_loss:1.5344517230987549 val_loss1.5767779350280762\n",
            "iteration 705 :train_loss:1.533279538154602 val_loss1.575584053993225\n",
            "iteration 706 :train_loss:1.5321110486984253 val_loss1.574393630027771\n",
            "iteration 707 :train_loss:1.5309463739395142 val_loss1.573207139968872\n",
            "iteration 708 :train_loss:1.529785394668579 val_loss1.5720242261886597\n",
            "iteration 709 :train_loss:1.528627872467041 val_loss1.5708447694778442\n",
            "iteration 710 :train_loss:1.5274738073349 val_loss1.5696691274642944\n",
            "iteration 711 :train_loss:1.526323676109314 val_loss1.5684969425201416\n",
            "iteration 712 :train_loss:1.525177240371704 val_loss1.5673285722732544\n",
            "iteration 713 :train_loss:1.5240342617034912 val_loss1.5661636590957642\n",
            "iteration 714 :train_loss:1.5228947401046753 val_loss1.5650023221969604\n",
            "iteration 715 :train_loss:1.5217586755752563 val_loss1.5638442039489746\n",
            "iteration 716 :train_loss:1.520626425743103 val_loss1.5626899003982544\n",
            "iteration 717 :train_loss:1.5194975137710571 val_loss1.5615389347076416\n",
            "iteration 718 :train_loss:1.5183721780776978 val_loss1.5603917837142944\n",
            "iteration 719 :train_loss:1.5172500610351562 val_loss1.5592477321624756\n",
            "iteration 720 :train_loss:1.5161316394805908 val_loss1.5581071376800537\n",
            "iteration 721 :train_loss:1.515016794204712 val_loss1.5569701194763184\n",
            "iteration 722 :train_loss:1.51390540599823 val_loss1.55583655834198\n",
            "iteration 723 :train_loss:1.512797236442566 val_loss1.554706335067749\n",
            "iteration 724 :train_loss:1.5116926431655884 val_loss1.5535796880722046\n",
            "iteration 725 :train_loss:1.5105911493301392 val_loss1.552456021308899\n",
            "iteration 726 :train_loss:1.509493350982666 val_loss1.5513360500335693\n",
            "iteration 727 :train_loss:1.5083987712860107 val_loss1.5502192974090576\n",
            "iteration 728 :train_loss:1.5073074102401733 val_loss1.5491058826446533\n",
            "iteration 729 :train_loss:1.5062193870544434 val_loss1.5479958057403564\n",
            "iteration 730 :train_loss:1.5051348209381104 val_loss1.5468889474868774\n",
            "iteration 731 :train_loss:1.5040535926818848 val_loss1.5457853078842163\n",
            "iteration 732 :train_loss:1.502975344657898 val_loss1.5446850061416626\n",
            "iteration 733 :train_loss:1.5019004344940186 val_loss1.5435880422592163\n",
            "iteration 734 :train_loss:1.5008289813995361 val_loss1.5424939393997192\n",
            "iteration 735 :train_loss:1.4997607469558716 val_loss1.5414035320281982\n",
            "iteration 736 :train_loss:1.4986953735351562 val_loss1.540316104888916\n",
            "iteration 737 :train_loss:1.4976335763931274 val_loss1.5392316579818726\n",
            "iteration 738 :train_loss:1.496574878692627 val_loss1.5381509065628052\n",
            "iteration 739 :train_loss:1.4955192804336548 val_loss1.5370728969573975\n",
            "iteration 740 :train_loss:1.494466781616211 val_loss1.5359981060028076\n",
            "iteration 741 :train_loss:1.493417739868164 val_loss1.534926414489746\n",
            "iteration 742 :train_loss:1.4923713207244873 val_loss1.5338579416275024\n",
            "iteration 743 :train_loss:1.4913283586502075 val_loss1.532792568206787\n",
            "iteration 744 :train_loss:1.4902883768081665 val_loss1.531730055809021\n",
            "iteration 745 :train_loss:1.4892514944076538 val_loss1.5306707620620728\n",
            "iteration 746 :train_loss:1.4882175922393799 val_loss1.5296145677566528\n",
            "iteration 747 :train_loss:1.4871869087219238 val_loss1.5285612344741821\n",
            "iteration 748 :train_loss:1.4861592054367065 val_loss1.5275112390518188\n",
            "iteration 749 :train_loss:1.4851346015930176 val_loss1.5264639854431152\n",
            "iteration 750 :train_loss:1.484113097190857 val_loss1.5254199504852295\n",
            "iteration 751 :train_loss:1.4830942153930664 val_loss1.524378776550293\n",
            "iteration 752 :train_loss:1.4820785522460938 val_loss1.5233404636383057\n",
            "iteration 753 :train_loss:1.4810657501220703 val_loss1.5223051309585571\n",
            "iteration 754 :train_loss:1.480055809020996 val_loss1.5212726593017578\n",
            "iteration 755 :train_loss:1.4790490865707397 val_loss1.5202430486679077\n",
            "iteration 756 :train_loss:1.4780449867248535 val_loss1.519216775894165\n",
            "iteration 757 :train_loss:1.4770442247390747 val_loss1.5181928873062134\n",
            "iteration 758 :train_loss:1.476046085357666 val_loss1.51717209815979\n",
            "iteration 759 :train_loss:1.4750508069992065 val_loss1.516154408454895\n",
            "iteration 760 :train_loss:1.4740581512451172 val_loss1.5151389837265015\n",
            "iteration 761 :train_loss:1.4730687141418457 val_loss1.5141266584396362\n",
            "iteration 762 :train_loss:1.4720821380615234 val_loss1.5131171941757202\n",
            "iteration 763 :train_loss:1.4710979461669922 val_loss1.5121104717254639\n",
            "iteration 764 :train_loss:1.4701168537139893 val_loss1.5111064910888672\n",
            "iteration 765 :train_loss:1.4691386222839355 val_loss1.5101056098937988\n",
            "iteration 766 :train_loss:1.468163251876831 val_loss1.5091073513031006\n",
            "iteration 767 :train_loss:1.4671906232833862 val_loss1.508111834526062\n",
            "iteration 768 :train_loss:1.4662206172943115 val_loss1.507118821144104\n",
            "iteration 769 :train_loss:1.465253233909607 val_loss1.5061286687850952\n",
            "iteration 770 :train_loss:1.464288592338562 val_loss1.505141258239746\n",
            "iteration 771 :train_loss:1.4633265733718872 val_loss1.5041565895080566\n",
            "iteration 772 :train_loss:1.4623675346374512 val_loss1.5031744241714478\n",
            "iteration 773 :train_loss:1.4614111185073853 val_loss1.502195119857788\n",
            "iteration 774 :train_loss:1.460457444190979 val_loss1.5012184381484985\n",
            "iteration 775 :train_loss:1.4595059156417847 val_loss1.500244140625\n",
            "iteration 776 :train_loss:1.4585576057434082 val_loss1.4992725849151611\n",
            "iteration 777 :train_loss:1.4576116800308228 val_loss1.498303771018982\n",
            "iteration 778 :train_loss:1.4566683769226074 val_loss1.4973374605178833\n",
            "iteration 779 :train_loss:1.4557278156280518 val_loss1.4963737726211548\n",
            "iteration 780 :train_loss:1.4547897577285767 val_loss1.4954125881195068\n",
            "iteration 781 :train_loss:1.4538542032241821 val_loss1.494454026222229\n",
            "iteration 782 :train_loss:1.4529212713241577 val_loss1.4934979677200317\n",
            "iteration 783 :train_loss:1.4519908428192139 val_loss1.4925445318222046\n",
            "iteration 784 :train_loss:1.4510630369186401 val_loss1.4915934801101685\n",
            "iteration 785 :train_loss:1.4501378536224365 val_loss1.4906450510025024\n",
            "iteration 786 :train_loss:1.449215054512024 val_loss1.4896992444992065\n",
            "iteration 787 :train_loss:1.448294758796692 val_loss1.488755702972412\n",
            "iteration 788 :train_loss:1.4473769664764404 val_loss1.4878147840499878\n",
            "iteration 789 :train_loss:1.4464616775512695 val_loss1.486876130104065\n",
            "iteration 790 :train_loss:1.4455488920211792 val_loss1.4859400987625122\n",
            "iteration 791 :train_loss:1.4446383714675903 val_loss1.4850064516067505\n",
            "iteration 792 :train_loss:1.4437304735183716 val_loss1.4840751886367798\n",
            "iteration 793 :train_loss:1.4428249597549438 val_loss1.4831463098526\n",
            "iteration 794 :train_loss:1.4419218301773071 val_loss1.482219934463501\n",
            "iteration 795 :train_loss:1.4410210847854614 val_loss1.4812957048416138\n",
            "iteration 796 :train_loss:1.4401229619979858 val_loss1.4803739786148071\n",
            "iteration 797 :train_loss:1.439226746559143 val_loss1.479454517364502\n",
            "iteration 798 :train_loss:1.43833327293396 val_loss1.4785374402999878\n",
            "iteration 799 :train_loss:1.4374419450759888 val_loss1.4776229858398438\n",
            "iteration 800 :train_loss:1.4365531206130981 val_loss1.476710557937622\n",
            "iteration 801 :train_loss:1.4356666803359985 val_loss1.4758005142211914\n",
            "iteration 802 :train_loss:1.4347822666168213 val_loss1.4748928546905518\n",
            "iteration 803 :train_loss:1.4339003562927246 val_loss1.473987340927124\n",
            "iteration 804 :train_loss:1.4330207109451294 val_loss1.4730842113494873\n",
            "iteration 805 :train_loss:1.4321434497833252 val_loss1.472183346748352\n",
            "iteration 806 :train_loss:1.4312682151794434 val_loss1.4712846279144287\n",
            "iteration 807 :train_loss:1.430395245552063 val_loss1.4703880548477173\n",
            "iteration 808 :train_loss:1.429524540901184 val_loss1.4694936275482178\n",
            "iteration 809 :train_loss:1.4286562204360962 val_loss1.4686018228530884\n",
            "iteration 810 :train_loss:1.4277899265289307 val_loss1.4677118062973022\n",
            "iteration 811 :train_loss:1.4269261360168457 val_loss1.4668241739273071\n",
            "iteration 812 :train_loss:1.426064133644104 val_loss1.4659383296966553\n",
            "iteration 813 :train_loss:1.4252045154571533 val_loss1.4650551080703735\n",
            "iteration 814 :train_loss:1.424347162246704 val_loss1.4641739130020142\n",
            "iteration 815 :train_loss:1.4234918355941772 val_loss1.4632947444915771\n",
            "iteration 816 :train_loss:1.4226387739181519 val_loss1.4624179601669312\n",
            "iteration 817 :train_loss:1.4217877388000488 val_loss1.4615429639816284\n",
            "iteration 818 :train_loss:1.4209389686584473 val_loss1.4606701135635376\n",
            "iteration 819 :train_loss:1.420092225074768 val_loss1.4597994089126587\n",
            "iteration 820 :train_loss:1.4192476272583008 val_loss1.4589309692382812\n",
            "iteration 821 :train_loss:1.4184050559997559 val_loss1.4580644369125366\n",
            "iteration 822 :train_loss:1.4175646305084229 val_loss1.4571998119354248\n",
            "iteration 823 :train_loss:1.4167263507843018 val_loss1.456337332725525\n",
            "iteration 824 :train_loss:1.415890097618103 val_loss1.4554767608642578\n",
            "iteration 825 :train_loss:1.415055751800537 val_loss1.4546185731887817\n",
            "iteration 826 :train_loss:1.4142236709594727 val_loss1.4537620544433594\n",
            "iteration 827 :train_loss:1.413393497467041 val_loss1.452907681465149\n",
            "iteration 828 :train_loss:1.4125652313232422 val_loss1.4520553350448608\n",
            "iteration 829 :train_loss:1.4117391109466553 val_loss1.4512050151824951\n",
            "iteration 830 :train_loss:1.4109151363372803 val_loss1.4503567218780518\n",
            "iteration 831 :train_loss:1.410093069076538 val_loss1.4495103359222412\n",
            "iteration 832 :train_loss:1.4092729091644287 val_loss1.4486654996871948\n",
            "iteration 833 :train_loss:1.4084547758102417 val_loss1.447823166847229\n",
            "iteration 834 :train_loss:1.407638430595398 val_loss1.446982741355896\n",
            "iteration 835 :train_loss:1.4068243503570557 val_loss1.4461438655853271\n",
            "iteration 836 :train_loss:1.406011939048767 val_loss1.4453070163726807\n",
            "iteration 837 :train_loss:1.4052014350891113 val_loss1.444472074508667\n",
            "iteration 838 :train_loss:1.4043930768966675 val_loss1.4436391592025757\n",
            "iteration 839 :train_loss:1.4035862684249878 val_loss1.442807912826538\n",
            "iteration 840 :train_loss:1.4027817249298096 val_loss1.4419786930084229\n",
            "iteration 841 :train_loss:1.4019789695739746 val_loss1.4411513805389404\n",
            "iteration 842 :train_loss:1.401178240776062 val_loss1.4403259754180908\n",
            "iteration 843 :train_loss:1.4003791809082031 val_loss1.439502239227295\n",
            "iteration 844 :train_loss:1.3995821475982666 val_loss1.4386805295944214\n",
            "iteration 845 :train_loss:1.3987864255905151 val_loss1.437860369682312\n",
            "iteration 846 :train_loss:1.3979932069778442 val_loss1.4370421171188354\n",
            "iteration 847 :train_loss:1.3972012996673584 val_loss1.4362257719039917\n",
            "iteration 848 :train_loss:1.3964115381240845 val_loss1.4354112148284912\n",
            "iteration 849 :train_loss:1.3956235647201538 val_loss1.434598445892334\n",
            "iteration 850 :train_loss:1.3948373794555664 val_loss1.4337873458862305\n",
            "iteration 851 :train_loss:1.3940529823303223 val_loss1.4329780340194702\n",
            "iteration 852 :train_loss:1.3932701349258423 val_loss1.4321705102920532\n",
            "iteration 853 :train_loss:1.3924893140792847 val_loss1.431364893913269\n",
            "iteration 854 :train_loss:1.3917101621627808 val_loss1.4305609464645386\n",
            "iteration 855 :train_loss:1.3909329175949097 val_loss1.4297584295272827\n",
            "iteration 856 :train_loss:1.3901571035385132 val_loss1.4289578199386597\n",
            "iteration 857 :train_loss:1.389383316040039 val_loss1.4281588792800903\n",
            "iteration 858 :train_loss:1.3886109590530396 val_loss1.4273617267608643\n",
            "iteration 859 :train_loss:1.3878403902053833 val_loss1.426566243171692\n",
            "iteration 860 :train_loss:1.3870717287063599 val_loss1.4257725477218628\n",
            "iteration 861 :train_loss:1.3863046169281006 val_loss1.4249805212020874\n",
            "iteration 862 :train_loss:1.385539174079895 val_loss1.4241899251937866\n",
            "iteration 863 :train_loss:1.3847754001617432 val_loss1.4234009981155396\n",
            "iteration 864 :train_loss:1.384013295173645 val_loss1.4226138591766357\n",
            "iteration 865 :train_loss:1.3832529783248901 val_loss1.421828269958496\n",
            "iteration 866 :train_loss:1.3824942111968994 val_loss1.4210444688796997\n",
            "iteration 867 :train_loss:1.3817369937896729 val_loss1.420262098312378\n",
            "iteration 868 :train_loss:1.3809814453125 val_loss1.4194812774658203\n",
            "iteration 869 :train_loss:1.3802274465560913 val_loss1.4187021255493164\n",
            "iteration 870 :train_loss:1.3794749975204468 val_loss1.4179246425628662\n",
            "iteration 871 :train_loss:1.378724455833435 val_loss1.4171487092971802\n",
            "iteration 872 :train_loss:1.3779752254486084 val_loss1.4163743257522583\n",
            "iteration 873 :train_loss:1.3772276639938354 val_loss1.415601134300232\n",
            "iteration 874 :train_loss:1.3764817714691162 val_loss1.414829969406128\n",
            "iteration 875 :train_loss:1.3757373094558716 val_loss1.4140598773956299\n",
            "iteration 876 :train_loss:1.3749945163726807 val_loss1.4132916927337646\n",
            "iteration 877 :train_loss:1.3742530345916748 val_loss1.4125248193740845\n",
            "iteration 878 :train_loss:1.3735132217407227 val_loss1.4117594957351685\n",
            "iteration 879 :train_loss:1.3727748394012451 val_loss1.4109957218170166\n",
            "iteration 880 :train_loss:1.3720381259918213 val_loss1.410233497619629\n",
            "iteration 881 :train_loss:1.371302843093872 val_loss1.4094723463058472\n",
            "iteration 882 :train_loss:1.3705689907073975 val_loss1.4087129831314087\n",
            "iteration 883 :train_loss:1.3698368072509766 val_loss1.407955288887024\n",
            "iteration 884 :train_loss:1.3691061735153198 val_loss1.4071986675262451\n",
            "iteration 885 :train_loss:1.3683768510818481 val_loss1.4064438343048096\n",
            "iteration 886 :train_loss:1.367648959159851 val_loss1.4056901931762695\n",
            "iteration 887 :train_loss:1.3669227361679077 val_loss1.4049382209777832\n",
            "iteration 888 :train_loss:1.3661975860595703 val_loss1.4041873216629028\n",
            "iteration 889 :train_loss:1.3654741048812866 val_loss1.4034380912780762\n",
            "iteration 890 :train_loss:1.3647518157958984 val_loss1.402690052986145\n",
            "iteration 891 :train_loss:1.364031195640564 val_loss1.401943325996399\n",
            "iteration 892 :train_loss:1.363312005996704 val_loss1.401198387145996\n",
            "iteration 893 :train_loss:1.3625942468643188 val_loss1.4004546403884888\n",
            "iteration 894 :train_loss:1.3618779182434082 val_loss1.399712324142456\n",
            "iteration 895 :train_loss:1.3611626625061035 val_loss1.3989713191986084\n",
            "iteration 896 :train_loss:1.3604490756988525 val_loss1.398231863975525\n",
            "iteration 897 :train_loss:1.3597368001937866 val_loss1.3974933624267578\n",
            "iteration 898 :train_loss:1.3590257167816162 val_loss1.3967565298080444\n",
            "iteration 899 :train_loss:1.3583163022994995 val_loss1.3960211277008057\n",
            "iteration 900 :train_loss:1.3576079607009888 val_loss1.3952866792678833\n",
            "iteration 901 :train_loss:1.3569011688232422 val_loss1.3945536613464355\n",
            "iteration 902 :train_loss:1.3561956882476807 val_loss1.3938220739364624\n",
            "iteration 903 :train_loss:1.3554913997650146 val_loss1.3930915594100952\n",
            "iteration 904 :train_loss:1.3547886610031128 val_loss1.3923625946044922\n",
            "iteration 905 :train_loss:1.3540868759155273 val_loss1.3916347026824951\n",
            "iteration 906 :train_loss:1.353386640548706 val_loss1.3909082412719727\n",
            "iteration 907 :train_loss:1.3526875972747803 val_loss1.3901829719543457\n",
            "iteration 908 :train_loss:1.3519898653030396 val_loss1.3894588947296143\n",
            "iteration 909 :train_loss:1.3512933254241943 val_loss1.3887362480163574\n",
            "iteration 910 :train_loss:1.3505983352661133 val_loss1.3880146741867065\n",
            "iteration 911 :train_loss:1.349904179573059 val_loss1.3872942924499512\n",
            "iteration 912 :train_loss:1.349211573600769 val_loss1.3865752220153809\n",
            "iteration 913 :train_loss:1.348520278930664 val_loss1.3858577013015747\n",
            "iteration 914 :train_loss:1.347830057144165 val_loss1.3851408958435059\n",
            "iteration 915 :train_loss:1.347141146659851 val_loss1.3844255208969116\n",
            "iteration 916 :train_loss:1.3464531898498535 val_loss1.383711338043213\n",
            "iteration 917 :train_loss:1.3457667827606201 val_loss1.3829982280731201\n",
            "iteration 918 :train_loss:1.3450815677642822 val_loss1.3822863101959229\n",
            "iteration 919 :train_loss:1.3443973064422607 val_loss1.3815757036209106\n",
            "iteration 920 :train_loss:1.3437143564224243 val_loss1.3808659315109253\n",
            "iteration 921 :train_loss:1.3430324792861938 val_loss1.380157709121704\n",
            "iteration 922 :train_loss:1.3423519134521484 val_loss1.3794505596160889\n",
            "iteration 923 :train_loss:1.341672658920288 val_loss1.3787446022033691\n",
            "iteration 924 :train_loss:1.3409943580627441 val_loss1.3780393600463867\n",
            "iteration 925 :train_loss:1.3403171300888062 val_loss1.3773356676101685\n",
            "iteration 926 :train_loss:1.3396414518356323 val_loss1.3766329288482666\n",
            "iteration 927 :train_loss:1.3389666080474854 val_loss1.3759315013885498\n",
            "iteration 928 :train_loss:1.3382927179336548 val_loss1.3752309083938599\n",
            "iteration 929 :train_loss:1.3376203775405884 val_loss1.374531865119934\n",
            "iteration 930 :train_loss:1.336949110031128 val_loss1.3738332986831665\n",
            "iteration 931 :train_loss:1.3362786769866943 val_loss1.3731361627578735\n",
            "iteration 932 :train_loss:1.3356094360351562 val_loss1.3724400997161865\n",
            "iteration 933 :train_loss:1.3349417448043823 val_loss1.371745228767395\n",
            "iteration 934 :train_loss:1.3342745304107666 val_loss1.3710511922836304\n",
            "iteration 935 :train_loss:1.333608865737915 val_loss1.3703584671020508\n",
            "iteration 936 :train_loss:1.3329440355300903 val_loss1.369666576385498\n",
            "iteration 937 :train_loss:1.3322803974151611 val_loss1.3689757585525513\n",
            "iteration 938 :train_loss:1.331617832183838 val_loss1.3682861328125\n",
            "iteration 939 :train_loss:1.3309563398361206 val_loss1.3675976991653442\n",
            "iteration 940 :train_loss:1.3302958011627197 val_loss1.3669098615646362\n",
            "iteration 941 :train_loss:1.329636573791504 val_loss1.3662233352661133\n",
            "iteration 942 :train_loss:1.3289780616760254 val_loss1.3655376434326172\n",
            "iteration 943 :train_loss:1.3283207416534424 val_loss1.3648529052734375\n",
            "iteration 944 :train_loss:1.3276644945144653 val_loss1.3641693592071533\n",
            "iteration 945 :train_loss:1.3270090818405151 val_loss1.363486886024475\n",
            "iteration 946 :train_loss:1.3263548612594604 val_loss1.3628051280975342\n",
            "iteration 947 :train_loss:1.3257015943527222 val_loss1.3621245622634888\n",
            "iteration 948 :train_loss:1.3250491619110107 val_loss1.3614447116851807\n",
            "iteration 949 :train_loss:1.3243979215621948 val_loss1.360766053199768\n",
            "iteration 950 :train_loss:1.3237475156784058 val_loss1.3600883483886719\n",
            "iteration 951 :train_loss:1.3230983018875122 val_loss1.3594114780426025\n",
            "iteration 952 :train_loss:1.322450041770935 val_loss1.3587356805801392\n",
            "iteration 953 :train_loss:1.3218026161193848 val_loss1.3580607175827026\n",
            "iteration 954 :train_loss:1.3211562633514404 val_loss1.3573867082595825\n",
            "iteration 955 :train_loss:1.3205106258392334 val_loss1.3567134141921997\n",
            "iteration 956 :train_loss:1.3198661804199219 val_loss1.3560411930084229\n",
            "iteration 957 :train_loss:1.3192224502563477 val_loss1.3553699254989624\n",
            "iteration 958 :train_loss:1.318579912185669 val_loss1.3546996116638184\n",
            "iteration 959 :train_loss:1.3179383277893066 val_loss1.354029893875122\n",
            "iteration 960 :train_loss:1.317297339439392 val_loss1.3533613681793213\n",
            "iteration 961 :train_loss:1.3166574239730835 val_loss1.3526935577392578\n",
            "iteration 962 :train_loss:1.3160184621810913 val_loss1.3520267009735107\n",
            "iteration 963 :train_loss:1.3153804540634155 val_loss1.351360559463501\n",
            "iteration 964 :train_loss:1.3147432804107666 val_loss1.3506954908370972\n",
            "iteration 965 :train_loss:1.314107060432434 val_loss1.3500312566757202\n",
            "iteration 966 :train_loss:1.3134716749191284 val_loss1.3493678569793701\n",
            "iteration 967 :train_loss:1.31283700466156 val_loss1.3487052917480469\n",
            "iteration 968 :train_loss:1.312203288078308 val_loss1.348043441772461\n",
            "iteration 969 :train_loss:1.3115705251693726 val_loss1.3473825454711914\n",
            "iteration 970 :train_loss:1.3109385967254639 val_loss1.3467226028442383\n",
            "iteration 971 :train_loss:1.3103076219558716 val_loss1.346063256263733\n",
            "iteration 972 :train_loss:1.3096774816513062 val_loss1.345404863357544\n",
            "iteration 973 :train_loss:1.3090481758117676 val_loss1.3447470664978027\n",
            "iteration 974 :train_loss:1.3084193468093872 val_loss1.3440903425216675\n",
            "iteration 975 :train_loss:1.3077917098999023 val_loss1.34343421459198\n",
            "iteration 976 :train_loss:1.3071647882461548 val_loss1.3427789211273193\n",
            "iteration 977 :train_loss:1.306538701057434 val_loss1.342124342918396\n",
            "iteration 978 :train_loss:1.3059136867523193 val_loss1.3414705991744995\n",
            "iteration 979 :train_loss:1.3052891492843628 val_loss1.3408176898956299\n",
            "iteration 980 :train_loss:1.3046653270721436 val_loss1.3401654958724976\n",
            "iteration 981 :train_loss:1.3040426969528198 val_loss1.3395140171051025\n",
            "iteration 982 :train_loss:1.3034205436706543 val_loss1.338863492012024\n",
            "iteration 983 :train_loss:1.3027993440628052 val_loss1.3382134437561035\n",
            "iteration 984 :train_loss:1.3021787405014038 val_loss1.3375643491744995\n",
            "iteration 985 :train_loss:1.3015590906143188 val_loss1.3369159698486328\n",
            "iteration 986 :train_loss:1.3009400367736816 val_loss1.3362680673599243\n",
            "iteration 987 :train_loss:1.3003218173980713 val_loss1.3356212377548218\n",
            "iteration 988 :train_loss:1.2997044324874878 val_loss1.334975004196167\n",
            "iteration 989 :train_loss:1.2990875244140625 val_loss1.33432936668396\n",
            "iteration 990 :train_loss:1.2984715700149536 val_loss1.3336845636367798\n",
            "iteration 991 :train_loss:1.2978564500808716 val_loss1.3330405950546265\n",
            "iteration 992 :train_loss:1.2972420454025269 val_loss1.332397222518921\n",
            "iteration 993 :train_loss:1.2966282367706299 val_loss1.3317545652389526\n",
            "iteration 994 :train_loss:1.2960151433944702 val_loss1.3311123847961426\n",
            "iteration 995 :train_loss:1.295403003692627 val_loss1.3304710388183594\n",
            "iteration 996 :train_loss:1.294791340827942 val_loss1.3298304080963135\n",
            "iteration 997 :train_loss:1.2941805124282837 val_loss1.3291903734207153\n",
            "iteration 998 :train_loss:1.2935701608657837 val_loss1.3285510540008545\n",
            "iteration 999 :train_loss:1.2929607629776 val_loss1.327912449836731\n",
            "iteration 1000 :train_loss:1.2923520803451538 val_loss1.3272744417190552\n",
            "iteration 1001 :train_loss:1.2917437553405762 val_loss1.3266369104385376\n",
            "iteration 1002 :train_loss:1.291136384010315 val_loss1.3260002136230469\n",
            "iteration 1003 :train_loss:1.2905296087265015 val_loss1.3253642320632935\n",
            "iteration 1004 :train_loss:1.2899235486984253 val_loss1.3247288465499878\n",
            "iteration 1005 :train_loss:1.289318323135376 val_loss1.3240940570831299\n",
            "iteration 1006 :train_loss:1.2887135744094849 val_loss1.3234598636627197\n",
            "iteration 1007 :train_loss:1.288109540939331 val_loss1.3228262662887573\n",
            "iteration 1008 :train_loss:1.2875059843063354 val_loss1.3221933841705322\n",
            "iteration 1009 :train_loss:1.2869032621383667 val_loss1.3215612173080444\n",
            "iteration 1010 :train_loss:1.2863011360168457 val_loss1.3209294080734253\n",
            "iteration 1011 :train_loss:1.285699725151062 val_loss1.320298433303833\n",
            "iteration 1012 :train_loss:1.2850987911224365 val_loss1.3196680545806885\n",
            "iteration 1013 :train_loss:1.2844984531402588 val_loss1.3190380334854126\n",
            "iteration 1014 :train_loss:1.283898949623108 val_loss1.3184090852737427\n",
            "iteration 1015 :train_loss:1.2832999229431152 val_loss1.3177801370620728\n",
            "iteration 1016 :train_loss:1.2827016115188599 val_loss1.3171521425247192\n",
            "iteration 1017 :train_loss:1.2821037769317627 val_loss1.316524624824524\n",
            "iteration 1018 :train_loss:1.2815067768096924 val_loss1.3158975839614868\n",
            "iteration 1019 :train_loss:1.2809100151062012 val_loss1.3152711391448975\n",
            "iteration 1020 :train_loss:1.2803142070770264 val_loss1.314645528793335\n",
            "iteration 1021 :train_loss:1.2797187566757202 val_loss1.3140201568603516\n",
            "iteration 1022 :train_loss:1.2791240215301514 val_loss1.313395619392395\n",
            "iteration 1023 :train_loss:1.2785296440124512 val_loss1.3127714395523071\n",
            "iteration 1024 :train_loss:1.2779361009597778 val_loss1.312147855758667\n",
            "iteration 1025 :train_loss:1.2773430347442627 val_loss1.3115246295928955\n",
            "iteration 1026 :train_loss:1.2767504453659058 val_loss1.3109021186828613\n",
            "iteration 1027 :train_loss:1.2761586904525757 val_loss1.310280203819275\n",
            "iteration 1028 :train_loss:1.2755672931671143 val_loss1.3096587657928467\n",
            "iteration 1029 :train_loss:1.2749764919281006 val_loss1.3090379238128662\n",
            "iteration 1030 :train_loss:1.2743861675262451 val_loss1.3084174394607544\n",
            "iteration 1031 :train_loss:1.273796558380127 val_loss1.3077974319458008\n",
            "iteration 1032 :train_loss:1.273207187652588 val_loss1.307178258895874\n",
            "iteration 1033 :train_loss:1.2726186513900757 val_loss1.3065593242645264\n",
            "iteration 1034 :train_loss:1.2720303535461426 val_loss1.3059407472610474\n",
            "iteration 1035 :train_loss:1.2714428901672363 val_loss1.3053228855133057\n",
            "iteration 1036 :train_loss:1.2708557844161987 val_loss1.3047056198120117\n",
            "iteration 1037 :train_loss:1.2702692747116089 val_loss1.3040885925292969\n",
            "iteration 1038 :train_loss:1.2696832418441772 val_loss1.3034722805023193\n",
            "iteration 1039 :train_loss:1.2690975666046143 val_loss1.302856206893921\n",
            "iteration 1040 :train_loss:1.2685126066207886 val_loss1.3022404909133911\n",
            "iteration 1041 :train_loss:1.267928123474121 val_loss1.301625370979309\n",
            "iteration 1042 :train_loss:1.2673438787460327 val_loss1.3010107278823853\n",
            "iteration 1043 :train_loss:1.2667603492736816 val_loss1.3003966808319092\n",
            "iteration 1044 :train_loss:1.2661772966384888 val_loss1.2997829914093018\n",
            "iteration 1045 :train_loss:1.265594720840454 val_loss1.2991697788238525\n",
            "iteration 1046 :train_loss:1.2650126218795776 val_loss1.2985570430755615\n",
            "iteration 1047 :train_loss:1.2644309997558594 val_loss1.2979446649551392\n",
            "iteration 1048 :train_loss:1.2638496160507202 val_loss1.297332525253296\n",
            "iteration 1049 :train_loss:1.2632689476013184 val_loss1.29672110080719\n",
            "iteration 1050 :train_loss:1.2626887559890747 val_loss1.2961100339889526\n",
            "iteration 1051 :train_loss:1.2621088027954102 val_loss1.2954994440078735\n",
            "iteration 1052 :train_loss:1.261529803276062 val_loss1.2948890924453735\n",
            "iteration 1053 :train_loss:1.2609506845474243 val_loss1.2942794561386108\n",
            "iteration 1054 :train_loss:1.260372281074524 val_loss1.2936701774597168\n",
            "iteration 1055 :train_loss:1.2597943544387817 val_loss1.2930611371994019\n",
            "iteration 1056 :train_loss:1.2592167854309082 val_loss1.292452335357666\n",
            "iteration 1057 :train_loss:1.2586396932601929 val_loss1.2918442487716675\n",
            "iteration 1058 :train_loss:1.2580629587173462 val_loss1.2912365198135376\n",
            "iteration 1059 :train_loss:1.2574864625930786 val_loss1.2906290292739868\n",
            "iteration 1060 :train_loss:1.256910800933838 val_loss1.2900221347808838\n",
            "iteration 1061 :train_loss:1.2563352584838867 val_loss1.2894154787063599\n",
            "iteration 1062 :train_loss:1.2557604312896729 val_loss1.2888094186782837\n",
            "iteration 1063 :train_loss:1.255185842514038 val_loss1.2882033586502075\n",
            "iteration 1064 :train_loss:1.2546114921569824 val_loss1.287597894668579\n",
            "iteration 1065 :train_loss:1.254037857055664 val_loss1.2869929075241089\n",
            "iteration 1066 :train_loss:1.2534643411636353 val_loss1.2863882780075073\n",
            "iteration 1067 :train_loss:1.2528915405273438 val_loss1.285784125328064\n",
            "iteration 1068 :train_loss:1.2523187398910522 val_loss1.2851799726486206\n",
            "iteration 1069 :train_loss:1.251746654510498 val_loss1.2845762968063354\n",
            "iteration 1070 :train_loss:1.2511746883392334 val_loss1.283973217010498\n",
            "iteration 1071 :train_loss:1.250603437423706 val_loss1.283370018005371\n",
            "iteration 1072 :train_loss:1.2500321865081787 val_loss1.2827675342559814\n",
            "iteration 1073 :train_loss:1.2494614124298096 val_loss1.2821654081344604\n",
            "iteration 1074 :train_loss:1.2488913536071777 val_loss1.2815635204315186\n",
            "iteration 1075 :train_loss:1.248321294784546 val_loss1.2809618711471558\n",
            "iteration 1076 :train_loss:1.2477517127990723 val_loss1.2803606986999512\n",
            "iteration 1077 :train_loss:1.2471826076507568 val_loss1.2797598838806152\n",
            "iteration 1078 :train_loss:1.2466137409210205 val_loss1.279159426689148\n",
            "iteration 1079 :train_loss:1.2460452318191528 val_loss1.2785589694976807\n",
            "iteration 1080 :train_loss:1.2454770803451538 val_loss1.2779591083526611\n",
            "iteration 1081 :train_loss:1.244909405708313 val_loss1.2773596048355103\n",
            "iteration 1082 :train_loss:1.2443419694900513 val_loss1.2767603397369385\n",
            "iteration 1083 :train_loss:1.2437748908996582 val_loss1.2761611938476562\n",
            "iteration 1084 :train_loss:1.2432080507278442 val_loss1.2755626440048218\n",
            "iteration 1085 :train_loss:1.242641568183899 val_loss1.2749642133712769\n",
            "iteration 1086 :train_loss:1.2420756816864014 val_loss1.2743661403656006\n",
            "iteration 1087 :train_loss:1.2415097951889038 val_loss1.273768424987793\n",
            "iteration 1088 :train_loss:1.2409443855285645 val_loss1.2731709480285645\n",
            "iteration 1089 :train_loss:1.2403790950775146 val_loss1.2725738286972046\n",
            "iteration 1090 :train_loss:1.2398141622543335 val_loss1.2719769477844238\n",
            "iteration 1091 :train_loss:1.2392498254776 val_loss1.2713804244995117\n",
            "iteration 1092 :train_loss:1.2386856079101562 val_loss1.2707841396331787\n",
            "iteration 1093 :train_loss:1.238121509552002 val_loss1.2701880931854248\n",
            "iteration 1094 :train_loss:1.2375580072402954 val_loss1.26959228515625\n",
            "iteration 1095 :train_loss:1.236994743347168 val_loss1.2689967155456543\n",
            "iteration 1096 :train_loss:1.2364318370819092 val_loss1.2684016227722168\n",
            "iteration 1097 :train_loss:1.2358691692352295 val_loss1.2678067684173584\n",
            "iteration 1098 :train_loss:1.235306739807129 val_loss1.2672119140625\n",
            "iteration 1099 :train_loss:1.2347445487976074 val_loss1.2666176557540894\n",
            "iteration 1100 :train_loss:1.234182596206665 val_loss1.2660233974456787\n",
            "iteration 1101 :train_loss:1.23362135887146 val_loss1.2654293775558472\n",
            "iteration 1102 :train_loss:1.2330597639083862 val_loss1.2648357152938843\n",
            "iteration 1103 :train_loss:1.2324988842010498 val_loss1.2642422914505005\n",
            "iteration 1104 :train_loss:1.2319382429122925 val_loss1.2636492252349854\n",
            "iteration 1105 :train_loss:1.2313778400421143 val_loss1.2630560398101807\n",
            "iteration 1106 :train_loss:1.230817437171936 val_loss1.2624632120132446\n",
            "iteration 1107 :train_loss:1.230257511138916 val_loss1.2618709802627563\n",
            "iteration 1108 :train_loss:1.2296979427337646 val_loss1.2612786293029785\n",
            "iteration 1109 :train_loss:1.2291386127471924 val_loss1.2606865167617798\n",
            "iteration 1110 :train_loss:1.2285794019699097 val_loss1.2600947618484497\n",
            "iteration 1111 :train_loss:1.228020429611206 val_loss1.2595032453536987\n",
            "iteration 1112 :train_loss:1.2274619340896606 val_loss1.2589118480682373\n",
            "iteration 1113 :train_loss:1.2269034385681152 val_loss1.2583205699920654\n",
            "iteration 1114 :train_loss:1.2263453006744385 val_loss1.2577297687530518\n",
            "iteration 1115 :train_loss:1.2257874011993408 val_loss1.257138967514038\n",
            "iteration 1116 :train_loss:1.2252297401428223 val_loss1.256548285484314\n",
            "iteration 1117 :train_loss:1.2246723175048828 val_loss1.255958080291748\n",
            "iteration 1118 :train_loss:1.224115252494812 val_loss1.2553677558898926\n",
            "iteration 1119 :train_loss:1.2235581874847412 val_loss1.2547780275344849\n",
            "iteration 1120 :train_loss:1.2230013608932495 val_loss1.2541881799697876\n",
            "iteration 1121 :train_loss:1.2224448919296265 val_loss1.2535984516143799\n",
            "iteration 1122 :train_loss:1.221888542175293 val_loss1.2530090808868408\n",
            "iteration 1123 :train_loss:1.2213324308395386 val_loss1.2524199485778809\n",
            "iteration 1124 :train_loss:1.2207765579223633 val_loss1.2518310546875\n",
            "iteration 1125 :train_loss:1.220220923423767 val_loss1.2512422800064087\n",
            "iteration 1126 :train_loss:1.2196654081344604 val_loss1.250653624534607\n",
            "iteration 1127 :train_loss:1.219110131263733 val_loss1.2500650882720947\n",
            "iteration 1128 :train_loss:1.2185550928115845 val_loss1.249476671218872\n",
            "iteration 1129 :train_loss:1.2180002927780151 val_loss1.248888611793518\n",
            "iteration 1130 :train_loss:1.217445731163025 val_loss1.2483006715774536\n",
            "iteration 1131 :train_loss:1.2168910503387451 val_loss1.2477129697799683\n",
            "iteration 1132 :train_loss:1.2163368463516235 val_loss1.247125267982483\n",
            "iteration 1133 :train_loss:1.215782880783081 val_loss1.2465378046035767\n",
            "iteration 1134 :train_loss:1.2152289152145386 val_loss1.24595046043396\n",
            "iteration 1135 :train_loss:1.2146751880645752 val_loss1.2453633546829224\n",
            "iteration 1136 :train_loss:1.214121699333191 val_loss1.2447763681411743\n",
            "iteration 1137 :train_loss:1.2135683298110962 val_loss1.2441895008087158\n",
            "iteration 1138 :train_loss:1.2130153179168701 val_loss1.243602991104126\n",
            "iteration 1139 :train_loss:1.2124624252319336 val_loss1.2430164813995361\n",
            "iteration 1140 :train_loss:1.2119094133377075 val_loss1.2424300909042358\n",
            "iteration 1141 :train_loss:1.2113568782806396 val_loss1.241843819618225\n",
            "iteration 1142 :train_loss:1.2108043432235718 val_loss1.2412577867507935\n",
            "iteration 1143 :train_loss:1.210252046585083 val_loss1.240671992301941\n",
            "iteration 1144 :train_loss:1.2096999883651733 val_loss1.2400861978530884\n",
            "iteration 1145 :train_loss:1.2091479301452637 val_loss1.2395005226135254\n",
            "iteration 1146 :train_loss:1.2085963487625122 val_loss1.2389148473739624\n",
            "iteration 1147 :train_loss:1.2080447673797607 val_loss1.2383297681808472\n",
            "iteration 1148 :train_loss:1.2074934244155884 val_loss1.2377444505691528\n",
            "iteration 1149 :train_loss:1.2069419622421265 val_loss1.2371593713760376\n",
            "iteration 1150 :train_loss:1.2063909769058228 val_loss1.2365741729736328\n",
            "iteration 1151 :train_loss:1.205839991569519 val_loss1.2359894514083862\n",
            "iteration 1152 :train_loss:1.2052892446517944 val_loss1.2354047298431396\n",
            "iteration 1153 :train_loss:1.2047386169433594 val_loss1.2348198890686035\n",
            "iteration 1154 :train_loss:1.2041881084442139 val_loss1.234235405921936\n",
            "iteration 1155 :train_loss:1.203637719154358 val_loss1.2336509227752686\n",
            "iteration 1156 :train_loss:1.2030874490737915 val_loss1.2330666780471802\n",
            "iteration 1157 :train_loss:1.2025372982025146 val_loss1.2324824333190918\n",
            "iteration 1158 :train_loss:1.2019872665405273 val_loss1.231898307800293\n",
            "iteration 1159 :train_loss:1.2014375925064087 val_loss1.2313143014907837\n",
            "iteration 1160 :train_loss:1.2008877992630005 val_loss1.230730414390564\n",
            "iteration 1161 :train_loss:1.200338363647461 val_loss1.2301466464996338\n",
            "iteration 1162 :train_loss:1.1997889280319214 val_loss1.2295628786087036\n",
            "iteration 1163 :train_loss:1.1992396116256714 val_loss1.228979468345642\n",
            "iteration 1164 :train_loss:1.198690414428711 val_loss1.2283958196640015\n",
            "iteration 1165 :train_loss:1.1981414556503296 val_loss1.2278125286102295\n",
            "iteration 1166 :train_loss:1.1975924968719482 val_loss1.2272292375564575\n",
            "iteration 1167 :train_loss:1.197043776512146 val_loss1.226646065711975\n",
            "iteration 1168 :train_loss:1.1964950561523438 val_loss1.2260630130767822\n",
            "iteration 1169 :train_loss:1.1959465742111206 val_loss1.2254799604415894\n",
            "iteration 1170 :train_loss:1.1953980922698975 val_loss1.224897027015686\n",
            "iteration 1171 :train_loss:1.1948498487472534 val_loss1.2243140935897827\n",
            "iteration 1172 :train_loss:1.1943016052246094 val_loss1.223731517791748\n",
            "iteration 1173 :train_loss:1.1937533617019653 val_loss1.2231488227844238\n",
            "iteration 1174 :train_loss:1.19320547580719 val_loss1.22256600856781\n",
            "iteration 1175 :train_loss:1.1926575899124146 val_loss1.221983551979065\n",
            "iteration 1176 :train_loss:1.1921097040176392 val_loss1.2214009761810303\n",
            "iteration 1177 :train_loss:1.1915620565414429 val_loss1.2208184003829956\n",
            "iteration 1178 :train_loss:1.1910145282745361 val_loss1.2202363014221191\n",
            "iteration 1179 :train_loss:1.190467119216919 val_loss1.2196540832519531\n",
            "iteration 1180 :train_loss:1.1899198293685913 val_loss1.2190717458724976\n",
            "iteration 1181 :train_loss:1.1893725395202637 val_loss1.2184897661209106\n",
            "iteration 1182 :train_loss:1.1888253688812256 val_loss1.2179076671600342\n",
            "iteration 1183 :train_loss:1.1882784366607666 val_loss1.2173256874084473\n",
            "iteration 1184 :train_loss:1.187731385231018 val_loss1.21674382686615\n",
            "iteration 1185 :train_loss:1.187184453010559 val_loss1.216162085533142\n",
            "iteration 1186 :train_loss:1.1866377592086792 val_loss1.2155802249908447\n",
            "iteration 1187 :train_loss:1.1860911846160889 val_loss1.2149986028671265\n",
            "iteration 1188 :train_loss:1.185544490814209 val_loss1.2144169807434082\n",
            "iteration 1189 :train_loss:1.1849980354309082 val_loss1.2138354778289795\n",
            "iteration 1190 :train_loss:1.1844514608383179 val_loss1.2132539749145508\n",
            "iteration 1191 :train_loss:1.1839051246643066 val_loss1.212672472000122\n",
            "iteration 1192 :train_loss:1.183358907699585 val_loss1.2120909690856934\n",
            "iteration 1193 :train_loss:1.1828126907348633 val_loss1.2115098237991333\n",
            "iteration 1194 :train_loss:1.1822665929794312 val_loss1.2109283208847046\n",
            "iteration 1195 :train_loss:1.181720495223999 val_loss1.210347056388855\n",
            "iteration 1196 :train_loss:1.181174635887146 val_loss1.209765911102295\n",
            "iteration 1197 :train_loss:1.1806286573410034 val_loss1.2091847658157349\n",
            "iteration 1198 :train_loss:1.18008291721344 val_loss1.2086035013198853\n",
            "iteration 1199 :train_loss:1.1795371770858765 val_loss1.2080224752426147\n",
            "iteration 1200 :train_loss:1.178991436958313 val_loss1.2074414491653442\n",
            "iteration 1201 :train_loss:1.1784456968307495 val_loss1.2068603038787842\n",
            "iteration 1202 :train_loss:1.1779001951217651 val_loss1.2062793970108032\n",
            "iteration 1203 :train_loss:1.1773546934127808 val_loss1.2056983709335327\n",
            "iteration 1204 :train_loss:1.1768091917037964 val_loss1.2051174640655518\n",
            "iteration 1205 :train_loss:1.1762639284133911 val_loss1.2045366764068604\n",
            "iteration 1206 :train_loss:1.1757186651229858 val_loss1.2039557695388794\n",
            "iteration 1207 :train_loss:1.175173282623291 val_loss1.203374981880188\n",
            "iteration 1208 :train_loss:1.1746281385421753 val_loss1.2027943134307861\n",
            "iteration 1209 :train_loss:1.1740831136703491 val_loss1.2022134065628052\n",
            "iteration 1210 :train_loss:1.1735379695892334 val_loss1.2016327381134033\n",
            "iteration 1211 :train_loss:1.1729930639266968 val_loss1.2010520696640015\n",
            "iteration 1212 :train_loss:1.1724480390548706 val_loss1.2004715204238892\n",
            "iteration 1213 :train_loss:1.171903133392334 val_loss1.1998909711837769\n",
            "iteration 1214 :train_loss:1.1713584661483765 val_loss1.1993101835250854\n",
            "iteration 1215 :train_loss:1.1708136796951294 val_loss1.1987297534942627\n",
            "iteration 1216 :train_loss:1.1702688932418823 val_loss1.1981492042541504\n",
            "iteration 1217 :train_loss:1.1697242259979248 val_loss1.1975687742233276\n",
            "iteration 1218 :train_loss:1.1691796779632568 val_loss1.1969883441925049\n",
            "iteration 1219 :train_loss:1.1686351299285889 val_loss1.1964079141616821\n",
            "iteration 1220 :train_loss:1.168090581893921 val_loss1.1958274841308594\n",
            "iteration 1221 :train_loss:1.167546272277832 val_loss1.1952471733093262\n",
            "iteration 1222 :train_loss:1.1670018434524536 val_loss1.194666862487793\n",
            "iteration 1223 :train_loss:1.1664574146270752 val_loss1.1940866708755493\n",
            "iteration 1224 :train_loss:1.1659131050109863 val_loss1.1935062408447266\n",
            "iteration 1225 :train_loss:1.1653687953948975 val_loss1.192926049232483\n",
            "iteration 1226 :train_loss:1.1648244857788086 val_loss1.1923458576202393\n",
            "iteration 1227 :train_loss:1.1642804145812988 val_loss1.1917656660079956\n",
            "iteration 1228 :train_loss:1.1637362241744995 val_loss1.1911853551864624\n",
            "iteration 1229 :train_loss:1.1631920337677002 val_loss1.1906052827835083\n",
            "iteration 1230 :train_loss:1.16264808177948 val_loss1.1900250911712646\n",
            "iteration 1231 :train_loss:1.1621040105819702 val_loss1.189444899559021\n",
            "iteration 1232 :train_loss:1.1615599393844604 val_loss1.188864827156067\n",
            "iteration 1233 :train_loss:1.1610158681869507 val_loss1.1882845163345337\n",
            "iteration 1234 :train_loss:1.1604719161987305 val_loss1.1877044439315796\n",
            "iteration 1235 :train_loss:1.1599280834197998 val_loss1.187124252319336\n",
            "iteration 1236 :train_loss:1.1593841314315796 val_loss1.1865442991256714\n",
            "iteration 1237 :train_loss:1.158840298652649 val_loss1.1859641075134277\n",
            "iteration 1238 :train_loss:1.1582963466644287 val_loss1.1853840351104736\n",
            "iteration 1239 :train_loss:1.157752513885498 val_loss1.18480384349823\n",
            "iteration 1240 :train_loss:1.1572086811065674 val_loss1.1842238903045654\n",
            "iteration 1241 :train_loss:1.1566648483276367 val_loss1.1836438179016113\n",
            "iteration 1242 :train_loss:1.1561211347579956 val_loss1.1830636262893677\n",
            "iteration 1243 :train_loss:1.155577540397644 val_loss1.1824836730957031\n",
            "iteration 1244 :train_loss:1.1550337076187134 val_loss1.1819034814834595\n",
            "iteration 1245 :train_loss:1.1544901132583618 val_loss1.1813234090805054\n",
            "iteration 1246 :train_loss:1.1539463996887207 val_loss1.1807433366775513\n",
            "iteration 1247 :train_loss:1.1534028053283691 val_loss1.1801633834838867\n",
            "iteration 1248 :train_loss:1.1528593301773071 val_loss1.1795833110809326\n",
            "iteration 1249 :train_loss:1.152315616607666 val_loss1.1790032386779785\n",
            "iteration 1250 :train_loss:1.1517720222473145 val_loss1.178423285484314\n",
            "iteration 1251 :train_loss:1.151228427886963 val_loss1.1778432130813599\n",
            "iteration 1252 :train_loss:1.1506848335266113 val_loss1.1772631406784058\n",
            "iteration 1253 :train_loss:1.1501413583755493 val_loss1.1766831874847412\n",
            "iteration 1254 :train_loss:1.1495976448059082 val_loss1.176103115081787\n",
            "iteration 1255 :train_loss:1.1490541696548462 val_loss1.175523042678833\n",
            "iteration 1256 :train_loss:1.1485105752944946 val_loss1.174942970275879\n",
            "iteration 1257 :train_loss:1.147966980934143 val_loss1.1743627786636353\n",
            "iteration 1258 :train_loss:1.147423505783081 val_loss1.1737827062606812\n",
            "iteration 1259 :train_loss:1.1468799114227295 val_loss1.173202633857727\n",
            "iteration 1260 :train_loss:1.146336555480957 val_loss1.1726224422454834\n",
            "iteration 1261 :train_loss:1.145793080329895 val_loss1.1720424890518188\n",
            "iteration 1262 :train_loss:1.1452494859695435 val_loss1.1714622974395752\n",
            "iteration 1263 :train_loss:1.1447060108184814 val_loss1.170882225036621\n",
            "iteration 1264 :train_loss:1.144162654876709 val_loss1.1703020334243774\n",
            "iteration 1265 :train_loss:1.143619179725647 val_loss1.1697219610214233\n",
            "iteration 1266 :train_loss:1.143075704574585 val_loss1.1691417694091797\n",
            "iteration 1267 :train_loss:1.1425323486328125 val_loss1.1685616970062256\n",
            "iteration 1268 :train_loss:1.14198899269104 val_loss1.1679816246032715\n",
            "iteration 1269 :train_loss:1.141445517539978 val_loss1.1674014329910278\n",
            "iteration 1270 :train_loss:1.1409021615982056 val_loss1.1668212413787842\n",
            "iteration 1271 :train_loss:1.140358805656433 val_loss1.1662410497665405\n",
            "iteration 1272 :train_loss:1.139815330505371 val_loss1.1656609773635864\n",
            "iteration 1273 :train_loss:1.1392719745635986 val_loss1.1650809049606323\n",
            "iteration 1274 :train_loss:1.1387287378311157 val_loss1.1645007133483887\n",
            "iteration 1275 :train_loss:1.1381852626800537 val_loss1.1639204025268555\n",
            "iteration 1276 :train_loss:1.1376420259475708 val_loss1.1633402109146118\n",
            "iteration 1277 :train_loss:1.1370985507965088 val_loss1.1627599000930786\n",
            "iteration 1278 :train_loss:1.1365551948547363 val_loss1.1621795892715454\n",
            "iteration 1279 :train_loss:1.1360118389129639 val_loss1.1615993976593018\n",
            "iteration 1280 :train_loss:1.1354684829711914 val_loss1.1610190868377686\n",
            "iteration 1281 :train_loss:1.134925127029419 val_loss1.1604387760162354\n",
            "iteration 1282 :train_loss:1.134381890296936 val_loss1.1598583459854126\n",
            "iteration 1283 :train_loss:1.133838415145874 val_loss1.159278154373169\n",
            "iteration 1284 :train_loss:1.1332951784133911 val_loss1.1586978435516357\n",
            "iteration 1285 :train_loss:1.1327519416809082 val_loss1.158117413520813\n",
            "iteration 1286 :train_loss:1.1322085857391357 val_loss1.1575371026992798\n",
            "iteration 1287 :train_loss:1.1316653490066528 val_loss1.156956672668457\n",
            "iteration 1288 :train_loss:1.1311219930648804 val_loss1.1563763618469238\n",
            "iteration 1289 :train_loss:1.1305787563323975 val_loss1.155795931816101\n",
            "iteration 1290 :train_loss:1.1300355195999146 val_loss1.1552153825759888\n",
            "iteration 1291 :train_loss:1.129492163658142 val_loss1.154634952545166\n",
            "iteration 1292 :train_loss:1.1289489269256592 val_loss1.1540545225143433\n",
            "iteration 1293 :train_loss:1.1284055709838867 val_loss1.1534740924835205\n",
            "iteration 1294 :train_loss:1.1278624534606934 val_loss1.1528935432434082\n",
            "iteration 1295 :train_loss:1.1273192167282104 val_loss1.1523131132125854\n",
            "iteration 1296 :train_loss:1.126775860786438 val_loss1.1517325639724731\n",
            "iteration 1297 :train_loss:1.1262327432632446 val_loss1.1511521339416504\n",
            "iteration 1298 :train_loss:1.1256896257400513 val_loss1.150571584701538\n",
            "iteration 1299 :train_loss:1.1251463890075684 val_loss1.1499911546707153\n",
            "iteration 1300 :train_loss:1.1246031522750854 val_loss1.1494107246398926\n",
            "iteration 1301 :train_loss:1.1240599155426025 val_loss1.1488301753997803\n",
            "iteration 1302 :train_loss:1.1235167980194092 val_loss1.148249626159668\n",
            "iteration 1303 :train_loss:1.1229736804962158 val_loss1.1476691961288452\n",
            "iteration 1304 :train_loss:1.1224303245544434 val_loss1.147088646888733\n",
            "iteration 1305 :train_loss:1.12188720703125 val_loss1.1465080976486206\n",
            "iteration 1306 :train_loss:1.1213440895080566 val_loss1.1459276676177979\n",
            "iteration 1307 :train_loss:1.1208008527755737 val_loss1.145347237586975\n",
            "iteration 1308 :train_loss:1.1202577352523804 val_loss1.1447666883468628\n",
            "iteration 1309 :train_loss:1.119714617729187 val_loss1.1441861391067505\n",
            "iteration 1310 :train_loss:1.1191715002059937 val_loss1.1436055898666382\n",
            "iteration 1311 :train_loss:1.1186285018920898 val_loss1.1430250406265259\n",
            "iteration 1312 :train_loss:1.1180853843688965 val_loss1.1424444913864136\n",
            "iteration 1313 :train_loss:1.1175422668457031 val_loss1.1418640613555908\n",
            "iteration 1314 :train_loss:1.1169990301132202 val_loss1.1412835121154785\n",
            "iteration 1315 :train_loss:1.1164560317993164 val_loss1.1407029628753662\n",
            "iteration 1316 :train_loss:1.115912914276123 val_loss1.140122413635254\n",
            "iteration 1317 :train_loss:1.1153697967529297 val_loss1.1395419836044312\n",
            "iteration 1318 :train_loss:1.1148267984390259 val_loss1.1389614343643188\n",
            "iteration 1319 :train_loss:1.1142836809158325 val_loss1.138380765914917\n",
            "iteration 1320 :train_loss:1.1137405633926392 val_loss1.1378000974655151\n",
            "iteration 1321 :train_loss:1.1131975650787354 val_loss1.1372195482254028\n",
            "iteration 1322 :train_loss:1.1126543283462524 val_loss1.136638879776001\n",
            "iteration 1323 :train_loss:1.1121113300323486 val_loss1.1360583305358887\n",
            "iteration 1324 :train_loss:1.1115682125091553 val_loss1.1354777812957764\n",
            "iteration 1325 :train_loss:1.1110252141952515 val_loss1.1348971128463745\n",
            "iteration 1326 :train_loss:1.1104822158813477 val_loss1.1343164443969727\n",
            "iteration 1327 :train_loss:1.1099390983581543 val_loss1.1337358951568604\n",
            "iteration 1328 :train_loss:1.1093961000442505 val_loss1.133155345916748\n",
            "iteration 1329 :train_loss:1.1088531017303467 val_loss1.1325746774673462\n",
            "iteration 1330 :train_loss:1.1083102226257324 val_loss1.1319941282272339\n",
            "iteration 1331 :train_loss:1.1077672243118286 val_loss1.1314136981964111\n",
            "iteration 1332 :train_loss:1.1072243452072144 val_loss1.1308331489562988\n",
            "iteration 1333 :train_loss:1.1066814661026 val_loss1.1302523612976074\n",
            "iteration 1334 :train_loss:1.1061385869979858 val_loss1.1296719312667847\n",
            "iteration 1335 :train_loss:1.105595588684082 val_loss1.1290912628173828\n",
            "iteration 1336 :train_loss:1.1050528287887573 val_loss1.1285107135772705\n",
            "iteration 1337 :train_loss:1.1045098304748535 val_loss1.1279300451278687\n",
            "iteration 1338 :train_loss:1.1039670705795288 val_loss1.1273494958877563\n",
            "iteration 1339 :train_loss:1.1034241914749146 val_loss1.1267688274383545\n",
            "iteration 1340 :train_loss:1.1028814315795898 val_loss1.1261882781982422\n",
            "iteration 1341 :train_loss:1.1023385524749756 val_loss1.1256077289581299\n",
            "iteration 1342 :train_loss:1.1017959117889404 val_loss1.1250271797180176\n",
            "iteration 1343 :train_loss:1.1012529134750366 val_loss1.1244466304779053\n",
            "iteration 1344 :train_loss:1.1007102727890015 val_loss1.123866081237793\n",
            "iteration 1345 :train_loss:1.1001673936843872 val_loss1.1232855319976807\n",
            "iteration 1346 :train_loss:1.099624752998352 val_loss1.1227049827575684\n",
            "iteration 1347 :train_loss:1.0990818738937378 val_loss1.122124433517456\n",
            "iteration 1348 :train_loss:1.0985393524169922 val_loss1.1215440034866333\n",
            "iteration 1349 :train_loss:1.0979965925216675 val_loss1.120963454246521\n",
            "iteration 1350 :train_loss:1.0974539518356323 val_loss1.1203830242156982\n",
            "iteration 1351 :train_loss:1.0969111919403076 val_loss1.119802474975586\n",
            "iteration 1352 :train_loss:1.096368670463562 val_loss1.1192220449447632\n",
            "iteration 1353 :train_loss:1.0958259105682373 val_loss1.11864173412323\n",
            "iteration 1354 :train_loss:1.0952833890914917 val_loss1.1180611848831177\n",
            "iteration 1355 :train_loss:1.094740867614746 val_loss1.117480754852295\n",
            "iteration 1356 :train_loss:1.094198226928711 val_loss1.1169003248214722\n",
            "iteration 1357 :train_loss:1.0936557054519653 val_loss1.1163198947906494\n",
            "iteration 1358 :train_loss:1.0931130647659302 val_loss1.1157394647598267\n",
            "iteration 1359 :train_loss:1.0925706624984741 val_loss1.1151591539382935\n",
            "iteration 1360 :train_loss:1.0920283794403076 val_loss1.1145788431167603\n",
            "iteration 1361 :train_loss:1.0914859771728516 val_loss1.1139984130859375\n",
            "iteration 1362 :train_loss:1.0909435749053955 val_loss1.1134181022644043\n",
            "iteration 1363 :train_loss:1.09040105342865 val_loss1.112837791442871\n",
            "iteration 1364 :train_loss:1.089858889579773 val_loss1.1122575998306274\n",
            "iteration 1365 :train_loss:1.0893166065216064 val_loss1.1116772890090942\n",
            "iteration 1366 :train_loss:1.0887744426727295 val_loss1.111096978187561\n",
            "iteration 1367 :train_loss:1.088232159614563 val_loss1.1105166673660278\n",
            "iteration 1368 :train_loss:1.087689995765686 val_loss1.1099365949630737\n",
            "iteration 1369 :train_loss:1.0871479511260986 val_loss1.10935640335083\n",
            "iteration 1370 :train_loss:1.0866057872772217 val_loss1.1087760925292969\n",
            "iteration 1371 :train_loss:1.0860638618469238 val_loss1.1081961393356323\n",
            "iteration 1372 :train_loss:1.0855216979980469 val_loss1.1076160669326782\n",
            "iteration 1373 :train_loss:1.084979772567749 val_loss1.1070361137390137\n",
            "iteration 1374 :train_loss:1.0844378471374512 val_loss1.10645592212677\n",
            "iteration 1375 :train_loss:1.0838960409164429 val_loss1.1058759689331055\n",
            "iteration 1376 :train_loss:1.083354115486145 val_loss1.105296015739441\n",
            "iteration 1377 :train_loss:1.0828123092651367 val_loss1.1047160625457764\n",
            "iteration 1378 :train_loss:1.082270622253418 val_loss1.1041362285614014\n",
            "iteration 1379 :train_loss:1.0817289352416992 val_loss1.1035563945770264\n",
            "iteration 1380 :train_loss:1.0811872482299805 val_loss1.1029765605926514\n",
            "iteration 1381 :train_loss:1.0806455612182617 val_loss1.1023967266082764\n",
            "iteration 1382 :train_loss:1.0801039934158325 val_loss1.1018171310424805\n",
            "iteration 1383 :train_loss:1.0795625448226929 val_loss1.101237416267395\n",
            "iteration 1384 :train_loss:1.0790210962295532 val_loss1.1006579399108887\n",
            "iteration 1385 :train_loss:1.078479528427124 val_loss1.1000784635543823\n",
            "iteration 1386 :train_loss:1.077938199043274 val_loss1.099498987197876\n",
            "iteration 1387 :train_loss:1.0773967504501343 val_loss1.0989195108413696\n",
            "iteration 1388 :train_loss:1.0768554210662842 val_loss1.0983401536941528\n",
            "iteration 1389 :train_loss:1.0763142108917236 val_loss1.097760796546936\n",
            "iteration 1390 :train_loss:1.0757731199264526 val_loss1.0971814393997192\n",
            "iteration 1391 :train_loss:1.075231909751892 val_loss1.096602201461792\n",
            "iteration 1392 :train_loss:1.0746906995773315 val_loss1.0960230827331543\n",
            "iteration 1393 :train_loss:1.07414972782135 val_loss1.0954437255859375\n",
            "iteration 1394 :train_loss:1.0736087560653687 val_loss1.094864845275879\n",
            "iteration 1395 :train_loss:1.0730677843093872 val_loss1.0942857265472412\n",
            "iteration 1396 :train_loss:1.0725268125534058 val_loss1.093706727027893\n",
            "iteration 1397 :train_loss:1.071986198425293 val_loss1.093127727508545\n",
            "iteration 1398 :train_loss:1.071445345878601 val_loss1.0925488471984863\n",
            "iteration 1399 :train_loss:1.0709046125411987 val_loss1.0919699668884277\n",
            "iteration 1400 :train_loss:1.0703638792037964 val_loss1.0913912057876587\n",
            "iteration 1401 :train_loss:1.0698233842849731 val_loss1.0908124446868896\n",
            "iteration 1402 :train_loss:1.06928288936615 val_loss1.0902336835861206\n",
            "iteration 1403 :train_loss:1.0687423944473267 val_loss1.0896550416946411\n",
            "iteration 1404 :train_loss:1.0682018995285034 val_loss1.0890763998031616\n",
            "iteration 1405 :train_loss:1.0676616430282593 val_loss1.0884978771209717\n",
            "iteration 1406 :train_loss:1.0671212673187256 val_loss1.0879194736480713\n",
            "iteration 1407 :train_loss:1.066581130027771 val_loss1.087341070175171\n",
            "iteration 1408 :train_loss:1.0660408735275269 val_loss1.08676278591156\n",
            "iteration 1409 :train_loss:1.0655009746551514 val_loss1.0861846208572388\n",
            "iteration 1410 :train_loss:1.0649608373641968 val_loss1.085606575012207\n",
            "iteration 1411 :train_loss:1.0644209384918213 val_loss1.0850284099578857\n",
            "iteration 1412 :train_loss:1.0638811588287354 val_loss1.0844504833221436\n",
            "iteration 1413 :train_loss:1.0633412599563599 val_loss1.0838725566864014\n",
            "iteration 1414 :train_loss:1.0628015995025635 val_loss1.0832946300506592\n",
            "iteration 1415 :train_loss:1.062261939048767 val_loss1.0827168226242065\n",
            "iteration 1416 :train_loss:1.0617223978042603 val_loss1.0821391344070435\n",
            "iteration 1417 :train_loss:1.0611827373504639 val_loss1.0815613269805908\n",
            "iteration 1418 :train_loss:1.0606433153152466 val_loss1.0809837579727173\n",
            "iteration 1419 :train_loss:1.0601040124893188 val_loss1.0804063081741333\n",
            "iteration 1420 :train_loss:1.0595647096633911 val_loss1.0798288583755493\n",
            "iteration 1421 :train_loss:1.0590254068374634 val_loss1.0792514085769653\n",
            "iteration 1422 :train_loss:1.0584863424301147 val_loss1.078674077987671\n",
            "iteration 1423 :train_loss:1.0579471588134766 val_loss1.078096866607666\n",
            "iteration 1424 :train_loss:1.0574082136154175 val_loss1.0775196552276611\n",
            "iteration 1425 :train_loss:1.0568692684173584 val_loss1.0769426822662354\n",
            "iteration 1426 :train_loss:1.0563304424285889 val_loss1.0763657093048096\n",
            "iteration 1427 :train_loss:1.0557917356491089 val_loss1.0757887363433838\n",
            "iteration 1428 :train_loss:1.055253028869629 val_loss1.075212001800537\n",
            "iteration 1429 :train_loss:1.0547144412994385 val_loss1.07463538646698\n",
            "iteration 1430 :train_loss:1.0541759729385376 val_loss1.0740586519241333\n",
            "iteration 1431 :train_loss:1.0536375045776367 val_loss1.0734820365905762\n",
            "iteration 1432 :train_loss:1.053099274635315 val_loss1.0729056596755981\n",
            "iteration 1433 :train_loss:1.0525610446929932 val_loss1.0723294019699097\n",
            "iteration 1434 :train_loss:1.052022933959961 val_loss1.0717531442642212\n",
            "iteration 1435 :train_loss:1.0514848232269287 val_loss1.0711770057678223\n",
            "iteration 1436 :train_loss:1.0509469509124756 val_loss1.0706011056900024\n",
            "iteration 1437 :train_loss:1.050409197807312 val_loss1.070025086402893\n",
            "iteration 1438 :train_loss:1.0498713254928589 val_loss1.0694493055343628\n",
            "iteration 1439 :train_loss:1.0493338108062744 val_loss1.068873405456543\n",
            "iteration 1440 :train_loss:1.0487961769104004 val_loss1.0682978630065918\n",
            "iteration 1441 :train_loss:1.048258662223816 val_loss1.0677223205566406\n",
            "iteration 1442 :train_loss:1.0477213859558105 val_loss1.0671467781066895\n",
            "iteration 1443 :train_loss:1.0471841096878052 val_loss1.0665713548660278\n",
            "iteration 1444 :train_loss:1.046647071838379 val_loss1.0659961700439453\n",
            "iteration 1445 :train_loss:1.0461100339889526 val_loss1.0654211044311523\n",
            "iteration 1446 :train_loss:1.0455729961395264 val_loss1.0648459196090698\n",
            "iteration 1447 :train_loss:1.0450361967086792 val_loss1.0642712116241455\n",
            "iteration 1448 :train_loss:1.0444995164871216 val_loss1.063696265220642\n",
            "iteration 1449 :train_loss:1.0439629554748535 val_loss1.0631215572357178\n",
            "iteration 1450 :train_loss:1.0434263944625854 val_loss1.062546968460083\n",
            "iteration 1451 :train_loss:1.0428900718688965 val_loss1.0619724988937378\n",
            "iteration 1452 :train_loss:1.0423539876937866 val_loss1.0613981485366821\n",
            "iteration 1453 :train_loss:1.0418176651000977 val_loss1.060823917388916\n",
            "iteration 1454 :train_loss:1.0412817001342773 val_loss1.060249924659729\n",
            "iteration 1455 :train_loss:1.040745735168457 val_loss1.0596758127212524\n",
            "iteration 1456 :train_loss:1.0402100086212158 val_loss1.059101939201355\n",
            "iteration 1457 :train_loss:1.0396742820739746 val_loss1.0585284233093262\n",
            "iteration 1458 :train_loss:1.0391385555267334 val_loss1.0579546689987183\n",
            "iteration 1459 :train_loss:1.0386031866073608 val_loss1.0573813915252686\n",
            "iteration 1460 :train_loss:1.0380678176879883 val_loss1.0568079948425293\n",
            "iteration 1461 :train_loss:1.0375324487686157 val_loss1.0562348365783691\n",
            "iteration 1462 :train_loss:1.0369974374771118 val_loss1.055661678314209\n",
            "iteration 1463 :train_loss:1.0364623069763184 val_loss1.0550886392593384\n",
            "iteration 1464 :train_loss:1.035927414894104 val_loss1.0545158386230469\n",
            "iteration 1465 :train_loss:1.0353927612304688 val_loss1.0539432764053345\n",
            "iteration 1466 :train_loss:1.0348581075668335 val_loss1.0533705949783325\n",
            "iteration 1467 :train_loss:1.0343235731124878 val_loss1.0527982711791992\n",
            "iteration 1468 :train_loss:1.0337892770767212 val_loss1.052226185798645\n",
            "iteration 1469 :train_loss:1.0332551002502441 val_loss1.0516539812088013\n",
            "iteration 1470 :train_loss:1.0327210426330566 val_loss1.0510821342468262\n",
            "iteration 1471 :train_loss:1.0321871042251587 val_loss1.050510287284851\n",
            "iteration 1472 :train_loss:1.0316532850265503 val_loss1.049938678741455\n",
            "iteration 1473 :train_loss:1.031119704246521 val_loss1.0493671894073486\n",
            "iteration 1474 :train_loss:1.0305862426757812 val_loss1.0487958192825317\n",
            "iteration 1475 :train_loss:1.030052900314331 val_loss1.0482245683670044\n",
            "iteration 1476 :train_loss:1.0295196771621704 val_loss1.0476535558700562\n",
            "iteration 1477 :train_loss:1.0289866924285889 val_loss1.047082543373108\n",
            "iteration 1478 :train_loss:1.0284538269042969 val_loss1.0465118885040283\n",
            "iteration 1479 :train_loss:1.027921199798584 val_loss1.0459413528442383\n",
            "iteration 1480 :train_loss:1.0273886919021606 val_loss1.0453709363937378\n",
            "iteration 1481 :train_loss:1.0268563032150269 val_loss1.0448006391525269\n",
            "iteration 1482 :train_loss:1.0263241529464722 val_loss1.044230580329895\n",
            "iteration 1483 :train_loss:1.0257920026779175 val_loss1.0436606407165527\n",
            "iteration 1484 :train_loss:1.025260090827942 val_loss1.0430908203125\n",
            "iteration 1485 :train_loss:1.0247282981872559 val_loss1.0425212383270264\n",
            "iteration 1486 :train_loss:1.024196743965149 val_loss1.0419517755508423\n",
            "iteration 1487 :train_loss:1.0236653089523315 val_loss1.0413824319839478\n",
            "iteration 1488 :train_loss:1.0231341123580933 val_loss1.0408134460449219\n",
            "iteration 1489 :train_loss:1.022602915763855 val_loss1.0402445793151855\n",
            "iteration 1490 :train_loss:1.0220719575881958 val_loss1.0396759510040283\n",
            "iteration 1491 :train_loss:1.0215412378311157 val_loss1.039107322692871\n",
            "iteration 1492 :train_loss:1.0210106372833252 val_loss1.0385390520095825\n",
            "iteration 1493 :train_loss:1.0204802751541138 val_loss1.0379709005355835\n",
            "iteration 1494 :train_loss:1.0199499130249023 val_loss1.037402868270874\n",
            "iteration 1495 :train_loss:1.01941978931427 val_loss1.036834955215454\n",
            "iteration 1496 :train_loss:1.0188899040222168 val_loss1.0362672805786133\n",
            "iteration 1497 :train_loss:1.0183601379394531 val_loss1.0356998443603516\n",
            "iteration 1498 :train_loss:1.0178306102752686 val_loss1.0351325273513794\n",
            "iteration 1499 :train_loss:1.0173012018203735 val_loss1.0345654487609863\n",
            "iteration 1500 :train_loss:1.016771912574768 val_loss1.0339984893798828\n",
            "iteration 1501 :train_loss:1.0162429809570312 val_loss1.0334316492080688\n",
            "iteration 1502 :train_loss:1.0157139301300049 val_loss1.032865047454834\n",
            "iteration 1503 :train_loss:1.0151852369308472 val_loss1.0322985649108887\n",
            "iteration 1504 :train_loss:1.014656662940979 val_loss1.031732439994812\n",
            "iteration 1505 :train_loss:1.0141284465789795 val_loss1.031166434288025\n",
            "iteration 1506 :train_loss:1.0136001110076904 val_loss1.0306005477905273\n",
            "iteration 1507 :train_loss:1.0130720138549805 val_loss1.0300348997116089\n",
            "iteration 1508 :train_loss:1.0125441551208496 val_loss1.029469609260559\n",
            "iteration 1509 :train_loss:1.0120166540145874 val_loss1.0289043188095093\n",
            "iteration 1510 :train_loss:1.011488914489746 val_loss1.0283392667770386\n",
            "iteration 1511 :train_loss:1.010961651802063 val_loss1.027774453163147\n",
            "iteration 1512 :train_loss:1.010434627532959 val_loss1.027209758758545\n",
            "iteration 1513 :train_loss:1.009907603263855 val_loss1.0266454219818115\n",
            "iteration 1514 :train_loss:1.0093806982040405 val_loss1.0260810852050781\n",
            "iteration 1515 :train_loss:1.0088541507720947 val_loss1.0255171060562134\n",
            "iteration 1516 :train_loss:1.008327841758728 val_loss1.0249533653259277\n",
            "iteration 1517 :train_loss:1.0078017711639404 val_loss1.0243897438049316\n",
            "iteration 1518 :train_loss:1.0072758197784424 val_loss1.0238263607025146\n",
            "iteration 1519 :train_loss:1.0067501068115234 val_loss1.0232630968093872\n",
            "iteration 1520 :train_loss:1.0062243938446045 val_loss1.022700309753418\n",
            "iteration 1521 :train_loss:1.0056991577148438 val_loss1.0221376419067383\n",
            "iteration 1522 :train_loss:1.0051740407943726 val_loss1.0215753316879272\n",
            "iteration 1523 :train_loss:1.0046491622924805 val_loss1.0210131406784058\n",
            "iteration 1524 :train_loss:1.0041242837905884 val_loss1.0204511880874634\n",
            "iteration 1525 :train_loss:1.003599762916565 val_loss1.0198894739151\n",
            "iteration 1526 :train_loss:1.0030755996704102 val_loss1.0193277597427368\n",
            "iteration 1527 :train_loss:1.0025514364242554 val_loss1.0187665224075317\n",
            "iteration 1528 :train_loss:1.0020273923873901 val_loss1.0182055234909058\n",
            "iteration 1529 :train_loss:1.001503825187683 val_loss1.0176447629928589\n",
            "iteration 1530 :train_loss:1.0009804964065552 val_loss1.0170842409133911\n",
            "iteration 1531 :train_loss:1.0004572868347168 val_loss1.0165239572525024\n",
            "iteration 1532 :train_loss:0.999934196472168 val_loss1.0159640312194824\n",
            "iteration 1533 :train_loss:0.9994114637374878 val_loss1.0154041051864624\n",
            "iteration 1534 :train_loss:0.9988888502120972 val_loss1.0148446559906006\n",
            "iteration 1535 :train_loss:0.9983665347099304 val_loss1.0142852067947388\n",
            "iteration 1536 :train_loss:0.9978444576263428 val_loss1.0137262344360352\n",
            "iteration 1537 :train_loss:0.9973225593566895 val_loss1.0131672620773315\n",
            "iteration 1538 :train_loss:0.9968007802963257 val_loss1.0126087665557861\n",
            "iteration 1539 :train_loss:0.9962793588638306 val_loss1.0120501518249512\n",
            "iteration 1540 :train_loss:0.9957581758499146 val_loss1.0114922523498535\n",
            "iteration 1541 :train_loss:0.9952371716499329 val_loss1.0109344720840454\n",
            "iteration 1542 :train_loss:0.9947163462638855 val_loss1.0103768110275269\n",
            "iteration 1543 :train_loss:0.9941959381103516 val_loss1.009819507598877\n",
            "iteration 1544 :train_loss:0.9936755895614624 val_loss1.0092624425888062\n",
            "iteration 1545 :train_loss:0.9931555986404419 val_loss1.0087056159973145\n",
            "iteration 1546 :train_loss:0.9926357865333557 val_loss1.0081491470336914\n",
            "iteration 1547 :train_loss:0.9921162724494934 val_loss1.007592797279358\n",
            "iteration 1548 :train_loss:0.9915969371795654 val_loss1.007036805152893\n",
            "iteration 1549 :train_loss:0.9910778999328613 val_loss1.0064810514450073\n",
            "iteration 1550 :train_loss:0.9905591011047363 val_loss1.0059255361557007\n",
            "iteration 1551 :train_loss:0.9900404810905457 val_loss1.0053703784942627\n",
            "iteration 1552 :train_loss:0.9895222187042236 val_loss1.0048154592514038\n",
            "iteration 1553 :train_loss:0.9890040755271912 val_loss1.004260778427124\n",
            "iteration 1554 :train_loss:0.9884861707687378 val_loss1.003706455230713\n",
            "iteration 1555 :train_loss:0.9879686236381531 val_loss1.0031522512435913\n",
            "iteration 1556 :train_loss:0.9874513745307922 val_loss1.002598524093628\n",
            "iteration 1557 :train_loss:0.9869343042373657 val_loss1.002044916152954\n",
            "iteration 1558 :train_loss:0.9864175319671631 val_loss1.001491665840149\n",
            "iteration 1559 :train_loss:0.9859009385108948 val_loss1.0009386539459229\n",
            "iteration 1560 :train_loss:0.9853845834732056 val_loss1.0003859996795654\n",
            "iteration 1561 :train_loss:0.9848686456680298 val_loss0.9998335242271423\n",
            "iteration 1562 :train_loss:0.9843528866767883 val_loss0.9992815256118774\n",
            "iteration 1563 :train_loss:0.9838373064994812 val_loss0.9987295866012573\n",
            "iteration 1564 :train_loss:0.983322024345398 val_loss0.9981780052185059\n",
            "iteration 1565 :train_loss:0.9828070402145386 val_loss0.997626781463623\n",
            "iteration 1566 :train_loss:0.9822923541069031 val_loss0.9970757961273193\n",
            "iteration 1567 :train_loss:0.9817777872085571 val_loss0.9965251684188843\n",
            "iteration 1568 :train_loss:0.9812635779380798 val_loss0.9959748387336731\n",
            "iteration 1569 :train_loss:0.9807496070861816 val_loss0.9954246878623962\n",
            "iteration 1570 :train_loss:0.9802359342575073 val_loss0.994874894618988\n",
            "iteration 1571 :train_loss:0.9797225594520569 val_loss0.9943254590034485\n",
            "iteration 1572 :train_loss:0.9792094826698303 val_loss0.993776261806488\n",
            "iteration 1573 :train_loss:0.9786967039108276 val_loss0.9932274222373962\n",
            "iteration 1574 :train_loss:0.978184163570404 val_loss0.9926789402961731\n",
            "iteration 1575 :train_loss:0.9776719808578491 val_loss0.992130696773529\n",
            "iteration 1576 :train_loss:0.9771599173545837 val_loss0.9915827512741089\n",
            "iteration 1577 :train_loss:0.976648211479187 val_loss0.9910351634025574\n",
            "iteration 1578 :train_loss:0.9761368632316589 val_loss0.9904879331588745\n",
            "iteration 1579 :train_loss:0.9756256937980652 val_loss0.9899410605430603\n",
            "iteration 1580 :train_loss:0.9751148223876953 val_loss0.98939448595047\n",
            "iteration 1581 :train_loss:0.9746042490005493 val_loss0.988848090171814\n",
            "iteration 1582 :train_loss:0.9740940928459167 val_loss0.9883022308349609\n",
            "iteration 1583 :train_loss:0.9735841155052185 val_loss0.9877564907073975\n",
            "iteration 1584 :train_loss:0.9730744361877441 val_loss0.9872111082077026\n",
            "iteration 1585 :train_loss:0.9725649952888489 val_loss0.9866661429405212\n",
            "iteration 1586 :train_loss:0.9720558524131775 val_loss0.9861214756965637\n",
            "iteration 1587 :train_loss:0.97154700756073 val_loss0.9855771064758301\n",
            "iteration 1588 :train_loss:0.9710384607315063 val_loss0.9850329756736755\n",
            "iteration 1589 :train_loss:0.9705302119255066 val_loss0.9844891428947449\n",
            "iteration 1590 :train_loss:0.9700221419334412 val_loss0.9839457273483276\n",
            "iteration 1591 :train_loss:0.9695145487785339 val_loss0.9834026098251343\n",
            "iteration 1592 :train_loss:0.9690070748329163 val_loss0.9828599095344543\n",
            "iteration 1593 :train_loss:0.9684998393058777 val_loss0.982317328453064\n",
            "iteration 1594 :train_loss:0.9679930210113525 val_loss0.981775164604187\n",
            "iteration 1595 :train_loss:0.9674865007400513 val_loss0.9812334179878235\n",
            "iteration 1596 :train_loss:0.9669803380966187 val_loss0.9806919097900391\n",
            "iteration 1597 :train_loss:0.9664744138717651 val_loss0.9801508188247681\n",
            "iteration 1598 :train_loss:0.9659687280654907 val_loss0.979610025882721\n",
            "iteration 1599 :train_loss:0.9654635190963745 val_loss0.9790695309638977\n",
            "iteration 1600 :train_loss:0.9649584889411926 val_loss0.9785293936729431\n",
            "iteration 1601 :train_loss:0.9644538164138794 val_loss0.9779895544052124\n",
            "iteration 1602 :train_loss:0.9639493823051453 val_loss0.9774501919746399\n",
            "iteration 1603 :train_loss:0.9634453058242798 val_loss0.9769110679626465\n",
            "iteration 1604 :train_loss:0.962941586971283 val_loss0.9763723611831665\n",
            "iteration 1605 :train_loss:0.9624382257461548 val_loss0.9758340120315552\n",
            "iteration 1606 :train_loss:0.9619351625442505 val_loss0.9752959609031677\n",
            "iteration 1607 :train_loss:0.9614323973655701 val_loss0.9747584462165833\n",
            "iteration 1608 :train_loss:0.9609298706054688 val_loss0.9742210507392883\n",
            "iteration 1609 :train_loss:0.9604278206825256 val_loss0.9736842513084412\n",
            "iteration 1610 :train_loss:0.9599261283874512 val_loss0.9731475710868835\n",
            "iteration 1611 :train_loss:0.9594245553016663 val_loss0.9726113677024841\n",
            "iteration 1612 :train_loss:0.95892333984375 val_loss0.9720755815505981\n",
            "iteration 1613 :train_loss:0.9584224224090576 val_loss0.971540093421936\n",
            "iteration 1614 :train_loss:0.9579219222068787 val_loss0.971004843711853\n",
            "iteration 1615 :train_loss:0.9574217200279236 val_loss0.9704701900482178\n",
            "iteration 1616 :train_loss:0.9569219350814819 val_loss0.9699356555938721\n",
            "iteration 1617 :train_loss:0.9564223289489746 val_loss0.9694015979766846\n",
            "iteration 1618 :train_loss:0.9559230804443359 val_loss0.9688678979873657\n",
            "iteration 1619 :train_loss:0.9554242491722107 val_loss0.9683346152305603\n",
            "iteration 1620 :train_loss:0.9549257755279541 val_loss0.9678017497062683\n",
            "iteration 1621 :train_loss:0.9544274210929871 val_loss0.9672691226005554\n",
            "iteration 1622 :train_loss:0.953929603099823 val_loss0.966736912727356\n",
            "iteration 1623 :train_loss:0.9534321427345276 val_loss0.9662050604820251\n",
            "iteration 1624 :train_loss:0.9529349207878113 val_loss0.9656737446784973\n",
            "iteration 1625 :train_loss:0.9524381160736084 val_loss0.9651426076889038\n",
            "iteration 1626 :train_loss:0.951941728591919 val_loss0.9646118879318237\n",
            "iteration 1627 :train_loss:0.951445460319519 val_loss0.9640815854072571\n",
            "iteration 1628 :train_loss:0.9509495496749878 val_loss0.9635517001152039\n",
            "iteration 1629 :train_loss:0.9504541158676147 val_loss0.9630221724510193\n",
            "iteration 1630 :train_loss:0.9499590992927551 val_loss0.962492823600769\n",
            "iteration 1631 :train_loss:0.9494643211364746 val_loss0.9619641304016113\n",
            "iteration 1632 :train_loss:0.9489697813987732 val_loss0.9614356160163879\n",
            "iteration 1633 :train_loss:0.94847571849823 val_loss0.9609076380729675\n",
            "iteration 1634 :train_loss:0.9479819536209106 val_loss0.9603797793388367\n",
            "iteration 1635 :train_loss:0.9474886655807495 val_loss0.9598526358604431\n",
            "iteration 1636 :train_loss:0.9469956159591675 val_loss0.9593257308006287\n",
            "iteration 1637 :train_loss:0.9465029835700989 val_loss0.9587992429733276\n",
            "iteration 1638 :train_loss:0.9460107684135437 val_loss0.9582732915878296\n",
            "iteration 1639 :train_loss:0.9455187320709229 val_loss0.9577475786209106\n",
            "iteration 1640 :train_loss:0.9450271129608154 val_loss0.9572224020957947\n",
            "iteration 1641 :train_loss:0.9445359110832214 val_loss0.9566975235939026\n",
            "iteration 1642 :train_loss:0.9440451264381409 val_loss0.9561730027198792\n",
            "iteration 1643 :train_loss:0.9435545206069946 val_loss0.9556488394737244\n",
            "iteration 1644 :train_loss:0.9430643916130066 val_loss0.955125093460083\n",
            "iteration 1645 :train_loss:0.942574679851532 val_loss0.9546018242835999\n",
            "iteration 1646 :train_loss:0.9420851469039917 val_loss0.9540790319442749\n",
            "iteration 1647 :train_loss:0.941596269607544 val_loss0.953556478023529\n",
            "iteration 1648 :train_loss:0.9411076307296753 val_loss0.9530344605445862\n",
            "iteration 1649 :train_loss:0.9406192302703857 val_loss0.9525128602981567\n",
            "iteration 1650 :train_loss:0.940131425857544 val_loss0.9519915580749512\n",
            "iteration 1651 :train_loss:0.9396439790725708 val_loss0.9514707326889038\n",
            "iteration 1652 :train_loss:0.9391567707061768 val_loss0.9509504437446594\n",
            "iteration 1653 :train_loss:0.9386699795722961 val_loss0.9504303932189941\n",
            "iteration 1654 :train_loss:0.9381837248802185 val_loss0.9499109387397766\n",
            "iteration 1655 :train_loss:0.9376977682113647 val_loss0.949391782283783\n",
            "iteration 1656 :train_loss:0.9372120499610901 val_loss0.9488731026649475\n",
            "iteration 1657 :train_loss:0.9367268681526184 val_loss0.9483548402786255\n",
            "iteration 1658 :train_loss:0.9362421035766602 val_loss0.9478369951248169\n",
            "iteration 1659 :train_loss:0.935757577419281 val_loss0.9473196268081665\n",
            "iteration 1660 :train_loss:0.9352735877037048 val_loss0.9468026161193848\n",
            "iteration 1661 :train_loss:0.9347898960113525 val_loss0.9462862014770508\n",
            "iteration 1662 :train_loss:0.9343066215515137 val_loss0.9457700252532959\n",
            "iteration 1663 :train_loss:0.9338237643241882 val_loss0.9452542662620544\n",
            "iteration 1664 :train_loss:0.9333412647247314 val_loss0.9447391033172607\n",
            "iteration 1665 :train_loss:0.9328592419624329 val_loss0.9442243576049805\n",
            "iteration 1666 :train_loss:0.9323775172233582 val_loss0.9437099099159241\n",
            "iteration 1667 :train_loss:0.9318960905075073 val_loss0.9431959986686707\n",
            "iteration 1668 :train_loss:0.9314151406288147 val_loss0.9426824450492859\n",
            "iteration 1669 :train_loss:0.9309346079826355 val_loss0.9421694278717041\n",
            "iteration 1670 :train_loss:0.9304545521736145 val_loss0.9416567087173462\n",
            "iteration 1671 :train_loss:0.9299747347831726 val_loss0.9411445260047913\n",
            "iteration 1672 :train_loss:0.9294955134391785 val_loss0.9406327605247498\n",
            "iteration 1673 :train_loss:0.9290164113044739 val_loss0.9401213526725769\n",
            "iteration 1674 :train_loss:0.928537905216217 val_loss0.9396105408668518\n",
            "iteration 1675 :train_loss:0.9280598163604736 val_loss0.9391001462936401\n",
            "iteration 1676 :train_loss:0.9275820255279541 val_loss0.9385899901390076\n",
            "iteration 1677 :train_loss:0.9271047711372375 val_loss0.9380804300308228\n",
            "iteration 1678 :train_loss:0.9266278147697449 val_loss0.9375712275505066\n",
            "iteration 1679 :train_loss:0.9261513352394104 val_loss0.9370625615119934\n",
            "iteration 1680 :train_loss:0.925675094127655 val_loss0.9365543127059937\n",
            "iteration 1681 :train_loss:0.9251994490623474 val_loss0.9360464215278625\n",
            "iteration 1682 :train_loss:0.9247241020202637 val_loss0.9355390667915344\n",
            "iteration 1683 :train_loss:0.9242493510246277 val_loss0.935032069683075\n",
            "iteration 1684 :train_loss:0.9237748384475708 val_loss0.9345256686210632\n",
            "iteration 1685 :train_loss:0.9233008027076721 val_loss0.9340196251869202\n",
            "iteration 1686 :train_loss:0.9228270649909973 val_loss0.9335140585899353\n",
            "iteration 1687 :train_loss:0.9223538637161255 val_loss0.9330089092254639\n",
            "iteration 1688 :train_loss:0.9218810796737671 val_loss0.9325041770935059\n",
            "iteration 1689 :train_loss:0.9214087128639221 val_loss0.9320000410079956\n",
            "iteration 1690 :train_loss:0.920936644077301 val_loss0.9314963221549988\n",
            "iteration 1691 :train_loss:0.9204651117324829 val_loss0.9309929609298706\n",
            "iteration 1692 :train_loss:0.9199939966201782 val_loss0.9304900169372559\n",
            "iteration 1693 :train_loss:0.9195231795310974 val_loss0.9299875497817993\n",
            "iteration 1694 :train_loss:0.9190530180931091 val_loss0.9294857382774353\n",
            "iteration 1695 :train_loss:0.9185832142829895 val_loss0.9289842247962952\n",
            "iteration 1696 :train_loss:0.9181138277053833 val_loss0.9284831881523132\n",
            "iteration 1697 :train_loss:0.9176448583602905 val_loss0.9279826879501343\n",
            "iteration 1698 :train_loss:0.9171763062477112 val_loss0.9274826049804688\n",
            "iteration 1699 :train_loss:0.9167082905769348 val_loss0.9269830584526062\n",
            "iteration 1700 :train_loss:0.9162405729293823 val_loss0.9264841079711914\n",
            "iteration 1701 :train_loss:0.915773332118988 val_loss0.9259854555130005\n",
            "iteration 1702 :train_loss:0.9153066277503967 val_loss0.9254872798919678\n",
            "iteration 1703 :train_loss:0.9148402214050293 val_loss0.924989640712738\n",
            "iteration 1704 :train_loss:0.9143744111061096 val_loss0.9244924783706665\n",
            "iteration 1705 :train_loss:0.9139089584350586 val_loss0.923995852470398\n",
            "iteration 1706 :train_loss:0.9134438037872314 val_loss0.923499584197998\n",
            "iteration 1707 :train_loss:0.9129791855812073 val_loss0.9230039119720459\n",
            "iteration 1708 :train_loss:0.9125149250030518 val_loss0.922508716583252\n",
            "iteration 1709 :train_loss:0.9120512008666992 val_loss0.9220139384269714\n",
            "iteration 1710 :train_loss:0.9115878939628601 val_loss0.9215196967124939\n",
            "iteration 1711 :train_loss:0.9111250638961792 val_loss0.9210258722305298\n",
            "iteration 1712 :train_loss:0.9106626510620117 val_loss0.9205325245857239\n",
            "iteration 1713 :train_loss:0.9102006554603577 val_loss0.9200397729873657\n",
            "iteration 1714 :train_loss:0.9097391366958618 val_loss0.9195473790168762\n",
            "iteration 1715 :train_loss:0.9092779755592346 val_loss0.9190555214881897\n",
            "iteration 1716 :train_loss:0.9088174104690552 val_loss0.9185641407966614\n",
            "iteration 1717 :train_loss:0.9083572030067444 val_loss0.918073296546936\n",
            "iteration 1718 :train_loss:0.9078974723815918 val_loss0.9175828695297241\n",
            "iteration 1719 :train_loss:0.9074382185935974 val_loss0.9170929789543152\n",
            "iteration 1720 :train_loss:0.9069792628288269 val_loss0.9166035652160645\n",
            "iteration 1721 :train_loss:0.9065209031105042 val_loss0.9161146283149719\n",
            "iteration 1722 :train_loss:0.9060627818107605 val_loss0.9156261682510376\n",
            "iteration 1723 :train_loss:0.9056053161621094 val_loss0.9151381254196167\n",
            "iteration 1724 :train_loss:0.9051482081413269 val_loss0.9146507382392883\n",
            "iteration 1725 :train_loss:0.9046916365623474 val_loss0.9141636490821838\n",
            "iteration 1726 :train_loss:0.9042354226112366 val_loss0.9136772155761719\n",
            "iteration 1727 :train_loss:0.9037797451019287 val_loss0.9131911993026733\n",
            "iteration 1728 :train_loss:0.9033244848251343 val_loss0.9127057790756226\n",
            "iteration 1729 :train_loss:0.9028696417808533 val_loss0.9122206568717957\n",
            "iteration 1730 :train_loss:0.9024152755737305 val_loss0.9117361903190613\n",
            "iteration 1731 :train_loss:0.9019614458084106 val_loss0.9112521409988403\n",
            "iteration 1732 :train_loss:0.9015079140663147 val_loss0.9107686281204224\n",
            "iteration 1733 :train_loss:0.9010550379753113 val_loss0.9102855920791626\n",
            "iteration 1734 :train_loss:0.9006025195121765 val_loss0.9098030924797058\n",
            "iteration 1735 :train_loss:0.9001504778862 val_loss0.9093210697174072\n",
            "iteration 1736 :train_loss:0.8996989727020264 val_loss0.9088395237922668\n",
            "iteration 1737 :train_loss:0.8992478251457214 val_loss0.908358633518219\n",
            "iteration 1738 :train_loss:0.8987970948219299 val_loss0.9078781604766846\n",
            "iteration 1739 :train_loss:0.8983469009399414 val_loss0.9073981642723083\n",
            "iteration 1740 :train_loss:0.8978971838951111 val_loss0.9069186449050903\n",
            "iteration 1741 :train_loss:0.8974478840827942 val_loss0.9064397811889648\n",
            "iteration 1742 :train_loss:0.8969990611076355 val_loss0.9059613347053528\n",
            "iteration 1743 :train_loss:0.896550714969635 val_loss0.9054834246635437\n",
            "iteration 1744 :train_loss:0.8961029648780823 val_loss0.905005931854248\n",
            "iteration 1745 :train_loss:0.8956555128097534 val_loss0.9045290946960449\n",
            "iteration 1746 :train_loss:0.8952085971832275 val_loss0.9040526747703552\n",
            "iteration 1747 :train_loss:0.8947621583938599 val_loss0.9035767912864685\n",
            "iteration 1748 :train_loss:0.8943161368370056 val_loss0.90310138463974\n",
            "iteration 1749 :train_loss:0.8938705921173096 val_loss0.9026263952255249\n",
            "iteration 1750 :train_loss:0.8934255242347717 val_loss0.9021521210670471\n",
            "iteration 1751 :train_loss:0.8929808735847473 val_loss0.9016782641410828\n",
            "iteration 1752 :train_loss:0.8925366401672363 val_loss0.9012048840522766\n",
            "iteration 1753 :train_loss:0.8920929431915283 val_loss0.9007320404052734\n",
            "iteration 1754 :train_loss:0.8916497230529785 val_loss0.9002596735954285\n",
            "iteration 1755 :train_loss:0.8912069201469421 val_loss0.8997879028320312\n",
            "iteration 1756 :train_loss:0.890764594078064 val_loss0.8993165493011475\n",
            "iteration 1757 :train_loss:0.8903228640556335 val_loss0.8988457918167114\n",
            "iteration 1758 :train_loss:0.889881432056427 val_loss0.8983754515647888\n",
            "iteration 1759 :train_loss:0.8894404768943787 val_loss0.8979056477546692\n",
            "iteration 1760 :train_loss:0.8890001773834229 val_loss0.8974363803863525\n",
            "iteration 1761 :train_loss:0.8885601162910461 val_loss0.8969676494598389\n",
            "iteration 1762 :train_loss:0.8881208300590515 val_loss0.8964994549751282\n",
            "iteration 1763 :train_loss:0.8876819014549255 val_loss0.8960317373275757\n",
            "iteration 1764 :train_loss:0.8872432708740234 val_loss0.8955644965171814\n",
            "iteration 1765 :train_loss:0.8868053555488586 val_loss0.8950977921485901\n",
            "iteration 1766 :train_loss:0.8863678574562073 val_loss0.8946316242218018\n",
            "iteration 1767 :train_loss:0.8859307765960693 val_loss0.8941658735275269\n",
            "iteration 1768 :train_loss:0.8854941725730896 val_loss0.893700897693634\n",
            "iteration 1769 :train_loss:0.8850581645965576 val_loss0.8932362794876099\n",
            "iteration 1770 :train_loss:0.8846226334571838 val_loss0.8927723169326782\n",
            "iteration 1771 :train_loss:0.8841874599456787 val_loss0.8923087120056152\n",
            "iteration 1772 :train_loss:0.8837528824806213 val_loss0.8918457627296448\n",
            "iteration 1773 :train_loss:0.8833187818527222 val_loss0.8913832902908325\n",
            "iteration 1774 :train_loss:0.882885217666626 val_loss0.8909214735031128\n",
            "iteration 1775 :train_loss:0.8824521899223328 val_loss0.8904601335525513\n",
            "iteration 1776 :train_loss:0.8820196390151978 val_loss0.8899993300437927\n",
            "iteration 1777 :train_loss:0.8815874457359314 val_loss0.8895390629768372\n",
            "iteration 1778 :train_loss:0.881155788898468 val_loss0.8890793323516846\n",
            "iteration 1779 :train_loss:0.8807246685028076 val_loss0.8886200785636902\n",
            "iteration 1780 :train_loss:0.8802940249443054 val_loss0.8881614208221436\n",
            "iteration 1781 :train_loss:0.8798638582229614 val_loss0.8877032995223999\n",
            "iteration 1782 :train_loss:0.8794341683387756 val_loss0.8872456550598145\n",
            "iteration 1783 :train_loss:0.8790050745010376 val_loss0.8867886662483215\n",
            "iteration 1784 :train_loss:0.8785765171051025 val_loss0.8863321542739868\n",
            "iteration 1785 :train_loss:0.8781482577323914 val_loss0.8858761787414551\n",
            "iteration 1786 :train_loss:0.8777205944061279 val_loss0.8854207992553711\n",
            "iteration 1787 :train_loss:0.8772934675216675 val_loss0.8849659562110901\n",
            "iteration 1788 :train_loss:0.8768666982650757 val_loss0.8845115900039673\n",
            "iteration 1789 :train_loss:0.8764405250549316 val_loss0.8840577602386475\n",
            "iteration 1790 :train_loss:0.876014769077301 val_loss0.8836044669151306\n",
            "iteration 1791 :train_loss:0.8755895495414734 val_loss0.8831517100334167\n",
            "iteration 1792 :train_loss:0.875164806842804 val_loss0.8826995491981506\n",
            "iteration 1793 :train_loss:0.8747406601905823 val_loss0.882247805595398\n",
            "iteration 1794 :train_loss:0.8743169903755188 val_loss0.8817967772483826\n",
            "iteration 1795 :train_loss:0.8738936185836792 val_loss0.8813463449478149\n",
            "iteration 1796 :train_loss:0.8734709620475769 val_loss0.880896270275116\n",
            "iteration 1797 :train_loss:0.873048722743988 val_loss0.8804469108581543\n",
            "iteration 1798 :train_loss:0.8726270794868469 val_loss0.8799981474876404\n",
            "iteration 1799 :train_loss:0.8722057938575745 val_loss0.8795498013496399\n",
            "iteration 1800 :train_loss:0.8717851638793945 val_loss0.8791019916534424\n",
            "iteration 1801 :train_loss:0.8713650107383728 val_loss0.8786547780036926\n",
            "iteration 1802 :train_loss:0.8709453344345093 val_loss0.8782081604003906\n",
            "iteration 1803 :train_loss:0.8705261945724487 val_loss0.8777620792388916\n",
            "iteration 1804 :train_loss:0.8701075911521912 val_loss0.8773165345191956\n",
            "iteration 1805 :train_loss:0.8696893453598022 val_loss0.8768715262413025\n",
            "iteration 1806 :train_loss:0.8692717552185059 val_loss0.8764270544052124\n",
            "iteration 1807 :train_loss:0.8688545227050781 val_loss0.8759832382202148\n",
            "iteration 1808 :train_loss:0.8684379458427429 val_loss0.8755399584770203\n",
            "iteration 1809 :train_loss:0.8680217862129211 val_loss0.8750972151756287\n",
            "iteration 1810 :train_loss:0.8676062822341919 val_loss0.8746550679206848\n",
            "iteration 1811 :train_loss:0.8671911358833313 val_loss0.8742133378982544\n",
            "iteration 1812 :train_loss:0.8667764663696289 val_loss0.8737722039222717\n",
            "iteration 1813 :train_loss:0.866362452507019 val_loss0.8733316659927368\n",
            "iteration 1814 :train_loss:0.8659488558769226 val_loss0.8728916049003601\n",
            "iteration 1815 :train_loss:0.8655357360839844 val_loss0.8724520802497864\n",
            "iteration 1816 :train_loss:0.8651232123374939 val_loss0.8720132112503052\n",
            "iteration 1817 :train_loss:0.8647112250328064 val_loss0.871574878692627\n",
            "iteration 1818 :train_loss:0.8642996549606323 val_loss0.8711371421813965\n",
            "iteration 1819 :train_loss:0.863888680934906 val_loss0.8706997632980347\n",
            "iteration 1820 :train_loss:0.8634782433509827 val_loss0.8702630996704102\n",
            "iteration 1821 :train_loss:0.8630681037902832 val_loss0.8698270320892334\n",
            "iteration 1822 :train_loss:0.862658679485321 val_loss0.8693914413452148\n",
            "iteration 1823 :train_loss:0.8622496724128723 val_loss0.8689563870429993\n",
            "iteration 1824 :train_loss:0.8618412017822266 val_loss0.8685219287872314\n",
            "iteration 1825 :train_loss:0.8614332675933838 val_loss0.8680880665779114\n",
            "iteration 1826 :train_loss:0.8610257506370544 val_loss0.8676546812057495\n",
            "iteration 1827 :train_loss:0.8606187701225281 val_loss0.8672218322753906\n",
            "iteration 1828 :train_loss:0.8602123856544495 val_loss0.8667896389961243\n",
            "iteration 1829 :train_loss:0.859806478023529 val_loss0.8663579821586609\n",
            "iteration 1830 :train_loss:0.8594011664390564 val_loss0.8659268617630005\n",
            "iteration 1831 :train_loss:0.8589961528778076 val_loss0.8654963374137878\n",
            "iteration 1832 :train_loss:0.8585917949676514 val_loss0.8650663495063782\n",
            "iteration 1833 :train_loss:0.8581879138946533 val_loss0.8646369576454163\n",
            "iteration 1834 :train_loss:0.8577845096588135 val_loss0.8642080426216125\n",
            "iteration 1835 :train_loss:0.8573816418647766 val_loss0.8637798428535461\n",
            "iteration 1836 :train_loss:0.8569793105125427 val_loss0.8633521199226379\n",
            "iteration 1837 :train_loss:0.8565775156021118 val_loss0.8629249334335327\n",
            "iteration 1838 :train_loss:0.8561761379241943 val_loss0.8624982833862305\n",
            "iteration 1839 :train_loss:0.8557753562927246 val_loss0.8620721697807312\n",
            "iteration 1840 :train_loss:0.8553750514984131 val_loss0.8616465926170349\n",
            "iteration 1841 :train_loss:0.8549752235412598 val_loss0.8612217307090759\n",
            "iteration 1842 :train_loss:0.8545759320259094 val_loss0.8607973456382751\n",
            "iteration 1843 :train_loss:0.8541772365570068 val_loss0.8603735566139221\n",
            "iteration 1844 :train_loss:0.8537790179252625 val_loss0.8599502444267273\n",
            "iteration 1845 :train_loss:0.8533812165260315 val_loss0.8595275282859802\n",
            "iteration 1846 :train_loss:0.8529839515686035 val_loss0.8591054677963257\n",
            "iteration 1847 :train_loss:0.8525872230529785 val_loss0.8586838841438293\n",
            "iteration 1848 :train_loss:0.8521910905838013 val_loss0.8582628965377808\n",
            "iteration 1849 :train_loss:0.8517953753471375 val_loss0.8578423857688904\n",
            "iteration 1850 :train_loss:0.8514001965522766 val_loss0.8574224710464478\n",
            "iteration 1851 :train_loss:0.851005494594574 val_loss0.8570030927658081\n",
            "iteration 1852 :train_loss:0.8506113290786743 val_loss0.8565843105316162\n",
            "iteration 1853 :train_loss:0.8502177000045776 val_loss0.8561661839485168\n",
            "iteration 1854 :train_loss:0.8498244881629944 val_loss0.8557485342025757\n",
            "iteration 1855 :train_loss:0.8494319915771484 val_loss0.8553313612937927\n",
            "iteration 1856 :train_loss:0.8490397930145264 val_loss0.8549148440361023\n",
            "iteration 1857 :train_loss:0.8486483097076416 val_loss0.8544989228248596\n",
            "iteration 1858 :train_loss:0.8482573628425598 val_loss0.8540835380554199\n",
            "iteration 1859 :train_loss:0.8478667736053467 val_loss0.8536686897277832\n",
            "iteration 1860 :train_loss:0.8474767208099365 val_loss0.8532545566558838\n",
            "iteration 1861 :train_loss:0.8470872640609741 val_loss0.852840781211853\n",
            "iteration 1862 :train_loss:0.8466982841491699 val_loss0.85242760181427\n",
            "iteration 1863 :train_loss:0.8463098406791687 val_loss0.8520151376724243\n",
            "iteration 1864 :train_loss:0.8459218740463257 val_loss0.8516031503677368\n",
            "iteration 1865 :train_loss:0.8455345034599304 val_loss0.8511916399002075\n",
            "iteration 1866 :train_loss:0.8451474905014038 val_loss0.8507808446884155\n",
            "iteration 1867 :train_loss:0.8447611927986145 val_loss0.850370466709137\n",
            "iteration 1868 :train_loss:0.8443753719329834 val_loss0.8499608039855957\n",
            "iteration 1869 :train_loss:0.8439899682998657 val_loss0.8495516180992126\n",
            "iteration 1870 :train_loss:0.843605101108551 val_loss0.8491430878639221\n",
            "iteration 1871 :train_loss:0.8432207703590393 val_loss0.8487350940704346\n",
            "iteration 1872 :train_loss:0.8428369760513306 val_loss0.84832763671875\n",
            "iteration 1873 :train_loss:0.8424537777900696 val_loss0.8479207158088684\n",
            "iteration 1874 :train_loss:0.8420710563659668 val_loss0.847514271736145\n",
            "iteration 1875 :train_loss:0.8416888117790222 val_loss0.8471085429191589\n",
            "iteration 1876 :train_loss:0.8413069844245911 val_loss0.8467034101486206\n",
            "iteration 1877 :train_loss:0.840925931930542 val_loss0.8462988138198853\n",
            "iteration 1878 :train_loss:0.8405452966690063 val_loss0.8458946943283081\n",
            "iteration 1879 :train_loss:0.8401651382446289 val_loss0.8454911708831787\n",
            "iteration 1880 :train_loss:0.8397855758666992 val_loss0.8450883030891418\n",
            "iteration 1881 :train_loss:0.839406430721283 val_loss0.8446859121322632\n",
            "iteration 1882 :train_loss:0.8390279412269592 val_loss0.8442841172218323\n",
            "iteration 1883 :train_loss:0.8386498093605042 val_loss0.8438828587532043\n",
            "iteration 1884 :train_loss:0.8382722735404968 val_loss0.8434821367263794\n",
            "iteration 1885 :train_loss:0.837895393371582 val_loss0.843082070350647\n",
            "iteration 1886 :train_loss:0.8375188112258911 val_loss0.8426824808120728\n",
            "iteration 1887 :train_loss:0.8371428847312927 val_loss0.8422834277153015\n",
            "iteration 1888 :train_loss:0.8367674946784973 val_loss0.8418850302696228\n",
            "iteration 1889 :train_loss:0.8363924622535706 val_loss0.8414872288703918\n",
            "iteration 1890 :train_loss:0.8360180854797363 val_loss0.8410898447036743\n",
            "iteration 1891 :train_loss:0.8356442451477051 val_loss0.8406932353973389\n",
            "iteration 1892 :train_loss:0.835270881652832 val_loss0.8402970433235168\n",
            "iteration 1893 :train_loss:0.8348981142044067 val_loss0.8399013876914978\n",
            "iteration 1894 :train_loss:0.8345257639884949 val_loss0.8395063281059265\n",
            "iteration 1895 :train_loss:0.8341540098190308 val_loss0.8391119241714478\n",
            "iteration 1896 :train_loss:0.8337827324867249 val_loss0.8387179970741272\n",
            "iteration 1897 :train_loss:0.8334119319915771 val_loss0.8383246064186096\n",
            "iteration 1898 :train_loss:0.8330417275428772 val_loss0.8379318714141846\n",
            "iteration 1899 :train_loss:0.8326720595359802 val_loss0.8375396132469177\n",
            "iteration 1900 :train_loss:0.8323028683662415 val_loss0.8371480703353882\n",
            "iteration 1901 :train_loss:0.8319343328475952 val_loss0.8367571234703064\n",
            "iteration 1902 :train_loss:0.8315661549568176 val_loss0.836366593837738\n",
            "iteration 1903 :train_loss:0.8311985731124878 val_loss0.8359767198562622\n",
            "iteration 1904 :train_loss:0.8308314681053162 val_loss0.8355873227119446\n",
            "iteration 1905 :train_loss:0.8304648995399475 val_loss0.8351984620094299\n",
            "iteration 1906 :train_loss:0.8300988078117371 val_loss0.8348103165626526\n",
            "iteration 1907 :train_loss:0.8297333121299744 val_loss0.8344227075576782\n",
            "iteration 1908 :train_loss:0.8293682336807251 val_loss0.8340355157852173\n",
            "iteration 1909 :train_loss:0.8290039300918579 val_loss0.8336490988731384\n",
            "iteration 1910 :train_loss:0.8286398649215698 val_loss0.833263099193573\n",
            "iteration 1911 :train_loss:0.8282763957977295 val_loss0.8328776359558105\n",
            "iteration 1912 :train_loss:0.8279134631156921 val_loss0.8324928283691406\n",
            "iteration 1913 :train_loss:0.8275511264801025 val_loss0.8321084976196289\n",
            "iteration 1914 :train_loss:0.8271892070770264 val_loss0.8317247629165649\n",
            "iteration 1915 :train_loss:0.826827883720398 val_loss0.8313416242599487\n",
            "iteration 1916 :train_loss:0.826466977596283 val_loss0.8309589624404907\n",
            "iteration 1917 :train_loss:0.8261066675186157 val_loss0.8305768966674805\n",
            "iteration 1918 :train_loss:0.8257469534873962 val_loss0.8301953077316284\n",
            "iteration 1919 :train_loss:0.8253875970840454 val_loss0.8298144340515137\n",
            "iteration 1920 :train_loss:0.8250289559364319 val_loss0.8294340372085571\n",
            "iteration 1921 :train_loss:0.8246707916259766 val_loss0.8290542364120483\n",
            "iteration 1922 :train_loss:0.8243129849433899 val_loss0.8286750316619873\n",
            "iteration 1923 :train_loss:0.823955774307251 val_loss0.8282963037490845\n",
            "iteration 1924 :train_loss:0.8235990405082703 val_loss0.8279181718826294\n",
            "iteration 1925 :train_loss:0.8232429027557373 val_loss0.8275406956672668\n",
            "iteration 1926 :train_loss:0.8228873014450073 val_loss0.8271637558937073\n",
            "iteration 1927 :train_loss:0.8225322365760803 val_loss0.8267874121665955\n",
            "iteration 1928 :train_loss:0.8221775889396667 val_loss0.8264114856719971\n",
            "iteration 1929 :train_loss:0.8218235373497009 val_loss0.8260361552238464\n",
            "iteration 1930 :train_loss:0.8214698433876038 val_loss0.8256614208221436\n",
            "iteration 1931 :train_loss:0.8211168646812439 val_loss0.8252872228622437\n",
            "iteration 1932 :train_loss:0.8207643628120422 val_loss0.8249136805534363\n",
            "iteration 1933 :train_loss:0.8204123377799988 val_loss0.8245405554771423\n",
            "iteration 1934 :train_loss:0.8200607299804688 val_loss0.8241680264472961\n",
            "iteration 1935 :train_loss:0.8197097778320312 val_loss0.823796272277832\n",
            "iteration 1936 :train_loss:0.8193593621253967 val_loss0.8234248757362366\n",
            "iteration 1937 :train_loss:0.8190093040466309 val_loss0.8230540752410889\n",
            "iteration 1938 :train_loss:0.8186599016189575 val_loss0.8226836919784546\n",
            "iteration 1939 :train_loss:0.8183109760284424 val_loss0.8223140239715576\n",
            "iteration 1940 :train_loss:0.8179624080657959 val_loss0.8219449520111084\n",
            "iteration 1941 :train_loss:0.8176146149635315 val_loss0.8215762972831726\n",
            "iteration 1942 :train_loss:0.8172671794891357 val_loss0.8212083578109741\n",
            "iteration 1943 :train_loss:0.816920280456543 val_loss0.8208408951759338\n",
            "iteration 1944 :train_loss:0.816573977470398 val_loss0.8204740881919861\n",
            "iteration 1945 :train_loss:0.8162280917167664 val_loss0.8201076984405518\n",
            "iteration 1946 :train_loss:0.8158828020095825 val_loss0.8197419047355652\n",
            "iteration 1947 :train_loss:0.8155380487442017 val_loss0.8193767070770264\n",
            "iteration 1948 :train_loss:0.8151937127113342 val_loss0.8190121054649353\n",
            "iteration 1949 :train_loss:0.8148499131202698 val_loss0.8186479210853577\n",
            "iteration 1950 :train_loss:0.8145067095756531 val_loss0.818284273147583\n",
            "iteration 1951 :train_loss:0.814163863658905 val_loss0.8179213404655457\n",
            "iteration 1952 :train_loss:0.8138217926025391 val_loss0.8175589442253113\n",
            "iteration 1953 :train_loss:0.813480019569397 val_loss0.8171969652175903\n",
            "iteration 1954 :train_loss:0.8131388425827026 val_loss0.8168358206748962\n",
            "iteration 1955 :train_loss:0.812798261642456 val_loss0.8164750337600708\n",
            "iteration 1956 :train_loss:0.8124580383300781 val_loss0.8161148428916931\n",
            "iteration 1957 :train_loss:0.812118411064148 val_loss0.8157552480697632\n",
            "iteration 1958 :train_loss:0.811779260635376 val_loss0.815396249294281\n",
            "iteration 1959 :train_loss:0.811440646648407 val_loss0.815037727355957\n",
            "iteration 1960 :train_loss:0.811102569103241 val_loss0.814679741859436\n",
            "iteration 1961 :train_loss:0.8107649683952332 val_loss0.8143224120140076\n",
            "iteration 1962 :train_loss:0.8104278445243835 val_loss0.8139656186103821\n",
            "iteration 1963 :train_loss:0.8100913166999817 val_loss0.8136093616485596\n",
            "iteration 1964 :train_loss:0.809755265712738 val_loss0.8132537007331848\n",
            "iteration 1965 :train_loss:0.8094198107719421 val_loss0.8128985166549683\n",
            "iteration 1966 :train_loss:0.8090847730636597 val_loss0.8125438690185547\n",
            "iteration 1967 :train_loss:0.8087502717971802 val_loss0.8121898770332336\n",
            "iteration 1968 :train_loss:0.8084162473678589 val_loss0.8118363618850708\n",
            "iteration 1969 :train_loss:0.8080827593803406 val_loss0.8114833831787109\n",
            "iteration 1970 :train_loss:0.8077497482299805 val_loss0.811130940914154\n",
            "iteration 1971 :train_loss:0.8074172735214233 val_loss0.8107790946960449\n",
            "iteration 1972 :train_loss:0.8070853352546692 val_loss0.8104279041290283\n",
            "iteration 1973 :train_loss:0.8067538142204285 val_loss0.8100771307945251\n",
            "iteration 1974 :train_loss:0.8064228892326355 val_loss0.809726893901825\n",
            "iteration 1975 :train_loss:0.8060924410820007 val_loss0.8093772530555725\n",
            "iteration 1976 :train_loss:0.805762529373169 val_loss0.809028148651123\n",
            "iteration 1977 :train_loss:0.8054330348968506 val_loss0.808679461479187\n",
            "iteration 1978 :train_loss:0.80510413646698 val_loss0.8083314895629883\n",
            "iteration 1979 :train_loss:0.8047756552696228 val_loss0.8079841136932373\n",
            "iteration 1980 :train_loss:0.8044477105140686 val_loss0.8076372146606445\n",
            "iteration 1981 :train_loss:0.8041203022003174 val_loss0.8072909116744995\n",
            "iteration 1982 :train_loss:0.8037934303283691 val_loss0.8069450855255127\n",
            "iteration 1983 :train_loss:0.8034670352935791 val_loss0.8065998554229736\n",
            "iteration 1984 :train_loss:0.8031410574913025 val_loss0.8062551021575928\n",
            "iteration 1985 :train_loss:0.8028156757354736 val_loss0.8059109449386597\n",
            "iteration 1986 :train_loss:0.8024908304214478 val_loss0.8055672645568848\n",
            "iteration 1987 :train_loss:0.8021664619445801 val_loss0.8052241802215576\n",
            "iteration 1988 :train_loss:0.8018425703048706 val_loss0.8048816919326782\n",
            "iteration 1989 :train_loss:0.8015191555023193 val_loss0.804539680480957\n",
            "iteration 1990 :train_loss:0.801196277141571 val_loss0.8041982054710388\n",
            "iteration 1991 :train_loss:0.800873875617981 val_loss0.8038572669029236\n",
            "iteration 1992 :train_loss:0.8005519509315491 val_loss0.8035169243812561\n",
            "iteration 1993 :train_loss:0.8002306222915649 val_loss0.8031771183013916\n",
            "iteration 1994 :train_loss:0.7999096512794495 val_loss0.8028377890586853\n",
            "iteration 1995 :train_loss:0.799589216709137 val_loss0.8024990558624268\n",
            "iteration 1996 :train_loss:0.7992693185806274 val_loss0.8021608591079712\n",
            "iteration 1997 :train_loss:0.7989498972892761 val_loss0.8018231391906738\n",
            "iteration 1998 :train_loss:0.7986310124397278 val_loss0.8014860153198242\n",
            "iteration 1999 :train_loss:0.7983127236366272 val_loss0.8011494874954224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "e7QXIVf1DZzB",
        "outputId": "98b1d5f8-30f9-4a0d-cf6d-296d2a35d17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test = predict(parameters, test_norm)\n",
        "TF_MAPE = MAPE(test_Y, predicted_test)\n",
        "print(\"The MAPE of the Tensorflow model is: \", TF_MAPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpjtEOnZDg2u",
        "outputId": "3f182993-59cf-4934-d0bf-68441d4b234f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MAPE of the Tensorflow model is:  0.49584582448005676\n"
          ]
        }
      ]
    }
  ]
}