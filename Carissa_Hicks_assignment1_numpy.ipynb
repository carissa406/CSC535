{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Modifying the model in lab 3.2 to do Regression"
      ],
      "metadata": {
        "id": "RAE3x-Y_0ppc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2QxeT_ceZ22"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set the random seed so the same random values are generated every time you run this function\n",
        "    np.random.seed(1)\n",
        "\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer\n",
        "    W1=np.random.uniform(size=(nh,nx), low=-0.01, high=0.01)\n",
        "    b1=np.zeros((nh,1))\n",
        "    W2=np.random.uniform(size=(ny,nh), low=-0.01, high=0.01)\n",
        "    b2=np.zeros((ny,1))\n",
        "   \n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "ijdAku1pRzm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward Pass\n",
        "#relu activation\n",
        "def relu(z):\n",
        "    return np.maximum(0,z)"
      ],
      "metadata": {
        "id": "tX82RbBEqLmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(parameters,X):\n",
        "    #dot product of the weight times the input plus bias\n",
        "    Z1= np.dot(parameters[\"W1\"],X)+parameters[\"b1\"] # b1 is broadcasted n times before it is added to np.dpt(W1,X1)\n",
        "    A1=relu(Z1) #apply relu activation on the output\n",
        "    #dot product of weights times A1 plus bias\n",
        "    Z2=np.dot(parameters[\"W2\"],A1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "    Yhat=Z2 #no activation function on regression problems\n",
        "   \n",
        "   #cache to store values to use in the backward pass\n",
        "    cache = {\"A1\": A1,\n",
        "             \"Z1\":Z1,\n",
        "             \"Z2\": Z2}\n",
        "    return Yhat,cache"
      ],
      "metadata": {
        "id": "LJ-L_o6UpUS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using mean squared error for the loss function\n",
        "def compute_loss(Y,Yhat):\n",
        "    n=Y.shape[1] \n",
        "    loss = (1/n) * np.sum((Y - Yhat)**2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "0eK5XInapWLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Backward Pass\n",
        "#Gradient of Mean Squared Error loss function\n",
        "def dMSE (Y, Yhat):\n",
        "    dMSE = Yhat - Y\n",
        "    return (dMSE)\n",
        "\n",
        "#Derivative of Relu\n",
        "def drelu(Z):\n",
        "    drelu=np.where(Z>0, 1.0, 0.0) \n",
        "    return drelu "
      ],
      "metadata": {
        "id": "DY6ZAfp4pYCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(parameters, cache, X, Y, Yhat):\n",
        "    n=X.shape[1]\n",
        "\n",
        "    dZ2=dMSE(Y,Yhat) #*(cache[\"Z2\"]) removed this because Yhat IS Z2 so you're multiplying the same thing together #no activation function on Z2\n",
        "    dW2=(1/n)*np.dot(dZ2,cache[\"A1\"].T)\n",
        "    db2=(1/n)*np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1=np.dot(parameters[\"W2\"].T,dZ2)\n",
        "    dZ1=dA1*drelu(cache[\"Z1\"])\n",
        "    dW1=(1/n)*np.dot(dZ1,X.T)\n",
        "    db1=(1/n)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    gradients={\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\":dW2,\n",
        "              \"db2\":db2\n",
        "              }\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "m6NM1GD5pZ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"]=parameters[\"W1\"]-learning_rate*gradients[\"dW1\"]\n",
        "    parameters[\"W2\"]=parameters[\"W2\"]-learning_rate*gradients[\"dW2\"]\n",
        "    parameters[\"b1\"]=parameters[\"b1\"]-learning_rate*gradients[\"db1\"]\n",
        "    parameters[\"b2\"]=parameters[\"b2\"]-learning_rate*gradients[\"db2\"]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "14VOitoopbrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "  \n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\" \n",
        "    \n",
        "    \n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "    \n",
        "    #one neuron in output layer with no activation function\n",
        "    ny=1\n",
        "    \n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "    \n",
        "    #initialize lists to store the training and valideation losses for each iteration. \n",
        "    val_loss=[]\n",
        "    train_loss=[]\n",
        "    \n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "        #run the forward pass on train_X\n",
        "        Yhat_train, train_cache= forward_pass(parameters,train_X)\n",
        "        \n",
        "        #run the forward pass on val_X\n",
        "        Yhat_val,val_cache= forward_pass(parameters,val_X)\n",
        "        \n",
        "        #compute the loss on the train and val datasets\n",
        "        train_loss.append(compute_loss(train_Y,Yhat_train))\n",
        "        val_loss.append(compute_loss(val_Y,Yhat_val))\n",
        "\n",
        "        \"\"\"\n",
        "        run the backward pass. Note that the backward pass is only run on the training data not the validation data\n",
        "        Because the learning must be only done on the training data and hence, validation data is not used to update\n",
        "        the model parameters.  \n",
        "        \"\"\"\n",
        "        gradients=backward_pass(parameters, train_cache, train_X, train_Y,Yhat_train)\n",
        "        \n",
        "        # update the parameters\n",
        "        parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "        \n",
        "        #print the trianing loss and validation loss for each iteration.\n",
        "        print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss[i],val_loss[i]))\n",
        "        \n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_loss,\n",
        "             \"train_loss\": train_loss}\n",
        "        \n",
        "        #return the parameters and the history\n",
        "    return parameters, history"
      ],
      "metadata": {
        "id": "kQVjS8thpgEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get predictions\n",
        "def predict(parameters,X):\n",
        "    Yhat,cache=forward_pass(parameters, X)\n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "S5_AIi_8pgo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Preparing California Housing Data"
      ],
      "metadata": {
        "id": "pHv1nzWE0w1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "df = pd.read_csv(\"sample_data/california_housing_train.csv\")"
      ],
      "metadata": {
        "id": "pKCEuBNdpkv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwKQAOMJ2ywJ",
        "outputId": "92b16aa4-29f2-4f0b-8304-95f231aba1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oM36ZH9yrIe4",
        "outputId": "3e92991b-73ef-4847-eef8-305efa2f2cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
              "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
              "2    -114.56     33.69                17.0        720.0           174.0   \n",
              "3    -114.57     33.64                14.0       1501.0           337.0   \n",
              "4    -114.57     33.57                20.0       1454.0           326.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1015.0       472.0         1.4936             66900.0  \n",
              "1      1129.0       463.0         1.8200             80100.0  \n",
              "2       333.0       117.0         1.6509             85700.0  \n",
              "3       515.0       226.0         3.1917             73400.0  \n",
              "4       624.0       262.0         1.9250             65500.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c07eaaf-b7cd-4548-ac29-240d2d37b46e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c07eaaf-b7cd-4548-ac29-240d2d37b46e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c07eaaf-b7cd-4548-ac29-240d2d37b46e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c07eaaf-b7cd-4548-ac29-240d2d37b46e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split training data into 80% training and 20% validation\n",
        "train = df.sample(frac=0.8, random_state=123)\n",
        "val = df.drop(train.index)"
      ],
      "metadata": {
        "id": "eEi_hOX6pmJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWZGpRwx3I2N",
        "outputId": "197541c3-9c17-42a9-aa95-744b34cdd8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13600, 9)\n",
            "(3400, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the input datasets train.csv and validation.csv and store them into numpy arrays\n",
        "train = train.to_numpy()\n",
        "test = pd.read_csv('sample_data/california_housing_test.csv').to_numpy()\n",
        "val = val.to_numpy()"
      ],
      "metadata": {
        "id": "ifJyJDcjrSyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ70kTgr6oWf",
        "outputId": "b11f271b-1d06-4482-a586-20bee2b3636c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#separate the features from the target variable (median_house_value) in train, val and test\n",
        "train_X = train[:,:-1]\n",
        "train_Y = train[...,-1] #labels\n",
        "\n",
        "test_X = test[:,:-1]\n",
        "test_Y = test[...,-1] #labels\n",
        "\n",
        "val_X = val[:,:-1]\n",
        "val_Y = val[...,-1] #labels"
      ],
      "metadata": {
        "id": "K5bGPJK51F4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "print(val_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_Y.shape)\n",
        "print(val_Y.shape)\n",
        "print(test_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFxDKwjg5C7F",
        "outputId": "d32d060b-3d15-41b1-d366-1e79d125c06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13600, 8)\n",
            "(3000, 8)\n",
            "(3400, 8)\n",
            "(13600,)\n",
            "(3000,)\n",
            "(3400,)\n",
            "[344700. 176500. 270500. ...  62000. 162500. 500001.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the data: subtract mean of each feature and divide by the std, so that the feature is centered around 0 and has a unit std\n",
        "train_norm = (train_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "test_norm = (test_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "val_norm = (val_X - np.mean(train_X, axis=0))/np.std(train_X, axis=0)\n",
        "\n",
        "print(train_norm.shape)\n",
        "print(test_norm.shape)\n",
        "print(val_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiiqIpGH5g05",
        "outputId": "edda8329-6446-49fb-e285-4f6d0aa7f1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13600, 8)\n",
            "(3000, 8)\n",
            "(3400, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#divide the median_house_values by 100k to scale them down\n",
        "train_Y = train_Y/100000\n",
        "test_Y = test_Y/100000\n",
        "val_Y = val_Y/100000"
      ],
      "metadata": {
        "id": "Ixk8jRfFxDN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transpose feature matricies for train,test,val and reshape target vectors to 2D arrays\n",
        "train_norm = train_norm.transpose()\n",
        "test_norm = test_norm.transpose()\n",
        "val_norm = val_norm.transpose()"
      ],
      "metadata": {
        "id": "KGtGca-BxSH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y=np.reshape(train_Y, (1, train_Y.size))\n",
        "test_Y=np.reshape(test_Y, (1, test_Y.size))\n",
        "val_Y=np.reshape(val_Y, (1, val_Y.size))"
      ],
      "metadata": {
        "id": "gydMq_Zm9Y0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_norm.shape)\n",
        "print(train_Y.shape)\n",
        "print(val_norm.shape)\n",
        "print(val_Y.shape)\n",
        "print(test_norm.shape)\n",
        "print(test_Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBhk8h3JzR4b",
        "outputId": "ef3a7eb2-e743-4b3e-f994-a23e8d6625c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 13600)\n",
            "(1, 13600)\n",
            "(8, 3400)\n",
            "(1, 3400)\n",
            "(8, 3000)\n",
            "(1, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gaxe9CAAfgsY",
        "outputId": "942e6d36-1661-40c8-8a8a-5ca94436f160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.24677127 -1.25325691  0.83758503 ...  0.8226148  -0.15045006\n",
            "   1.20186059]\n",
            " [-1.4115248   1.07578173 -0.79904894 ... -0.94398598  0.42590277\n",
            "  -1.32269242]\n",
            " [-0.5241687   0.50848037 -1.15964505 ...  0.66734946  0.42904583\n",
            "  -0.5241687 ]\n",
            " ...\n",
            " [ 1.5769497   0.35804834 -0.71440947 ... -0.90294458 -0.53113582\n",
            "  -0.18475737]\n",
            " [ 1.83553998  0.43008071 -0.44898931 ... -0.90694795 -0.60427414\n",
            "   0.269532  ]\n",
            " [-0.91672605  0.63369272 -0.78484527 ...  3.43211399 -0.77908144\n",
            "  -0.7285289 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training and hyper-parameter tuning"
      ],
      "metadata": {
        "id": "4JDbDt5T01vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations= 2000\n",
        "parameters, history=create_nn_model(train_norm,train_Y,128, val_norm, val_Y, iterations, 0.3)"
      ],
      "metadata": {
        "id": "jP2sDNDWpraT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022ee46e-c6c9-460e-8ebd-d5d255b6a490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.629278381831812 val_loss5.690261924048548\n",
            "iteration 1 :train_loss:3.4256438714559265 val_loss3.480361791403734\n",
            "iteration 2 :train_loss:2.347095020270553 val_loss2.3973622734830973\n",
            "iteration 3 :train_loss:1.8156431067258187 val_loss1.8625256262841148\n",
            "iteration 4 :train_loss:1.5500815400110144 val_loss1.594179653881603\n",
            "iteration 5 :train_loss:1.4117172517940586 val_loss1.4532612959018516\n",
            "iteration 6 :train_loss:1.3316057517353783 val_loss1.3705281487741054\n",
            "iteration 7 :train_loss:1.275123288437766 val_loss1.3111153340423105\n",
            "iteration 8 :train_loss:1.2247246939055558 val_loss1.2573178203190691\n",
            "iteration 9 :train_loss:1.1717560822936641 val_loss1.200384830490324\n",
            "iteration 10 :train_loss:1.1127315333364638 val_loss1.136809173350687\n",
            "iteration 11 :train_loss:1.0477258768520177 val_loss1.0668565895329294\n",
            "iteration 12 :train_loss:0.9796227069434857 val_loss0.9937403052498149\n",
            "iteration 13 :train_loss:0.9130288966183586 val_loss0.9224840591827036\n",
            "iteration 14 :train_loss:0.8525667053870938 val_loss0.8581063063657924\n",
            "iteration 15 :train_loss:0.8011857893208538 val_loss0.8037479821383788\n",
            "iteration 16 :train_loss:0.7595860497474525 val_loss0.7600274509464451\n",
            "iteration 17 :train_loss:0.7267609751992145 val_loss0.7257135380331927\n",
            "iteration 18 :train_loss:0.701013616156049 val_loss0.6988881399852808\n",
            "iteration 19 :train_loss:0.6806770874923678 val_loss0.6776941337543748\n",
            "iteration 20 :train_loss:0.6643609894121381 val_loss0.660647471498859\n",
            "iteration 21 :train_loss:0.6510026435238552 val_loss0.6466155650485429\n",
            "iteration 22 :train_loss:0.6397982797750877 val_loss0.6347848614852828\n",
            "iteration 23 :train_loss:0.6301698460825559 val_loss0.6245338610381954\n",
            "iteration 24 :train_loss:0.6216876865124801 val_loss0.6154346909954131\n",
            "iteration 25 :train_loss:0.6140523443861438 val_loss0.6071880087868398\n",
            "iteration 26 :train_loss:0.6070431789728437 val_loss0.5995831967075854\n",
            "iteration 27 :train_loss:0.6005219690134425 val_loss0.5924799104654849\n",
            "iteration 28 :train_loss:0.5943745342846349 val_loss0.5857805837081517\n",
            "iteration 29 :train_loss:0.588532364278785 val_loss0.5794108167527926\n",
            "iteration 30 :train_loss:0.5829323783276484 val_loss0.5733109266861397\n",
            "iteration 31 :train_loss:0.5775281924955187 val_loss0.5674371733615194\n",
            "iteration 32 :train_loss:0.5722863276611803 val_loss0.5617615415947913\n",
            "iteration 33 :train_loss:0.5671997685586478 val_loss0.5562670965685572\n",
            "iteration 34 :train_loss:0.5622438097686964 val_loss0.5509320055047489\n",
            "iteration 35 :train_loss:0.5574254083099757 val_loss0.5457574287967171\n",
            "iteration 36 :train_loss:0.5527375725290317 val_loss0.5407326472163885\n",
            "iteration 37 :train_loss:0.5481820648337845 val_loss0.5358584233967241\n",
            "iteration 38 :train_loss:0.5437549285506631 val_loss0.5311308885095735\n",
            "iteration 39 :train_loss:0.539455092684193 val_loss0.5265407401052146\n",
            "iteration 40 :train_loss:0.5352914091641462 val_loss0.5220916969121829\n",
            "iteration 41 :train_loss:0.531264617077612 val_loss0.517779885606176\n",
            "iteration 42 :train_loss:0.5273747987585915 val_loss0.513592184391682\n",
            "iteration 43 :train_loss:0.5236182998940934 val_loss0.5095319283897278\n",
            "iteration 44 :train_loss:0.5199909638880885 val_loss0.5056062463032003\n",
            "iteration 45 :train_loss:0.5164926438846066 val_loss0.5018024437750171\n",
            "iteration 46 :train_loss:0.5131293939088851 val_loss0.4981352776634484\n",
            "iteration 47 :train_loss:0.5098929177739272 val_loss0.49459884517875086\n",
            "iteration 48 :train_loss:0.5067817457037621 val_loss0.4911911749891232\n",
            "iteration 49 :train_loss:0.5037945563547253 val_loss0.4879142304565842\n",
            "iteration 50 :train_loss:0.5009298665426246 val_loss0.48475678786995396\n",
            "iteration 51 :train_loss:0.49818463264383417 val_loss0.4817145682433567\n",
            "iteration 52 :train_loss:0.49555633884690137 val_loss0.4787833188643686\n",
            "iteration 53 :train_loss:0.49303870431625063 val_loss0.4759595363886186\n",
            "iteration 54 :train_loss:0.49063009941759106 val_loss0.4732460093814711\n",
            "iteration 55 :train_loss:0.4883248975482404 val_loss0.4706364319295369\n",
            "iteration 56 :train_loss:0.48611776395425793 val_loss0.4681272689045109\n",
            "iteration 57 :train_loss:0.4840099097945715 val_loss0.46571606106923297\n",
            "iteration 58 :train_loss:0.4819937918472047 val_loss0.463401634453952\n",
            "iteration 59 :train_loss:0.48006730376998935 val_loss0.4611786332760791\n",
            "iteration 60 :train_loss:0.4782287705250998 val_loss0.45904736340975605\n",
            "iteration 61 :train_loss:0.47647311462139547 val_loss0.4570010827551459\n",
            "iteration 62 :train_loss:0.4747945811819426 val_loss0.4550363312383096\n",
            "iteration 63 :train_loss:0.47319354213121156 val_loss0.4531526726443957\n",
            "iteration 64 :train_loss:0.47166782470006713 val_loss0.45134490963248375\n",
            "iteration 65 :train_loss:0.47021152383935083 val_loss0.4496088095860675\n",
            "iteration 66 :train_loss:0.4688193181827557 val_loss0.4479427841836838\n",
            "iteration 67 :train_loss:0.4674910903869388 val_loss0.4463448541200376\n",
            "iteration 68 :train_loss:0.46621957320093527 val_loss0.4448076947216538\n",
            "iteration 69 :train_loss:0.4650047751457691 val_loss0.4433305231527562\n",
            "iteration 70 :train_loss:0.4638432602767805 val_loss0.4419145411050556\n",
            "iteration 71 :train_loss:0.4627290869745901 val_loss0.4405530495049704\n",
            "iteration 72 :train_loss:0.46166168938177776 val_loss0.43924225149678775\n",
            "iteration 73 :train_loss:0.46063475718778574 val_loss0.4379764995451923\n",
            "iteration 74 :train_loss:0.45964494481352614 val_loss0.4367542293871985\n",
            "iteration 75 :train_loss:0.4586924840478044 val_loss0.4355763477245596\n",
            "iteration 76 :train_loss:0.4577751061339036 val_loss0.4344443958446219\n",
            "iteration 77 :train_loss:0.4568856657467065 val_loss0.4333524006636603\n",
            "iteration 78 :train_loss:0.4560272552116525 val_loss0.43229788469579444\n",
            "iteration 79 :train_loss:0.45519574721143136 val_loss0.43128344238344274\n",
            "iteration 80 :train_loss:0.45438898131005995 val_loss0.43030145759731403\n",
            "iteration 81 :train_loss:0.45360354176107986 val_loss0.429349416461534\n",
            "iteration 82 :train_loss:0.45284663885544496 val_loss0.42843370576939416\n",
            "iteration 83 :train_loss:0.45211626044625597 val_loss0.42755129808935805\n",
            "iteration 84 :train_loss:0.4514129833404319 val_loss0.42670364727470084\n",
            "iteration 85 :train_loss:0.45073185674892124 val_loss0.42588756426664887\n",
            "iteration 86 :train_loss:0.4500741336287818 val_loss0.4250949770192963\n",
            "iteration 87 :train_loss:0.4494394332515373 val_loss0.42433058934813916\n",
            "iteration 88 :train_loss:0.4488244494126625 val_loss0.4235919599254106\n",
            "iteration 89 :train_loss:0.4482283472142792 val_loss0.42287742134563916\n",
            "iteration 90 :train_loss:0.44764987381227644 val_loss0.42218302182061157\n",
            "iteration 91 :train_loss:0.44708503187308773 val_loss0.42150709673823505\n",
            "iteration 92 :train_loss:0.44653794582642714 val_loss0.42084799683549046\n",
            "iteration 93 :train_loss:0.4460081347041432 val_loss0.42021336067028653\n",
            "iteration 94 :train_loss:0.44549203506935997 val_loss0.41959346464682334\n",
            "iteration 95 :train_loss:0.4449870473081193 val_loss0.4189931068156631\n",
            "iteration 96 :train_loss:0.44449568470227147 val_loss0.4184068549484153\n",
            "iteration 97 :train_loss:0.4440169754130025 val_loss0.41783994825204185\n",
            "iteration 98 :train_loss:0.4435504308903924 val_loss0.4172815578008324\n",
            "iteration 99 :train_loss:0.4430937180975327 val_loss0.4167369315716604\n",
            "iteration 100 :train_loss:0.44264778623191936 val_loss0.41619831765083476\n",
            "iteration 101 :train_loss:0.44221036274325126 val_loss0.41567491021496805\n",
            "iteration 102 :train_loss:0.4417822548612376 val_loss0.4151647701212146\n",
            "iteration 103 :train_loss:0.4413600594823239 val_loss0.41466196799426847\n",
            "iteration 104 :train_loss:0.4409448634364629 val_loss0.4141667597444306\n",
            "iteration 105 :train_loss:0.44053736744889777 val_loss0.41367780889168576\n",
            "iteration 106 :train_loss:0.4401367380322451 val_loss0.41319718746801753\n",
            "iteration 107 :train_loss:0.4397410832035348 val_loss0.41272971447771795\n",
            "iteration 108 :train_loss:0.4393512853961014 val_loss0.4122621830620202\n",
            "iteration 109 :train_loss:0.4389672998284334 val_loss0.41181173698808976\n",
            "iteration 110 :train_loss:0.4385882866514296 val_loss0.4113679883220455\n",
            "iteration 111 :train_loss:0.4382155502371315 val_loss0.4109318608762877\n",
            "iteration 112 :train_loss:0.43784883160627863 val_loss0.41049662979693535\n",
            "iteration 113 :train_loss:0.4374874748566083 val_loss0.4100741029032308\n",
            "iteration 114 :train_loss:0.4371316965490073 val_loss0.40965771738995815\n",
            "iteration 115 :train_loss:0.43677963944668213 val_loss0.4092486858513804\n",
            "iteration 116 :train_loss:0.43643269056929684 val_loss0.40884614178983547\n",
            "iteration 117 :train_loss:0.4360874647976923 val_loss0.4084523073921022\n",
            "iteration 118 :train_loss:0.43574409595852553 val_loss0.40805851984217933\n",
            "iteration 119 :train_loss:0.4354044535403453 val_loss0.40767834039256123\n",
            "iteration 120 :train_loss:0.43506981008544776 val_loss0.40729751017502563\n",
            "iteration 121 :train_loss:0.43473860430396477 val_loss0.40693245084871005\n",
            "iteration 122 :train_loss:0.43441079123053344 val_loss0.4065550978491546\n",
            "iteration 123 :train_loss:0.4340854352270393 val_loss0.40619467434429424\n",
            "iteration 124 :train_loss:0.4337642786114489 val_loss0.40582289079051753\n",
            "iteration 125 :train_loss:0.4334470976062703 val_loss0.40547628399723606\n",
            "iteration 126 :train_loss:0.43313461764396427 val_loss0.4051169207747305\n",
            "iteration 127 :train_loss:0.43282421428449874 val_loss0.4047714644469435\n",
            "iteration 128 :train_loss:0.43251456851055503 val_loss0.40442424103506497\n",
            "iteration 129 :train_loss:0.43220795708642334 val_loss0.4040783887691047\n",
            "iteration 130 :train_loss:0.4319039794932051 val_loss0.40374319229483185\n",
            "iteration 131 :train_loss:0.431603139248819 val_loss0.403402951845403\n",
            "iteration 132 :train_loss:0.43130402377648336 val_loss0.4030719353125279\n",
            "iteration 133 :train_loss:0.4310076108640919 val_loss0.40274432298443824\n",
            "iteration 134 :train_loss:0.43071305514027397 val_loss0.4024230691814166\n",
            "iteration 135 :train_loss:0.4304205067207943 val_loss0.40210412462774064\n",
            "iteration 136 :train_loss:0.43013236022761525 val_loss0.4017891273003947\n",
            "iteration 137 :train_loss:0.42984630126058576 val_loss0.40147303538671786\n",
            "iteration 138 :train_loss:0.4295617207803206 val_loss0.40116513218395056\n",
            "iteration 139 :train_loss:0.42927890376785716 val_loss0.400854134571168\n",
            "iteration 140 :train_loss:0.42899954112850197 val_loss0.4005473983980549\n",
            "iteration 141 :train_loss:0.42872177110103116 val_loss0.40024440774328673\n",
            "iteration 142 :train_loss:0.4284473249875502 val_loss0.39994007586929237\n",
            "iteration 143 :train_loss:0.42817462130392986 val_loss0.3996447309716245\n",
            "iteration 144 :train_loss:0.42790320179366786 val_loss0.39935003664947294\n",
            "iteration 145 :train_loss:0.4276338763277396 val_loss0.3990632731349001\n",
            "iteration 146 :train_loss:0.4273659416403891 val_loss0.39877295101341753\n",
            "iteration 147 :train_loss:0.4270990820406775 val_loss0.39849420356754206\n",
            "iteration 148 :train_loss:0.42683451237960884 val_loss0.3981993377599006\n",
            "iteration 149 :train_loss:0.4265725882682756 val_loss0.3979246420345002\n",
            "iteration 150 :train_loss:0.4263122365743811 val_loss0.3976348498072407\n",
            "iteration 151 :train_loss:0.4260538536697784 val_loss0.3973654011244323\n",
            "iteration 152 :train_loss:0.4257968525248246 val_loss0.3970855647530703\n",
            "iteration 153 :train_loss:0.4255399597950841 val_loss0.3968182053635157\n",
            "iteration 154 :train_loss:0.4252838938057036 val_loss0.3965445684194564\n",
            "iteration 155 :train_loss:0.425029457708478 val_loss0.3962841796560352\n",
            "iteration 156 :train_loss:0.424775583007328 val_loss0.3960134735976093\n",
            "iteration 157 :train_loss:0.42452237142067184 val_loss0.39575791468909766\n",
            "iteration 158 :train_loss:0.42427105250096925 val_loss0.3954916007585914\n",
            "iteration 159 :train_loss:0.4240211674667572 val_loss0.3952391751013387\n",
            "iteration 160 :train_loss:0.4237733608359525 val_loss0.39497680087640363\n",
            "iteration 161 :train_loss:0.42352554175322504 val_loss0.3947222634097554\n",
            "iteration 162 :train_loss:0.4232783210497089 val_loss0.39446493931761567\n",
            "iteration 163 :train_loss:0.4230320267375671 val_loss0.39421049900180266\n",
            "iteration 164 :train_loss:0.4227876787874307 val_loss0.3939555705272465\n",
            "iteration 165 :train_loss:0.4225428079978661 val_loss0.39370798218675007\n",
            "iteration 166 :train_loss:0.4222973382972966 val_loss0.3934544069906899\n",
            "iteration 167 :train_loss:0.42205245138032976 val_loss0.3932105624438355\n",
            "iteration 168 :train_loss:0.42180880513657387 val_loss0.392955003850431\n",
            "iteration 169 :train_loss:0.421564692171736 val_loss0.39271589035954463\n",
            "iteration 170 :train_loss:0.42132127520047064 val_loss0.39245821130742475\n",
            "iteration 171 :train_loss:0.4210784303865994 val_loss0.39222507942244256\n",
            "iteration 172 :train_loss:0.42083514601307487 val_loss0.39196430146818434\n",
            "iteration 173 :train_loss:0.42059148543793307 val_loss0.3917295557652768\n",
            "iteration 174 :train_loss:0.42034838202303154 val_loss0.39147466598476277\n",
            "iteration 175 :train_loss:0.42010656505104477 val_loss0.39123619391832454\n",
            "iteration 176 :train_loss:0.4198656068772575 val_loss0.3909775776131369\n",
            "iteration 177 :train_loss:0.4196248594003074 val_loss0.39074086137201136\n",
            "iteration 178 :train_loss:0.419385259095918 val_loss0.39048594558266275\n",
            "iteration 179 :train_loss:0.41914652359287097 val_loss0.39025205866812\n",
            "iteration 180 :train_loss:0.41890824564054874 val_loss0.38999144462315444\n",
            "iteration 181 :train_loss:0.4186694680605578 val_loss0.38975177040495584\n",
            "iteration 182 :train_loss:0.4184296929393005 val_loss0.3894951856578831\n",
            "iteration 183 :train_loss:0.41819048801527214 val_loss0.38926591121666526\n",
            "iteration 184 :train_loss:0.4179520809713152 val_loss0.3890024676682054\n",
            "iteration 185 :train_loss:0.41771482259085224 val_loss0.3887716988337447\n",
            "iteration 186 :train_loss:0.41747811861010453 val_loss0.38851175261041876\n",
            "iteration 187 :train_loss:0.41724149575263 val_loss0.3882854599907034\n",
            "iteration 188 :train_loss:0.41700542869828156 val_loss0.3880246398689904\n",
            "iteration 189 :train_loss:0.4167710431946443 val_loss0.3878013929510032\n",
            "iteration 190 :train_loss:0.41653589450570155 val_loss0.387538075375158\n",
            "iteration 191 :train_loss:0.41630116632797337 val_loss0.3873173341924883\n",
            "iteration 192 :train_loss:0.41606777961797003 val_loss0.38706536828486265\n",
            "iteration 193 :train_loss:0.4158345903131781 val_loss0.3868446729673523\n",
            "iteration 194 :train_loss:0.41560127223064586 val_loss0.3865864553935721\n",
            "iteration 195 :train_loss:0.41536679288610867 val_loss0.38636685595730563\n",
            "iteration 196 :train_loss:0.41513196600589597 val_loss0.3861099908166906\n",
            "iteration 197 :train_loss:0.41489739318566676 val_loss0.38589487999539973\n",
            "iteration 198 :train_loss:0.4146635174458856 val_loss0.38563063753834115\n",
            "iteration 199 :train_loss:0.4144301507214159 val_loss0.3854190506971784\n",
            "iteration 200 :train_loss:0.41419605442420926 val_loss0.38515520248468443\n",
            "iteration 201 :train_loss:0.41396252682650453 val_loss0.3849362793658744\n",
            "iteration 202 :train_loss:0.41373059027194675 val_loss0.3846815380094018\n",
            "iteration 203 :train_loss:0.41349745157184126 val_loss0.38446191267600394\n",
            "iteration 204 :train_loss:0.4132632810062146 val_loss0.38420352403109764\n",
            "iteration 205 :train_loss:0.4130289958474565 val_loss0.3839853159564503\n",
            "iteration 206 :train_loss:0.41279462005883416 val_loss0.3837251313196655\n",
            "iteration 207 :train_loss:0.41256036914723576 val_loss0.3835093040360043\n",
            "iteration 208 :train_loss:0.41232495322267165 val_loss0.38324762476764923\n",
            "iteration 209 :train_loss:0.4120896168228647 val_loss0.38303085293978334\n",
            "iteration 210 :train_loss:0.41185526960653945 val_loss0.3827724316668319\n",
            "iteration 211 :train_loss:0.41162179065802224 val_loss0.3825606428693829\n",
            "iteration 212 :train_loss:0.41138930852802097 val_loss0.38230219747629945\n",
            "iteration 213 :train_loss:0.41115879236493935 val_loss0.38209152522027306\n",
            "iteration 214 :train_loss:0.4109291073875699 val_loss0.3818428735345276\n",
            "iteration 215 :train_loss:0.41069855885456175 val_loss0.3816286953206896\n",
            "iteration 216 :train_loss:0.41046835178901653 val_loss0.3813826776776405\n",
            "iteration 217 :train_loss:0.41023702824957337 val_loss0.38116416196789793\n",
            "iteration 218 :train_loss:0.41000439709971576 val_loss0.3809171834174834\n",
            "iteration 219 :train_loss:0.40977156804181813 val_loss0.3806956451920722\n",
            "iteration 220 :train_loss:0.4095382356422142 val_loss0.3804480310096216\n",
            "iteration 221 :train_loss:0.4093054907009539 val_loss0.38022893227848203\n",
            "iteration 222 :train_loss:0.409071951881767 val_loss0.3799786041464721\n",
            "iteration 223 :train_loss:0.40883886624070953 val_loss0.3797616719115493\n",
            "iteration 224 :train_loss:0.40860658657396304 val_loss0.37951135173753536\n",
            "iteration 225 :train_loss:0.4083739636246793 val_loss0.3792943463470716\n",
            "iteration 226 :train_loss:0.4081414091993356 val_loss0.3790419186318991\n",
            "iteration 227 :train_loss:0.40791034365512435 val_loss0.37882411650241476\n",
            "iteration 228 :train_loss:0.4076807637940887 val_loss0.37857464562589266\n",
            "iteration 229 :train_loss:0.4074509509021278 val_loss0.3783562877409913\n",
            "iteration 230 :train_loss:0.407220033311372 val_loss0.37811116350690005\n",
            "iteration 231 :train_loss:0.4069909761484539 val_loss0.3778946648458675\n",
            "iteration 232 :train_loss:0.40676261004039316 val_loss0.3776538435772668\n",
            "iteration 233 :train_loss:0.4065335916383839 val_loss0.37744251685106517\n",
            "iteration 234 :train_loss:0.40630451908930765 val_loss0.3772031624906455\n",
            "iteration 235 :train_loss:0.4060758802693771 val_loss0.3769940941501599\n",
            "iteration 236 :train_loss:0.405849242864703 val_loss0.3767534418720989\n",
            "iteration 237 :train_loss:0.4056240302437307 val_loss0.37654628227957126\n",
            "iteration 238 :train_loss:0.4053997254439795 val_loss0.3763054857166154\n",
            "iteration 239 :train_loss:0.40517585339236034 val_loss0.3760999438115503\n",
            "iteration 240 :train_loss:0.40495250775466757 val_loss0.3758602924078758\n",
            "iteration 241 :train_loss:0.40472919955569847 val_loss0.37565398259071087\n",
            "iteration 242 :train_loss:0.4045066896248629 val_loss0.37541316581695633\n",
            "iteration 243 :train_loss:0.4042851129513963 val_loss0.375207734172584\n",
            "iteration 244 :train_loss:0.40406448958461727 val_loss0.37497150936156975\n",
            "iteration 245 :train_loss:0.40384480354345853 val_loss0.3747712929335132\n",
            "iteration 246 :train_loss:0.40362618662525174 val_loss0.37453389541368404\n",
            "iteration 247 :train_loss:0.4034076840535835 val_loss0.37433767988517197\n",
            "iteration 248 :train_loss:0.4031889594167617 val_loss0.37409734469346817\n",
            "iteration 249 :train_loss:0.40297075305282604 val_loss0.37390694279612974\n",
            "iteration 250 :train_loss:0.4027521984982604 val_loss0.37366362946418546\n",
            "iteration 251 :train_loss:0.4025352411737914 val_loss0.3734827779768994\n",
            "iteration 252 :train_loss:0.4023190409467958 val_loss0.3732330761195445\n",
            "iteration 253 :train_loss:0.4021039215610007 val_loss0.3730564442521624\n",
            "iteration 254 :train_loss:0.40188849571461677 val_loss0.3728042613669893\n",
            "iteration 255 :train_loss:0.40167328978412586 val_loss0.3726301245759261\n",
            "iteration 256 :train_loss:0.40146027173965915 val_loss0.37238667939899356\n",
            "iteration 257 :train_loss:0.40125012876875993 val_loss0.37221714668379663\n",
            "iteration 258 :train_loss:0.40104070389567326 val_loss0.3719751247412142\n",
            "iteration 259 :train_loss:0.40083275067143775 val_loss0.3718097791078824\n",
            "iteration 260 :train_loss:0.40062608303980884 val_loss0.3715661214395492\n",
            "iteration 261 :train_loss:0.40042006204186553 val_loss0.37140296871807105\n",
            "iteration 262 :train_loss:0.40021518439919346 val_loss0.3711614669642524\n",
            "iteration 263 :train_loss:0.40000970121201224 val_loss0.3709985160867109\n",
            "iteration 264 :train_loss:0.39980571624450956 val_loss0.37075863261226494\n",
            "iteration 265 :train_loss:0.3996038158820358 val_loss0.37060537844325037\n",
            "iteration 266 :train_loss:0.39940267675878904 val_loss0.37036359934867286\n",
            "iteration 267 :train_loss:0.3992020722204225 val_loss0.3702143949148527\n",
            "iteration 268 :train_loss:0.39900216729502147 val_loss0.36996927159734183\n",
            "iteration 269 :train_loss:0.3988033640622176 val_loss0.3698230431118898\n",
            "iteration 270 :train_loss:0.39860566643342815 val_loss0.3695774567576311\n",
            "iteration 271 :train_loss:0.3984077722096431 val_loss0.36943786030154974\n",
            "iteration 272 :train_loss:0.3982105510573373 val_loss0.36919051492393695\n",
            "iteration 273 :train_loss:0.39801383177091654 val_loss0.36905233053017766\n",
            "iteration 274 :train_loss:0.39781794881798765 val_loss0.36880003458744803\n",
            "iteration 275 :train_loss:0.39762310555487784 val_loss0.3686704849074728\n",
            "iteration 276 :train_loss:0.39742880332507513 val_loss0.3684166365280978\n",
            "iteration 277 :train_loss:0.397234758955738 val_loss0.3682911773455791\n",
            "iteration 278 :train_loss:0.39704256278785083 val_loss0.3680368454108001\n",
            "iteration 279 :train_loss:0.39685089486129516 val_loss0.36791780249287814\n",
            "iteration 280 :train_loss:0.3966594687734654 val_loss0.367661717409364\n",
            "iteration 281 :train_loss:0.3964691440077861 val_loss0.3675504387513554\n",
            "iteration 282 :train_loss:0.3962793824828012 val_loss0.3672895778962792\n",
            "iteration 283 :train_loss:0.3960909830006358 val_loss0.3671874067198084\n",
            "iteration 284 :train_loss:0.39590348444227824 val_loss0.3669223376495579\n",
            "iteration 285 :train_loss:0.39571826276496885 val_loss0.366830053055903\n",
            "iteration 286 :train_loss:0.39553317146413086 val_loss0.36655689431923566\n",
            "iteration 287 :train_loss:0.3953488082575586 val_loss0.36647094816665593\n",
            "iteration 288 :train_loss:0.3951663851623834 val_loss0.36619940911503246\n",
            "iteration 289 :train_loss:0.3949845278870812 val_loss0.3661270756208884\n",
            "iteration 290 :train_loss:0.3948031367532823 val_loss0.36584487463588933\n",
            "iteration 291 :train_loss:0.3946229911395924 val_loss0.3657868491451962\n",
            "iteration 292 :train_loss:0.3944441034809923 val_loss0.3654911674423943\n",
            "iteration 293 :train_loss:0.3942689257353056 val_loss0.36545467444028323\n",
            "iteration 294 :train_loss:0.3940937841341322 val_loss0.3651419211023585\n",
            "iteration 295 :train_loss:0.3939212851274194 val_loss0.36512802250569265\n",
            "iteration 296 :train_loss:0.39374866165078504 val_loss0.36479833455486826\n",
            "iteration 297 :train_loss:0.39357906281859356 val_loss0.3648079564332544\n",
            "iteration 298 :train_loss:0.3934093219766942 val_loss0.3644602043969715\n",
            "iteration 299 :train_loss:0.39324273308436297 val_loss0.3644931535704793\n",
            "iteration 300 :train_loss:0.393077324298783 val_loss0.36411910289570354\n",
            "iteration 301 :train_loss:0.3929147850306173 val_loss0.36418972721613285\n",
            "iteration 302 :train_loss:0.39275505030137425 val_loss0.36378740721878244\n",
            "iteration 303 :train_loss:0.39259962276174715 val_loss0.36390535351806713\n",
            "iteration 304 :train_loss:0.3924432318519499 val_loss0.36346513158485994\n",
            "iteration 305 :train_loss:0.39229405551689095 val_loss0.3636321781024076\n",
            "iteration 306 :train_loss:0.39214215174035144 val_loss0.36315066015718106\n",
            "iteration 307 :train_loss:0.3919974109969551 val_loss0.3633714637408648\n",
            "iteration 308 :train_loss:0.39185391745305154 val_loss0.3628465204091679\n",
            "iteration 309 :train_loss:0.3917197829594763 val_loss0.3631404967410645\n",
            "iteration 310 :train_loss:0.3915858321575427 val_loss0.362561105590719\n",
            "iteration 311 :train_loss:0.3914637761387467 val_loss0.36294236752300135\n",
            "iteration 312 :train_loss:0.3913403732058322 val_loss0.362299452607545\n",
            "iteration 313 :train_loss:0.3912341344806806 val_loss0.36277910319158974\n",
            "iteration 314 :train_loss:0.3911270544814613 val_loss0.3620664134078427\n",
            "iteration 315 :train_loss:0.39104876550910533 val_loss0.3626757223786888\n",
            "iteration 316 :train_loss:0.3909669567788053 val_loss0.36188285044871554\n",
            "iteration 317 :train_loss:0.39093175379428213 val_loss0.36265816154667413\n",
            "iteration 318 :train_loss:0.39088321584204877 val_loss0.36177239978594034\n",
            "iteration 319 :train_loss:0.39091072508953706 val_loss0.36276147391366725\n",
            "iteration 320 :train_loss:0.3909023262195204 val_loss0.36175987420684336\n",
            "iteration 321 :train_loss:0.3910197644938451 val_loss0.3630201947567922\n",
            "iteration 322 :train_loss:0.3910753869308494 val_loss0.36189308511535767\n",
            "iteration 323 :train_loss:0.3913345381718766 val_loss0.3635305007039456\n",
            "iteration 324 :train_loss:0.39143675185574717 val_loss0.3622137155053324\n",
            "iteration 325 :train_loss:0.3918544309144434 val_loss0.36427776733300493\n",
            "iteration 326 :train_loss:0.3920230013783252 val_loss0.3627614733665756\n",
            "iteration 327 :train_loss:0.3926665254418041 val_loss0.36535041841843\n",
            "iteration 328 :train_loss:0.3928523239944836 val_loss0.36356863512861454\n",
            "iteration 329 :train_loss:0.39378733778841635 val_loss0.3667660732354076\n",
            "iteration 330 :train_loss:0.39391683149170814 val_loss0.3646305487106909\n",
            "iteration 331 :train_loss:0.395230008403998 val_loss0.36853735571733776\n",
            "iteration 332 :train_loss:0.395271558177653 val_loss0.3659891030786708\n",
            "iteration 333 :train_loss:0.39696024300467614 val_loss0.37061345533955775\n",
            "iteration 334 :train_loss:0.3967204250587136 val_loss0.3674651792487891\n",
            "iteration 335 :train_loss:0.39863847150900134 val_loss0.37258741068049694\n",
            "iteration 336 :train_loss:0.3980230870510738 val_loss0.3688117638951886\n",
            "iteration 337 :train_loss:0.4000246844643521 val_loss0.37420247393773337\n",
            "iteration 338 :train_loss:0.39884168721396773 val_loss0.36967882574591127\n",
            "iteration 339 :train_loss:0.4007694370102392 val_loss0.37506775935971165\n",
            "iteration 340 :train_loss:0.3990839382624804 val_loss0.36995125367777193\n",
            "iteration 341 :train_loss:0.4006583615880005 val_loss0.374958544391842\n",
            "iteration 342 :train_loss:0.3985273408211513 val_loss0.36941505449113293\n",
            "iteration 343 :train_loss:0.39968592541857334 val_loss0.37389578171230914\n",
            "iteration 344 :train_loss:0.3974390313188709 val_loss0.36834951925637566\n",
            "iteration 345 :train_loss:0.3981806186376984 val_loss0.37224221337930957\n",
            "iteration 346 :train_loss:0.396036198753821 val_loss0.36697379470103736\n",
            "iteration 347 :train_loss:0.39647628197746265 val_loss0.37037153015038693\n",
            "iteration 348 :train_loss:0.3945902010840839 val_loss0.3655578261935066\n",
            "iteration 349 :train_loss:0.39481852795133027 val_loss0.36855433224938916\n",
            "iteration 350 :train_loss:0.3932015089913248 val_loss0.36420564512544273\n",
            "iteration 351 :train_loss:0.39335073886952193 val_loss0.3669524510874616\n",
            "iteration 352 :train_loss:0.39198865151277534 val_loss0.363038414137852\n",
            "iteration 353 :train_loss:0.392114057683005 val_loss0.36560814740752\n",
            "iteration 354 :train_loss:0.3909653638544251 val_loss0.36205761971501643\n",
            "iteration 355 :train_loss:0.3910722649775631 val_loss0.364482836266984\n",
            "iteration 356 :train_loss:0.39009289408203585 val_loss0.3612302976201245\n",
            "iteration 357 :train_loss:0.39016955759106825 val_loss0.36351588305181176\n",
            "iteration 358 :train_loss:0.389332618970743 val_loss0.36051468458548946\n",
            "iteration 359 :train_loss:0.38940265088734494 val_loss0.36269739110218735\n",
            "iteration 360 :train_loss:0.38868501315000065 val_loss0.35990559193792415\n",
            "iteration 361 :train_loss:0.3887454638491717 val_loss0.36200249422464825\n",
            "iteration 362 :train_loss:0.388141582777901 val_loss0.35939655204455473\n",
            "iteration 363 :train_loss:0.38821861799028456 val_loss0.36145781587853526\n",
            "iteration 364 :train_loss:0.38769665749832827 val_loss0.3589784651294135\n",
            "iteration 365 :train_loss:0.38780972946310843 val_loss0.3610512073927599\n",
            "iteration 366 :train_loss:0.38734966235820906 val_loss0.35865616980820975\n",
            "iteration 367 :train_loss:0.38746740655890743 val_loss0.3607231612873104\n",
            "iteration 368 :train_loss:0.38706830431903866 val_loss0.3584026688007838\n",
            "iteration 369 :train_loss:0.3872506189959849 val_loss0.3605430080614339\n",
            "iteration 370 :train_loss:0.3868828530448877 val_loss0.3582334261973977\n",
            "iteration 371 :train_loss:0.38709248263653767 val_loss0.3604287283467774\n",
            "iteration 372 :train_loss:0.38674740121459017 val_loss0.35811561990105256\n",
            "iteration 373 :train_loss:0.38699690353194277 val_loss0.3603866017845384\n",
            "iteration 374 :train_loss:0.38666716046744587 val_loss0.3580510436015088\n",
            "iteration 375 :train_loss:0.38698828959813664 val_loss0.36044413381345997\n",
            "iteration 376 :train_loss:0.3866670279472996 val_loss0.35806571249292585\n",
            "iteration 377 :train_loss:0.38706057818681294 val_loss0.36059756211988464\n",
            "iteration 378 :train_loss:0.38673244160499476 val_loss0.3581519747373499\n",
            "iteration 379 :train_loss:0.3871645764478901 val_loss0.3607815391959981\n",
            "iteration 380 :train_loss:0.3868262370662118 val_loss0.35826101850676445\n",
            "iteration 381 :train_loss:0.38733216001959425 val_loss0.36103645133644446\n",
            "iteration 382 :train_loss:0.3869831279420137 val_loss0.35843578790644215\n",
            "iteration 383 :train_loss:0.3875398506962002 val_loss0.3613344616375491\n",
            "iteration 384 :train_loss:0.38713119521431744 val_loss0.358609565867662\n",
            "iteration 385 :train_loss:0.38772672362791516 val_loss0.3616033236608561\n",
            "iteration 386 :train_loss:0.3872953450479153 val_loss0.35879723880931935\n",
            "iteration 387 :train_loss:0.3879327392917567 val_loss0.3618874478622787\n",
            "iteration 388 :train_loss:0.38747835126104707 val_loss0.359001618904024\n",
            "iteration 389 :train_loss:0.38812214645361465 val_loss0.3621484921871354\n",
            "iteration 390 :train_loss:0.3876123404892477 val_loss0.35915447214087265\n",
            "iteration 391 :train_loss:0.3883113230424254 val_loss0.3624067196099483\n",
            "iteration 392 :train_loss:0.38775058316825955 val_loss0.35931391463130996\n",
            "iteration 393 :train_loss:0.38845980370517064 val_loss0.36261210065068317\n",
            "iteration 394 :train_loss:0.3878386072109358 val_loss0.35941706498252163\n",
            "iteration 395 :train_loss:0.38855455443428755 val_loss0.36275139965687997\n",
            "iteration 396 :train_loss:0.387879794591259 val_loss0.3594758837225032\n",
            "iteration 397 :train_loss:0.38858471652731114 val_loss0.36281897212069103\n",
            "iteration 398 :train_loss:0.3878488277627603 val_loss0.3594584209631451\n",
            "iteration 399 :train_loss:0.3885270852023985 val_loss0.3627870939174158\n",
            "iteration 400 :train_loss:0.38775544237783605 val_loss0.3593860436505245\n",
            "iteration 401 :train_loss:0.3884277941160518 val_loss0.36271289364647286\n",
            "iteration 402 :train_loss:0.38762845203904905 val_loss0.3592873823988686\n",
            "iteration 403 :train_loss:0.38825671592790745 val_loss0.36256357105074544\n",
            "iteration 404 :train_loss:0.3874034108077429 val_loss0.35909399541114\n",
            "iteration 405 :train_loss:0.38797730425808713 val_loss0.36229298299872875\n",
            "iteration 406 :train_loss:0.3871026553561345 val_loss0.35881680085391077\n",
            "iteration 407 :train_loss:0.38766678686087164 val_loss0.3619849130356205\n",
            "iteration 408 :train_loss:0.38681214910174794 val_loss0.358553788874994\n",
            "iteration 409 :train_loss:0.3873201892763511 val_loss0.3616365873712428\n",
            "iteration 410 :train_loss:0.38648493208554097 val_loss0.35825258580189895\n",
            "iteration 411 :train_loss:0.3869994566840427 val_loss0.36131549503145083\n",
            "iteration 412 :train_loss:0.3862031095753514 val_loss0.35799994513879946\n",
            "iteration 413 :train_loss:0.3866820510371755 val_loss0.3610022178044885\n",
            "iteration 414 :train_loss:0.38591272479061306 val_loss0.3577384142856436\n",
            "iteration 415 :train_loss:0.3864077561075306 val_loss0.36073655180605063\n",
            "iteration 416 :train_loss:0.3856623639598025 val_loss0.3575095375167436\n",
            "iteration 417 :train_loss:0.3861779431536017 val_loss0.36051898480778294\n",
            "iteration 418 :train_loss:0.38546574893140517 val_loss0.35733586818606106\n",
            "iteration 419 :train_loss:0.3859916217647927 val_loss0.3603478231993262\n",
            "iteration 420 :train_loss:0.3852791640123338 val_loss0.35716889184796174\n",
            "iteration 421 :train_loss:0.38580469797692774 val_loss0.3601745628734986\n",
            "iteration 422 :train_loss:0.3850802266744851 val_loss0.3569909903071836\n",
            "iteration 423 :train_loss:0.38561541411231176 val_loss0.3599953424437599\n",
            "iteration 424 :train_loss:0.3848970818946722 val_loss0.35682584828358005\n",
            "iteration 425 :train_loss:0.3854270471698202 val_loss0.3598191188728924\n",
            "iteration 426 :train_loss:0.38473128880621793 val_loss0.3566804759764715\n",
            "iteration 427 :train_loss:0.38528553481982863 val_loss0.35969432838395354\n",
            "iteration 428 :train_loss:0.38458019620583384 val_loss0.3565496297862042\n",
            "iteration 429 :train_loss:0.38513253014585763 val_loss0.3595546229106765\n",
            "iteration 430 :train_loss:0.3844276452840924 val_loss0.35641921627582646\n",
            "iteration 431 :train_loss:0.38498773897222954 val_loss0.35942552417921453\n",
            "iteration 432 :train_loss:0.38428722965677503 val_loss0.3562965027757544\n",
            "iteration 433 :train_loss:0.3848514294364455 val_loss0.3593021059363027\n",
            "iteration 434 :train_loss:0.3841489117212933 val_loss0.35617633176558294\n",
            "iteration 435 :train_loss:0.3846442509100239 val_loss0.35910235742537644\n",
            "iteration 436 :train_loss:0.3839506200691751 val_loss0.35600575390027017\n",
            "iteration 437 :train_loss:0.3844628605156608 val_loss0.3589347988279688\n",
            "iteration 438 :train_loss:0.38379375455141956 val_loss0.35587643768518584\n",
            "iteration 439 :train_loss:0.38436485648672164 val_loss0.3588601239915347\n",
            "iteration 440 :train_loss:0.3837022859103258 val_loss0.3558116866768713\n",
            "iteration 441 :train_loss:0.3842331708141415 val_loss0.3587492808085154\n",
            "iteration 442 :train_loss:0.3835837026427006 val_loss0.3557225859481551\n",
            "iteration 443 :train_loss:0.38408640268126776 val_loss0.3586226956377556\n",
            "iteration 444 :train_loss:0.3834640079410326 val_loss0.3556287175989756\n",
            "iteration 445 :train_loss:0.3840421112124752 val_loss0.3586080409062442\n",
            "iteration 446 :train_loss:0.3834056988256849 val_loss0.35559704677166626\n",
            "iteration 447 :train_loss:0.38397441676750477 val_loss0.35855981577156665\n",
            "iteration 448 :train_loss:0.38332219056177347 val_loss0.35554342352443435\n",
            "iteration 449 :train_loss:0.3838876539889695 val_loss0.35849157195632547\n",
            "iteration 450 :train_loss:0.3832369534240191 val_loss0.3554879254907231\n",
            "iteration 451 :train_loss:0.38386493356629026 val_loss0.3584927908531142\n",
            "iteration 452 :train_loss:0.38320971496914996 val_loss0.3554876019411469\n",
            "iteration 453 :train_loss:0.38382381430246765 val_loss0.35847493323998747\n",
            "iteration 454 :train_loss:0.3831453281206195 val_loss0.3554548680607197\n",
            "iteration 455 :train_loss:0.3837224332494972 val_loss0.3583892911712524\n",
            "iteration 456 :train_loss:0.38305441900681275 val_loss0.3553910983073654\n",
            "iteration 457 :train_loss:0.38365785137831526 val_loss0.3583439911388221\n",
            "iteration 458 :train_loss:0.3829776011567802 val_loss0.35533658723392425\n",
            "iteration 459 :train_loss:0.3834913073104528 val_loss0.3581847920678774\n",
            "iteration 460 :train_loss:0.3828369654911681 val_loss0.3552179693460668\n",
            "iteration 461 :train_loss:0.3834222695983637 val_loss0.3581380668859837\n",
            "iteration 462 :train_loss:0.3827625465469522 val_loss0.3551628071009716\n",
            "iteration 463 :train_loss:0.3832758611781774 val_loss0.3580007295314016\n",
            "iteration 464 :train_loss:0.38263875603476644 val_loss0.3550567584101288\n",
            "iteration 465 :train_loss:0.38324669532638617 val_loss0.35800054337701587\n",
            "iteration 466 :train_loss:0.3825975547668008 val_loss0.3550349789964418\n",
            "iteration 467 :train_loss:0.3831288490594445 val_loss0.3578984546333149\n",
            "iteration 468 :train_loss:0.3825038901245599 val_loss0.3549598572278755\n",
            "iteration 469 :train_loss:0.3830755920699651 val_loss0.3578694964702581\n",
            "iteration 470 :train_loss:0.3824693559837949 val_loss0.35494102068445244\n",
            "iteration 471 :train_loss:0.3831218080138126 val_loss0.35794497527418767\n",
            "iteration 472 :train_loss:0.3824707964020531 val_loss0.3549584606139445\n",
            "iteration 473 :train_loss:0.38301883900593975 val_loss0.3578539610478488\n",
            "iteration 474 :train_loss:0.3823953491363937 val_loss0.3548966909645077\n",
            "iteration 475 :train_loss:0.3829739793806802 val_loss0.3578302618850422\n",
            "iteration 476 :train_loss:0.38236270931652194 val_loss0.3548733116538646\n",
            "iteration 477 :train_loss:0.3829685823784326 val_loss0.35784564815908443\n",
            "iteration 478 :train_loss:0.3823588612532341 val_loss0.35488271352090134\n",
            "iteration 479 :train_loss:0.38301248114021647 val_loss0.35791552280322037\n",
            "iteration 480 :train_loss:0.38235561571321935 val_loss0.3548929704890382\n",
            "iteration 481 :train_loss:0.38294695810978746 val_loss0.3578631239931074\n",
            "iteration 482 :train_loss:0.38227453584370535 val_loss0.354828319716671\n",
            "iteration 483 :train_loss:0.3828802675697494 val_loss0.35781111171833097\n",
            "iteration 484 :train_loss:0.38218910677645684 val_loss0.3547577805398747\n",
            "iteration 485 :train_loss:0.38278711017771855 val_loss0.3577315135140066\n",
            "iteration 486 :train_loss:0.38209313224603786 val_loss0.3546758673164481\n",
            "iteration 487 :train_loss:0.38270243870513426 val_loss0.35766334984948167\n",
            "iteration 488 :train_loss:0.3820113803693013 val_loss0.3546093255965127\n",
            "iteration 489 :train_loss:0.38257711949672035 val_loss0.35755010603943876\n",
            "iteration 490 :train_loss:0.38189307732837663 val_loss0.3545034088238461\n",
            "iteration 491 :train_loss:0.3824145977991791 val_loss0.35739515576279457\n",
            "iteration 492 :train_loss:0.38173189000816266 val_loss0.3543566744911912\n",
            "iteration 493 :train_loss:0.3823215626504087 val_loss0.3573151075685044\n",
            "iteration 494 :train_loss:0.3816492898832583 val_loss0.3542847994422124\n",
            "iteration 495 :train_loss:0.38215539478293076 val_loss0.3571518311562124\n",
            "iteration 496 :train_loss:0.38147235943213814 val_loss0.3541199879200676\n",
            "iteration 497 :train_loss:0.3819899169445999 val_loss0.3569885932841473\n",
            "iteration 498 :train_loss:0.38132626551211707 val_loss0.3539859466351988\n",
            "iteration 499 :train_loss:0.3817967005168777 val_loss0.3567927921558497\n",
            "iteration 500 :train_loss:0.3811472264153604 val_loss0.35381650183908087\n",
            "iteration 501 :train_loss:0.38166282052098155 val_loss0.35666154557865526\n",
            "iteration 502 :train_loss:0.38101876117917605 val_loss0.35370046660362436\n",
            "iteration 503 :train_loss:0.3815051915757423 val_loss0.3565089950071834\n",
            "iteration 504 :train_loss:0.38086976763598235 val_loss0.3535629634372882\n",
            "iteration 505 :train_loss:0.38136346125152193 val_loss0.35637420295254285\n",
            "iteration 506 :train_loss:0.3807649919303535 val_loss0.3534717891249087\n",
            "iteration 507 :train_loss:0.3812718151256641 val_loss0.35629615911384727\n",
            "iteration 508 :train_loss:0.3806764470517566 val_loss0.3533922605751538\n",
            "iteration 509 :train_loss:0.3812427923711279 val_loss0.35628534722283894\n",
            "iteration 510 :train_loss:0.38064494141694777 val_loss0.35337215025891255\n",
            "iteration 511 :train_loss:0.3811316840550953 val_loss0.3561810454087934\n",
            "iteration 512 :train_loss:0.3805505246289224 val_loss0.35328665784811114\n",
            "iteration 513 :train_loss:0.3811036630479373 val_loss0.3561712057419216\n",
            "iteration 514 :train_loss:0.38054490291551607 val_loss0.35329271116484184\n",
            "iteration 515 :train_loss:0.3810757823877706 val_loss0.35616128055141827\n",
            "iteration 516 :train_loss:0.3805144219147632 val_loss0.35326917032662897\n",
            "iteration 517 :train_loss:0.3811122883399136 val_loss0.35622168825168565\n",
            "iteration 518 :train_loss:0.38052697027355675 val_loss0.3532935418247881\n",
            "iteration 519 :train_loss:0.38107413604883095 val_loss0.3561949413210227\n",
            "iteration 520 :train_loss:0.3804878444697306 val_loss0.3532691239789309\n",
            "iteration 521 :train_loss:0.3810348695706346 val_loss0.3561665702570985\n",
            "iteration 522 :train_loss:0.3804713849038449 val_loss0.3532581679766382\n",
            "iteration 523 :train_loss:0.381010159706272 val_loss0.35615544467709614\n",
            "iteration 524 :train_loss:0.3804385023712838 val_loss0.35323258911090716\n",
            "iteration 525 :train_loss:0.38102216768340635 val_loss0.3561860117519604\n",
            "iteration 526 :train_loss:0.38042522367028125 val_loss0.3532317598119781\n",
            "iteration 527 :train_loss:0.38092437107923266 val_loss0.3560952933778498\n",
            "iteration 528 :train_loss:0.3803287037097418 val_loss0.3531425222221148\n",
            "iteration 529 :train_loss:0.3808324426719588 val_loss0.35601543320850326\n",
            "iteration 530 :train_loss:0.3802344722345348 val_loss0.35305902782735465\n",
            "iteration 531 :train_loss:0.3807982310927055 val_loss0.3559942562876975\n",
            "iteration 532 :train_loss:0.38019133540571104 val_loss0.3530253419863109\n",
            "iteration 533 :train_loss:0.3806849523207025 val_loss0.3558866691347969\n",
            "iteration 534 :train_loss:0.3800704080682388 val_loss0.35291937432324505\n",
            "iteration 535 :train_loss:0.38062839056911624 val_loss0.35584603227355954\n",
            "iteration 536 :train_loss:0.38000802111973325 val_loss0.3528674176656029\n",
            "iteration 537 :train_loss:0.3804442204048168 val_loss0.35565763190480815\n",
            "iteration 538 :train_loss:0.3798360365008047 val_loss0.3527044728136298\n",
            "iteration 539 :train_loss:0.38036956805811706 val_loss0.3555921502862732\n",
            "iteration 540 :train_loss:0.3797490476474775 val_loss0.3526269540472776\n",
            "iteration 541 :train_loss:0.3801901811462561 val_loss0.35540736796328243\n",
            "iteration 542 :train_loss:0.3796046075518832 val_loss0.35248958370372613\n",
            "iteration 543 :train_loss:0.3801128851030201 val_loss0.3553358898093794\n",
            "iteration 544 :train_loss:0.37950447638985635 val_loss0.35239851521403043\n",
            "iteration 545 :train_loss:0.3799215559480187 val_loss0.35513842273084295\n",
            "iteration 546 :train_loss:0.3793506875262211 val_loss0.3522516464162348\n",
            "iteration 547 :train_loss:0.3798885734961953 val_loss0.35511707469014653\n",
            "iteration 548 :train_loss:0.37930992387111845 val_loss0.3522166224497625\n",
            "iteration 549 :train_loss:0.3797478535744812 val_loss0.3549707644724789\n",
            "iteration 550 :train_loss:0.37919863694695105 val_loss0.3521120591396609\n",
            "iteration 551 :train_loss:0.37973525689681575 val_loss0.35497153519973157\n",
            "iteration 552 :train_loss:0.3791744172466906 val_loss0.3520974688772711\n",
            "iteration 553 :train_loss:0.37962640798639785 val_loss0.3548626379922696\n",
            "iteration 554 :train_loss:0.37907417502538676 val_loss0.35200443711175644\n",
            "iteration 555 :train_loss:0.3795323744483915 val_loss0.3547735341527103\n",
            "iteration 556 :train_loss:0.37898875276582933 val_loss0.35192679879836886\n",
            "iteration 557 :train_loss:0.3794322876333219 val_loss0.35467165105889925\n",
            "iteration 558 :train_loss:0.37890253486801356 val_loss0.35184311358226816\n",
            "iteration 559 :train_loss:0.37933310075412896 val_loss0.3545744529299513\n",
            "iteration 560 :train_loss:0.3788219235861475 val_loss0.35176436892555035\n",
            "iteration 561 :train_loss:0.3792881842689203 val_loss0.3545320831529546\n",
            "iteration 562 :train_loss:0.3787881674593594 val_loss0.3517331390689119\n",
            "iteration 563 :train_loss:0.37921963011900145 val_loss0.3544643997808835\n",
            "iteration 564 :train_loss:0.37872849180349927 val_loss0.3516741368296469\n",
            "iteration 565 :train_loss:0.37916095840170516 val_loss0.35440403896405376\n",
            "iteration 566 :train_loss:0.378671408104995 val_loss0.35161613953636506\n",
            "iteration 567 :train_loss:0.37911676698021046 val_loss0.3543614043866484\n",
            "iteration 568 :train_loss:0.3786381368405187 val_loss0.35158260814454234\n",
            "iteration 569 :train_loss:0.37910878428346073 val_loss0.3543561481445833\n",
            "iteration 570 :train_loss:0.3786054516172521 val_loss0.3515488490036516\n",
            "iteration 571 :train_loss:0.3790070129689707 val_loss0.35424917092169067\n",
            "iteration 572 :train_loss:0.37853252546213856 val_loss0.3514702434966315\n",
            "iteration 573 :train_loss:0.3790038210527077 val_loss0.35424685824125973\n",
            "iteration 574 :train_loss:0.37852844744414077 val_loss0.3514677533164773\n",
            "iteration 575 :train_loss:0.3789487818190664 val_loss0.35418852567398235\n",
            "iteration 576 :train_loss:0.37847309500101034 val_loss0.3514087257514357\n",
            "iteration 577 :train_loss:0.37893476072465343 val_loss0.35417883818917983\n",
            "iteration 578 :train_loss:0.3784737445770061 val_loss0.351409108523285\n",
            "iteration 579 :train_loss:0.37892012935306674 val_loss0.3541642767481517\n",
            "iteration 580 :train_loss:0.3784626443125126 val_loss0.35139907807732135\n",
            "iteration 581 :train_loss:0.3789491271413035 val_loss0.35420292123153424\n",
            "iteration 582 :train_loss:0.3784856863691477 val_loss0.35142885932339246\n",
            "iteration 583 :train_loss:0.3789221178909021 val_loss0.35417574219959747\n",
            "iteration 584 :train_loss:0.37846687547445645 val_loss0.35141067642910895\n",
            "iteration 585 :train_loss:0.37895891204155513 val_loss0.35421813987595374\n",
            "iteration 586 :train_loss:0.3784790761968215 val_loss0.3514326894289757\n",
            "iteration 587 :train_loss:0.3789152298295209 val_loss0.35417190983893787\n",
            "iteration 588 :train_loss:0.3784240759041831 val_loss0.35138091471971833\n",
            "iteration 589 :train_loss:0.3788611025227966 val_loss0.3541137687823963\n",
            "iteration 590 :train_loss:0.3783594375437523 val_loss0.35132077308870424\n",
            "iteration 591 :train_loss:0.3787903938299098 val_loss0.3540326670608748\n",
            "iteration 592 :train_loss:0.3782816914114921 val_loss0.35124242021228863\n",
            "iteration 593 :train_loss:0.3786991881212447 val_loss0.3539292847243699\n",
            "iteration 594 :train_loss:0.3781893936144678 val_loss0.3511538734050632\n",
            "iteration 595 :train_loss:0.3785960632684595 val_loss0.3538162059153876\n",
            "iteration 596 :train_loss:0.37809894352313306 val_loss0.3510648078008771\n",
            "iteration 597 :train_loss:0.37851483324951124 val_loss0.3537315417709684\n",
            "iteration 598 :train_loss:0.37802125893111876 val_loss0.35099219563391476\n",
            "iteration 599 :train_loss:0.3784316650608299 val_loss0.3536393635959571\n",
            "iteration 600 :train_loss:0.3779336613986619 val_loss0.35090672403317946\n",
            "iteration 601 :train_loss:0.37831635057302543 val_loss0.35351617138692787\n",
            "iteration 602 :train_loss:0.377820757233826 val_loss0.35079955288340475\n",
            "iteration 603 :train_loss:0.3781971329508179 val_loss0.35338763243020455\n",
            "iteration 604 :train_loss:0.37771203797817937 val_loss0.3506938570176183\n",
            "iteration 605 :train_loss:0.37811968087300546 val_loss0.35330746209813935\n",
            "iteration 606 :train_loss:0.37762598210117854 val_loss0.3506130673895647\n",
            "iteration 607 :train_loss:0.37795634420154145 val_loss0.35313179263425803\n",
            "iteration 608 :train_loss:0.3774886559396516 val_loss0.3504782286112258\n",
            "iteration 609 :train_loss:0.3779140381472851 val_loss0.3530951810210192\n",
            "iteration 610 :train_loss:0.3774366537133677 val_loss0.3504328329118207\n",
            "iteration 611 :train_loss:0.3778405416355869 val_loss0.3530213307440716\n",
            "iteration 612 :train_loss:0.3773779537484628 val_loss0.35037941585705434\n",
            "iteration 613 :train_loss:0.37778142704074574 val_loss0.35296224511032104\n",
            "iteration 614 :train_loss:0.37731713821601665 val_loss0.3503259017629779\n",
            "iteration 615 :train_loss:0.37773865385057837 val_loss0.35292226834627144\n",
            "iteration 616 :train_loss:0.3772873358480109 val_loss0.3503040058008714\n",
            "iteration 617 :train_loss:0.3777066897573661 val_loss0.3528919679827805\n",
            "iteration 618 :train_loss:0.37726585688157915 val_loss0.35028420528484305\n",
            "iteration 619 :train_loss:0.3776616602294 val_loss0.35284907812974403\n",
            "iteration 620 :train_loss:0.3772113049792031 val_loss0.3502315567756834\n",
            "iteration 621 :train_loss:0.3775883444683107 val_loss0.3527760604435453\n",
            "iteration 622 :train_loss:0.3771392202309136 val_loss0.35016207834007973\n",
            "iteration 623 :train_loss:0.3775570568435777 val_loss0.35275000256421246\n",
            "iteration 624 :train_loss:0.3771095050640688 val_loss0.3501355089464819\n",
            "iteration 625 :train_loss:0.3775361120637007 val_loss0.35273392129966513\n",
            "iteration 626 :train_loss:0.3770911309511959 val_loss0.3501199075162804\n",
            "iteration 627 :train_loss:0.3775247955809566 val_loss0.35272838590583666\n",
            "iteration 628 :train_loss:0.3770573092332617 val_loss0.35009593668646066\n",
            "iteration 629 :train_loss:0.37747473390781083 val_loss0.35268485097695607\n",
            "iteration 630 :train_loss:0.3770046236793122 val_loss0.3500473871852471\n",
            "iteration 631 :train_loss:0.37739544276635484 val_loss0.3526097108942286\n",
            "iteration 632 :train_loss:0.37696306590787315 val_loss0.35001195078526176\n",
            "iteration 633 :train_loss:0.3773627632488669 val_loss0.352585669035425\n",
            "iteration 634 :train_loss:0.37693271130493333 val_loss0.3499842502553193\n",
            "iteration 635 :train_loss:0.37736139456739154 val_loss0.3525953246637447\n",
            "iteration 636 :train_loss:0.3769353244657375 val_loss0.3499944872595268\n",
            "iteration 637 :train_loss:0.37735465430234316 val_loss0.3526006695714941\n",
            "iteration 638 :train_loss:0.3769396274964751 val_loss0.35000055128521773\n",
            "iteration 639 :train_loss:0.3773823983047714 val_loss0.35264190735304146\n",
            "iteration 640 :train_loss:0.37695392237289493 val_loss0.3500208336390684\n",
            "iteration 641 :train_loss:0.3773594099917536 val_loss0.35262168845662095\n",
            "iteration 642 :train_loss:0.37694095279152673 val_loss0.35000984050329265\n",
            "iteration 643 :train_loss:0.37734138232110853 val_loss0.3526071311939049\n",
            "iteration 644 :train_loss:0.3769341271482473 val_loss0.3500036436766535\n",
            "iteration 645 :train_loss:0.37735028083126426 val_loss0.35262221765537866\n",
            "iteration 646 :train_loss:0.3769289941808426 val_loss0.3499995356904782\n",
            "iteration 647 :train_loss:0.3773523136168907 val_loss0.35263143777527545\n",
            "iteration 648 :train_loss:0.37692502405848943 val_loss0.3499999484422186\n",
            "iteration 649 :train_loss:0.3772598090819298 val_loss0.35253432575846955\n",
            "iteration 650 :train_loss:0.3768586981811372 val_loss0.34993269317925857\n",
            "iteration 651 :train_loss:0.3772252665416725 val_loss0.352502973876168\n",
            "iteration 652 :train_loss:0.3767954555032169 val_loss0.34987802834250326\n",
            "iteration 653 :train_loss:0.37715947579923226 val_loss0.35243744911738734\n",
            "iteration 654 :train_loss:0.3767378829096179 val_loss0.3498220729601663\n",
            "iteration 655 :train_loss:0.37708133255949583 val_loss0.352358748209896\n",
            "iteration 656 :train_loss:0.37666268728581837 val_loss0.3497546710900748\n",
            "iteration 657 :train_loss:0.3770061357282325 val_loss0.35227924616271133\n",
            "iteration 658 :train_loss:0.3765936882326212 val_loss0.3496897305192748\n",
            "iteration 659 :train_loss:0.3769246953026399 val_loss0.3521931377000746\n",
            "iteration 660 :train_loss:0.37652679804603 val_loss0.3496313161348749\n",
            "iteration 661 :train_loss:0.3768612088356524 val_loss0.35213090846576023\n",
            "iteration 662 :train_loss:0.37648117169985024 val_loss0.34959161393604476\n",
            "iteration 663 :train_loss:0.37682589562771623 val_loss0.3521020626833136\n",
            "iteration 664 :train_loss:0.37645804656032167 val_loss0.34957709923304553\n",
            "iteration 665 :train_loss:0.37678922238979656 val_loss0.35206402006412013\n",
            "iteration 666 :train_loss:0.37643521214787895 val_loss0.3495655447019441\n",
            "iteration 667 :train_loss:0.3767719533661673 val_loss0.3520522950072101\n",
            "iteration 668 :train_loss:0.376431814528863 val_loss0.34957507276730115\n",
            "iteration 669 :train_loss:0.3767073954051796 val_loss0.35198797900650275\n",
            "iteration 670 :train_loss:0.3763633789654954 val_loss0.3495184224345217\n",
            "iteration 671 :train_loss:0.37666896848237014 val_loss0.35195487882935367\n",
            "iteration 672 :train_loss:0.37631221177924634 val_loss0.34948719832536157\n",
            "iteration 673 :train_loss:0.37654975353178916 val_loss0.3518367479934759\n",
            "iteration 674 :train_loss:0.3761994878789565 val_loss0.3493856919074055\n",
            "iteration 675 :train_loss:0.3765149384175095 val_loss0.3518117762414811\n",
            "iteration 676 :train_loss:0.3761518802106609 val_loss0.34936403699234325\n",
            "iteration 677 :train_loss:0.3764026408127594 val_loss0.3517016377564972\n",
            "iteration 678 :train_loss:0.3760374555876377 val_loss0.34926471882933474\n",
            "iteration 679 :train_loss:0.37628076624508094 val_loss0.3515856384586539\n",
            "iteration 680 :train_loss:0.375937020582692 val_loss0.34918128440451013\n",
            "iteration 681 :train_loss:0.3762171865676703 val_loss0.35153559778331245\n",
            "iteration 682 :train_loss:0.37587415362909077 val_loss0.3491336974500762\n",
            "iteration 683 :train_loss:0.37626670138151835 val_loss0.3515920375483066\n",
            "iteration 684 :train_loss:0.375948260106261 val_loss0.3492314187151388\n",
            "iteration 685 :train_loss:0.376253153067581 val_loss0.35158088912119584\n",
            "iteration 686 :train_loss:0.3759465659377659 val_loss0.3492477492139398\n",
            "iteration 687 :train_loss:0.376345525903573 val_loss0.35169254947425294\n",
            "iteration 688 :train_loss:0.37603737215053756 val_loss0.3493566931729836\n",
            "iteration 689 :train_loss:0.3764636124241192 val_loss0.3518307356646215\n",
            "iteration 690 :train_loss:0.3761339126519016 val_loss0.3494722013024299\n",
            "iteration 691 :train_loss:0.37648211292802125 val_loss0.35185677871238935\n",
            "iteration 692 :train_loss:0.37616426703888517 val_loss0.34951630343688694\n",
            "iteration 693 :train_loss:0.3765807434963867 val_loss0.3519692535070338\n",
            "iteration 694 :train_loss:0.3762662923864278 val_loss0.3496362959732572\n",
            "iteration 695 :train_loss:0.3766402027581513 val_loss0.35204241991230484\n",
            "iteration 696 :train_loss:0.37631908385445134 val_loss0.34969930863684545\n",
            "iteration 697 :train_loss:0.37668471862186037 val_loss0.3520965851982366\n",
            "iteration 698 :train_loss:0.3763571415703646 val_loss0.34975494606176555\n",
            "iteration 699 :train_loss:0.3766879828461819 val_loss0.35210720239249194\n",
            "iteration 700 :train_loss:0.3763516441665578 val_loss0.34975708976582875\n",
            "iteration 701 :train_loss:0.376692605531684 val_loss0.3521204623169602\n",
            "iteration 702 :train_loss:0.3763522474861877 val_loss0.34977032007226394\n",
            "iteration 703 :train_loss:0.3766742481040787 val_loss0.35210514513974267\n",
            "iteration 704 :train_loss:0.3763194073610868 val_loss0.34975174884584026\n",
            "iteration 705 :train_loss:0.37656083906867543 val_loss0.35199193396258266\n",
            "iteration 706 :train_loss:0.3762196553266441 val_loss0.3496660536857363\n",
            "iteration 707 :train_loss:0.3765290657867041 val_loss0.35197171981144476\n",
            "iteration 708 :train_loss:0.3761901781297868 val_loss0.3496468915995193\n",
            "iteration 709 :train_loss:0.3764174129084887 val_loss0.3518577165639416\n",
            "iteration 710 :train_loss:0.3760787574983214 val_loss0.3495486738341171\n",
            "iteration 711 :train_loss:0.37639185892915805 val_loss0.35184184737392754\n",
            "iteration 712 :train_loss:0.37604588442420134 val_loss0.3495227755830243\n",
            "iteration 713 :train_loss:0.37623806742416954 val_loss0.3516836690090158\n",
            "iteration 714 :train_loss:0.3758960950420709 val_loss0.34937956926659874\n",
            "iteration 715 :train_loss:0.37619284445640455 val_loss0.35164650049093216\n",
            "iteration 716 :train_loss:0.3758288974704998 val_loss0.34932039984812413\n",
            "iteration 717 :train_loss:0.3760529794403641 val_loss0.3515065186111534\n",
            "iteration 718 :train_loss:0.37568427299812635 val_loss0.34918596794037\n",
            "iteration 719 :train_loss:0.3758622542216 val_loss0.35131249853650554\n",
            "iteration 720 :train_loss:0.375515359111499 val_loss0.3490238634573996\n",
            "iteration 721 :train_loss:0.37570265701844735 val_loss0.35115797679911476\n",
            "iteration 722 :train_loss:0.3753512889100759 val_loss0.3488737410395753\n",
            "iteration 723 :train_loss:0.375510231413804 val_loss0.35096513855172146\n",
            "iteration 724 :train_loss:0.37514131466107525 val_loss0.34867216775682003\n",
            "iteration 725 :train_loss:0.37528598830697296 val_loss0.3507373873225143\n",
            "iteration 726 :train_loss:0.3749375114651913 val_loss0.34848002824932\n",
            "iteration 727 :train_loss:0.3751005526403034 val_loss0.3505520973077359\n",
            "iteration 728 :train_loss:0.37476691439735943 val_loss0.3483180999692141\n",
            "iteration 729 :train_loss:0.37491706373657796 val_loss0.3503675058574585\n",
            "iteration 730 :train_loss:0.3745852273844144 val_loss0.34814902886738935\n",
            "iteration 731 :train_loss:0.3747444419990003 val_loss0.35019303585337647\n",
            "iteration 732 :train_loss:0.37444335380218324 val_loss0.3480181767337686\n",
            "iteration 733 :train_loss:0.3746085142918736 val_loss0.3500622446696607\n",
            "iteration 734 :train_loss:0.3743198350123471 val_loss0.347908515682949\n",
            "iteration 735 :train_loss:0.3744477700757575 val_loss0.349901688261872\n",
            "iteration 736 :train_loss:0.37416877910412416 val_loss0.3477705727811305\n",
            "iteration 737 :train_loss:0.37436456145589714 val_loss0.34982751445462246\n",
            "iteration 738 :train_loss:0.374069312106809 val_loss0.3476841259793955\n",
            "iteration 739 :train_loss:0.3742018862797376 val_loss0.3496608325034859\n",
            "iteration 740 :train_loss:0.37392304652688596 val_loss0.34754921012564916\n",
            "iteration 741 :train_loss:0.37411722002839587 val_loss0.3495816474494026\n",
            "iteration 742 :train_loss:0.3738474244217676 val_loss0.34748247776721464\n",
            "iteration 743 :train_loss:0.3739836741569385 val_loss0.3494483513587263\n",
            "iteration 744 :train_loss:0.37374323579159086 val_loss0.3473867820475483\n",
            "iteration 745 :train_loss:0.3739925263713152 val_loss0.34946916040852793\n",
            "iteration 746 :train_loss:0.37374727447379197 val_loss0.34740121467138524\n",
            "iteration 747 :train_loss:0.37388987897250253 val_loss0.3493644352374367\n",
            "iteration 748 :train_loss:0.37366065940088466 val_loss0.34732171678316376\n",
            "iteration 749 :train_loss:0.3739143262690933 val_loss0.34940201810116667\n",
            "iteration 750 :train_loss:0.3737059858787744 val_loss0.3473772422744403\n",
            "iteration 751 :train_loss:0.37396794773453723 val_loss0.34946280442624617\n",
            "iteration 752 :train_loss:0.37374817585718545 val_loss0.3474249361929735\n",
            "iteration 753 :train_loss:0.37396453984825523 val_loss0.3494638053243052\n",
            "iteration 754 :train_loss:0.37376925821565515 val_loss0.3474524976739942\n",
            "iteration 755 :train_loss:0.37400910823714884 val_loss0.34951660589132105\n",
            "iteration 756 :train_loss:0.37382106155550765 val_loss0.34751094042587954\n",
            "iteration 757 :train_loss:0.3740738696574807 val_loss0.3495912045242875\n",
            "iteration 758 :train_loss:0.37388051169857467 val_loss0.34757613830603823\n",
            "iteration 759 :train_loss:0.3741555845371164 val_loss0.3496849280300464\n",
            "iteration 760 :train_loss:0.3739487002094258 val_loss0.34765159262866857\n",
            "iteration 761 :train_loss:0.3741573580468232 val_loss0.3496918653211857\n",
            "iteration 762 :train_loss:0.37396053251621475 val_loss0.34767131803730117\n",
            "iteration 763 :train_loss:0.37423913575281453 val_loss0.34978540370148264\n",
            "iteration 764 :train_loss:0.37403054427639065 val_loss0.34774950790750186\n",
            "iteration 765 :train_loss:0.3742812559756891 val_loss0.34983513657398774\n",
            "iteration 766 :train_loss:0.37405247122419455 val_loss0.3477726321716224\n",
            "iteration 767 :train_loss:0.3742321375428119 val_loss0.3497844887271529\n",
            "iteration 768 :train_loss:0.37402509969489006 val_loss0.34775231879272706\n",
            "iteration 769 :train_loss:0.37428201673341416 val_loss0.349845597356087\n",
            "iteration 770 :train_loss:0.3740564519281703 val_loss0.34779248861408923\n",
            "iteration 771 :train_loss:0.3742102967880333 val_loss0.3497733191734047\n",
            "iteration 772 :train_loss:0.37397718584767314 val_loss0.3477182774746054\n",
            "iteration 773 :train_loss:0.37417716391293154 val_loss0.3497427676188082\n",
            "iteration 774 :train_loss:0.3739462738579193 val_loss0.347693643798733\n",
            "iteration 775 :train_loss:0.37409712425454805 val_loss0.3496606425079947\n",
            "iteration 776 :train_loss:0.37386148155052124 val_loss0.3476151314091422\n",
            "iteration 777 :train_loss:0.37407182649755216 val_loss0.34963890272873627\n",
            "iteration 778 :train_loss:0.3738125301159501 val_loss0.3475709745785839\n",
            "iteration 779 :train_loss:0.37395316822231767 val_loss0.3495146383754722\n",
            "iteration 780 :train_loss:0.37369444183630424 val_loss0.34745656239830636\n",
            "iteration 781 :train_loss:0.37387009394636056 val_loss0.3494318353186924\n",
            "iteration 782 :train_loss:0.37360089754500375 val_loss0.34736989553088515\n",
            "iteration 783 :train_loss:0.37370488730400103 val_loss0.34925582384838183\n",
            "iteration 784 :train_loss:0.3734471018307003 val_loss0.3472204174967042\n",
            "iteration 785 :train_loss:0.37356974372811463 val_loss0.3491176992302665\n",
            "iteration 786 :train_loss:0.373310212926494 val_loss0.3470914716983673\n",
            "iteration 787 :train_loss:0.37343281569481135 val_loss0.348976367383023\n",
            "iteration 788 :train_loss:0.37320477395755886 val_loss0.3469914122186215\n",
            "iteration 789 :train_loss:0.3733276815862729 val_loss0.3488709225510758\n",
            "iteration 790 :train_loss:0.3730972396421513 val_loss0.3468913720133664\n",
            "iteration 791 :train_loss:0.373204843303842 val_loss0.3487465368475698\n",
            "iteration 792 :train_loss:0.37294069392993234 val_loss0.3467382698811958\n",
            "iteration 793 :train_loss:0.3730753227623164 val_loss0.3486146222751635\n",
            "iteration 794 :train_loss:0.3728534636991948 val_loss0.34665376928878244\n",
            "iteration 795 :train_loss:0.37297149513500727 val_loss0.34850903576241055\n",
            "iteration 796 :train_loss:0.37275683693504025 val_loss0.34655758677073706\n",
            "iteration 797 :train_loss:0.3728764432820671 val_loss0.34841349230813484\n",
            "iteration 798 :train_loss:0.3726521155842335 val_loss0.34645545857894994\n",
            "iteration 799 :train_loss:0.37275891967268787 val_loss0.3482920971571351\n",
            "iteration 800 :train_loss:0.3725596591035055 val_loss0.3463652407408451\n",
            "iteration 801 :train_loss:0.37274392247951094 val_loss0.34828354000633355\n",
            "iteration 802 :train_loss:0.3725230978507029 val_loss0.34633287746832064\n",
            "iteration 803 :train_loss:0.3726408305840756 val_loss0.34817641257722093\n",
            "iteration 804 :train_loss:0.37243348317912917 val_loss0.34624057640522643\n",
            "iteration 805 :train_loss:0.37251937381318595 val_loss0.3480513594459634\n",
            "iteration 806 :train_loss:0.3723163843767731 val_loss0.3461266752119341\n",
            "iteration 807 :train_loss:0.37245370170176645 val_loss0.3479879166234988\n",
            "iteration 808 :train_loss:0.37223115167709075 val_loss0.3460410985432095\n",
            "iteration 809 :train_loss:0.37235804940702066 val_loss0.34788940889898773\n",
            "iteration 810 :train_loss:0.3721384116230939 val_loss0.3459513479020395\n",
            "iteration 811 :train_loss:0.3722632659608844 val_loss0.3477907040110501\n",
            "iteration 812 :train_loss:0.3720517163800121 val_loss0.3458620246848806\n",
            "iteration 813 :train_loss:0.37217888477372496 val_loss0.34770374946703586\n",
            "iteration 814 :train_loss:0.37196928555077924 val_loss0.34578446434522364\n",
            "iteration 815 :train_loss:0.37211304039954707 val_loss0.34763897377131914\n",
            "iteration 816 :train_loss:0.3719059248299945 val_loss0.34572274876057646\n",
            "iteration 817 :train_loss:0.3720390872739426 val_loss0.3475655795890983\n",
            "iteration 818 :train_loss:0.37185398472308134 val_loss0.3456764418611853\n",
            "iteration 819 :train_loss:0.3720000952037662 val_loss0.3475250867918326\n",
            "iteration 820 :train_loss:0.3718091810454683 val_loss0.34562962944787834\n",
            "iteration 821 :train_loss:0.37195106389422034 val_loss0.34747470333019287\n",
            "iteration 822 :train_loss:0.37176436327943746 val_loss0.34558741341829374\n",
            "iteration 823 :train_loss:0.3719211164630696 val_loss0.3474448258813238\n",
            "iteration 824 :train_loss:0.3717389907497956 val_loss0.34556262934453597\n",
            "iteration 825 :train_loss:0.3718833677998961 val_loss0.34740932734485763\n",
            "iteration 826 :train_loss:0.37169720977476745 val_loss0.3455250665004896\n",
            "iteration 827 :train_loss:0.3718116086042744 val_loss0.3473354910814257\n",
            "iteration 828 :train_loss:0.37164835350896225 val_loss0.3454733236404696\n",
            "iteration 829 :train_loss:0.3718420040401345 val_loss0.3473731952493782\n",
            "iteration 830 :train_loss:0.37165281265071703 val_loss0.3454807440679487\n",
            "iteration 831 :train_loss:0.371764099451401 val_loss0.3472877367512807\n",
            "iteration 832 :train_loss:0.37157722660539966 val_loss0.34540172195218305\n",
            "iteration 833 :train_loss:0.3717333433999776 val_loss0.34725692983459944\n",
            "iteration 834 :train_loss:0.37155949058706467 val_loss0.3453876078163396\n",
            "iteration 835 :train_loss:0.37168842839299876 val_loss0.34720748453340183\n",
            "iteration 836 :train_loss:0.37150992943564054 val_loss0.3453371089704398\n",
            "iteration 837 :train_loss:0.37162965746317383 val_loss0.3471466161262038\n",
            "iteration 838 :train_loss:0.3714534347958137 val_loss0.34527848831005453\n",
            "iteration 839 :train_loss:0.3715816868274305 val_loss0.34709616706804686\n",
            "iteration 840 :train_loss:0.3714068650670917 val_loss0.3452334006592986\n",
            "iteration 841 :train_loss:0.37155946486602914 val_loss0.34707348611683086\n",
            "iteration 842 :train_loss:0.37137803565017447 val_loss0.345203840514084\n",
            "iteration 843 :train_loss:0.3714904985504445 val_loss0.34700445669260205\n",
            "iteration 844 :train_loss:0.371302077455721 val_loss0.34512700677010316\n",
            "iteration 845 :train_loss:0.37146046087140117 val_loss0.3469741381470497\n",
            "iteration 846 :train_loss:0.37127756662193057 val_loss0.3451045107137808\n",
            "iteration 847 :train_loss:0.3713278968365262 val_loss0.3468330401237921\n",
            "iteration 848 :train_loss:0.37116109933614694 val_loss0.3449896475414164\n",
            "iteration 849 :train_loss:0.3713188376567416 val_loss0.34683470127655036\n",
            "iteration 850 :train_loss:0.37114312016509937 val_loss0.3449754324681068\n",
            "iteration 851 :train_loss:0.3712946712591394 val_loss0.3468122972348311\n",
            "iteration 852 :train_loss:0.3711052029000019 val_loss0.3449413411727328\n",
            "iteration 853 :train_loss:0.37123290649877605 val_loss0.3467523635346962\n",
            "iteration 854 :train_loss:0.3710476632504009 val_loss0.34488897520227846\n",
            "iteration 855 :train_loss:0.37119655554414477 val_loss0.3467167046689404\n",
            "iteration 856 :train_loss:0.37102544557047257 val_loss0.3448690553197703\n",
            "iteration 857 :train_loss:0.3711595266768464 val_loss0.3466795374834004\n",
            "iteration 858 :train_loss:0.3709901483712225 val_loss0.34483497362690174\n",
            "iteration 859 :train_loss:0.37116052222388396 val_loss0.3466820466410423\n",
            "iteration 860 :train_loss:0.3709857344338857 val_loss0.34483192901961346\n",
            "iteration 861 :train_loss:0.37112019198049623 val_loss0.3466362570389416\n",
            "iteration 862 :train_loss:0.370938380790515 val_loss0.34478390737724274\n",
            "iteration 863 :train_loss:0.3710573211786151 val_loss0.34656943771516374\n",
            "iteration 864 :train_loss:0.3708689993277765 val_loss0.34471846927048366\n",
            "iteration 865 :train_loss:0.3710142854751364 val_loss0.34652618439078353\n",
            "iteration 866 :train_loss:0.37082403276951703 val_loss0.3446781973841512\n",
            "iteration 867 :train_loss:0.3709604841439924 val_loss0.34647100074099796\n",
            "iteration 868 :train_loss:0.37076876055223246 val_loss0.34462295252797903\n",
            "iteration 869 :train_loss:0.3708785562519856 val_loss0.34638394491411817\n",
            "iteration 870 :train_loss:0.3706998919429912 val_loss0.3445616077459042\n",
            "iteration 871 :train_loss:0.37085429875141085 val_loss0.34636517171982323\n",
            "iteration 872 :train_loss:0.3706684794789087 val_loss0.34453584337780174\n",
            "iteration 873 :train_loss:0.3708687908413204 val_loss0.3463809484928511\n",
            "iteration 874 :train_loss:0.37068885284479813 val_loss0.3445654040619098\n",
            "iteration 875 :train_loss:0.3709390680272225 val_loss0.34646071248084737\n",
            "iteration 876 :train_loss:0.3707524033556712 val_loss0.34463786883550224\n",
            "iteration 877 :train_loss:0.37090659969835593 val_loss0.34642526780962474\n",
            "iteration 878 :train_loss:0.3707216320021519 val_loss0.34461169433890976\n",
            "iteration 879 :train_loss:0.3708794158904592 val_loss0.34640003984801465\n",
            "iteration 880 :train_loss:0.3706933699658221 val_loss0.34458906412154056\n",
            "iteration 881 :train_loss:0.37082978674315037 val_loss0.3463505371010905\n",
            "iteration 882 :train_loss:0.3706548352542769 val_loss0.3445562127026552\n",
            "iteration 883 :train_loss:0.37083923603243 val_loss0.3463692305848469\n",
            "iteration 884 :train_loss:0.3706545792590155 val_loss0.344562211823687\n",
            "iteration 885 :train_loss:0.37087324616401524 val_loss0.34641317125456306\n",
            "iteration 886 :train_loss:0.3706705525895601 val_loss0.3445885237930334\n",
            "iteration 887 :train_loss:0.3707732336298564 val_loss0.3463077117774019\n",
            "iteration 888 :train_loss:0.3705695863318023 val_loss0.34449367317371427\n",
            "iteration 889 :train_loss:0.37071251378278197 val_loss0.346254054547385\n",
            "iteration 890 :train_loss:0.3705122354596979 val_loss0.34444418376696084\n",
            "iteration 891 :train_loss:0.3706318014691682 val_loss0.34617680414561713\n",
            "iteration 892 :train_loss:0.37043921338820823 val_loss0.3443786151506759\n",
            "iteration 893 :train_loss:0.37057896089742676 val_loss0.3461281484446587\n",
            "iteration 894 :train_loss:0.3703879572280101 val_loss0.3443396039391054\n",
            "iteration 895 :train_loss:0.3705300830098785 val_loss0.3460858586911779\n",
            "iteration 896 :train_loss:0.37033127199923205 val_loss0.3442910124609743\n",
            "iteration 897 :train_loss:0.3704322683696813 val_loss0.3459907616658165\n",
            "iteration 898 :train_loss:0.37023893781363754 val_loss0.3442082914793648\n",
            "iteration 899 :train_loss:0.3703754611917008 val_loss0.3459423470776928\n",
            "iteration 900 :train_loss:0.3701905841771183 val_loss0.34416880276125134\n",
            "iteration 901 :train_loss:0.3702919107682473 val_loss0.345864990835667\n",
            "iteration 902 :train_loss:0.37011776929699874 val_loss0.34410802386813116\n",
            "iteration 903 :train_loss:0.3702748751039907 val_loss0.3458581469474161\n",
            "iteration 904 :train_loss:0.37009769363569506 val_loss0.3441023555981002\n",
            "iteration 905 :train_loss:0.37024829014716654 val_loss0.3458406192903771\n",
            "iteration 906 :train_loss:0.37006686967477614 val_loss0.3440815382287005\n",
            "iteration 907 :train_loss:0.3701727470148024 val_loss0.3457709493704771\n",
            "iteration 908 :train_loss:0.36998863241578095 val_loss0.3440110525262682\n",
            "iteration 909 :train_loss:0.3700882407550011 val_loss0.3456915373401448\n",
            "iteration 910 :train_loss:0.3699039212136058 val_loss0.3439373502556886\n",
            "iteration 911 :train_loss:0.3700437949549889 val_loss0.34565641310663725\n",
            "iteration 912 :train_loss:0.3698585833000673 val_loss0.34390316998794274\n",
            "iteration 913 :train_loss:0.3699313948486241 val_loss0.34554861778885443\n",
            "iteration 914 :train_loss:0.36974735398277747 val_loss0.3437998197837357\n",
            "iteration 915 :train_loss:0.36988665385495556 val_loss0.3455173318174023\n",
            "iteration 916 :train_loss:0.369693688723903 val_loss0.34375607708052724\n",
            "iteration 917 :train_loss:0.3698152718401123 val_loss0.345447961434914\n",
            "iteration 918 :train_loss:0.36962336807835816 val_loss0.3436975973448979\n",
            "iteration 919 :train_loss:0.3696785518203868 val_loss0.34531904436246724\n",
            "iteration 920 :train_loss:0.36949620696925367 val_loss0.34357535532610195\n",
            "iteration 921 :train_loss:0.36963897583425326 val_loss0.3452907733137712\n",
            "iteration 922 :train_loss:0.36946050888101645 val_loss0.34355182254628935\n",
            "iteration 923 :train_loss:0.3696068458985308 val_loss0.3452666282257274\n",
            "iteration 924 :train_loss:0.3694227371600728 val_loss0.3435253840368412\n",
            "iteration 925 :train_loss:0.36960568679032196 val_loss0.34528036524753536\n",
            "iteration 926 :train_loss:0.3694289841878849 val_loss0.34353969376741483\n",
            "iteration 927 :train_loss:0.3695155954873956 val_loss0.34519305938675027\n",
            "iteration 928 :train_loss:0.3693655413365888 val_loss0.3434826777822802\n",
            "iteration 929 :train_loss:0.3694983071471885 val_loss0.3451929860095075\n",
            "iteration 930 :train_loss:0.3693462850129017 val_loss0.34347369168348835\n",
            "iteration 931 :train_loss:0.36952890792837295 val_loss0.34524489082161725\n",
            "iteration 932 :train_loss:0.36937695227995093 val_loss0.3435134865017014\n",
            "iteration 933 :train_loss:0.3695396847540558 val_loss0.3452672349376682\n",
            "iteration 934 :train_loss:0.3693887790772454 val_loss0.3435354148043082\n",
            "iteration 935 :train_loss:0.3695084679277301 val_loss0.34524725830976855\n",
            "iteration 936 :train_loss:0.3693561227180929 val_loss0.34351116651362806\n",
            "iteration 937 :train_loss:0.36946078274012384 val_loss0.3452096481960132\n",
            "iteration 938 :train_loss:0.3693087877474155 val_loss0.3434750887881764\n",
            "iteration 939 :train_loss:0.3694639519295821 val_loss0.3452281092899056\n",
            "iteration 940 :train_loss:0.36930299313267734 val_loss0.3434837302490093\n",
            "iteration 941 :train_loss:0.36941761985143784 val_loss0.34518865615522887\n",
            "iteration 942 :train_loss:0.36925947842939233 val_loss0.3434522335871331\n",
            "iteration 943 :train_loss:0.369327396765258 val_loss0.3451098314514888\n",
            "iteration 944 :train_loss:0.3691772304410354 val_loss0.3433796107182245\n",
            "iteration 945 :train_loss:0.369254969672534 val_loss0.34504472048436274\n",
            "iteration 946 :train_loss:0.3691003187531964 val_loss0.3433194650091666\n",
            "iteration 947 :train_loss:0.3692001946207121 val_loss0.3450008636555954\n",
            "iteration 948 :train_loss:0.3690470938242111 val_loss0.3432816271484323\n",
            "iteration 949 :train_loss:0.3691318418938366 val_loss0.34494495914615014\n",
            "iteration 950 :train_loss:0.3689813364508521 val_loss0.34322508503680843\n",
            "iteration 951 :train_loss:0.36904630148148737 val_loss0.3448672916296521\n",
            "iteration 952 :train_loss:0.3689155346472177 val_loss0.34316948029431843\n",
            "iteration 953 :train_loss:0.36898040229766504 val_loss0.34481482487749576\n",
            "iteration 954 :train_loss:0.36885498828865454 val_loss0.3431209961400076\n",
            "iteration 955 :train_loss:0.3689257641328869 val_loss0.3447723970247916\n",
            "iteration 956 :train_loss:0.36880313906379963 val_loss0.34308422631578883\n",
            "iteration 957 :train_loss:0.36892533956595913 val_loss0.3447884347323304\n",
            "iteration 958 :train_loss:0.36880382198134737 val_loss0.343099284335927\n",
            "iteration 959 :train_loss:0.3689091066391142 val_loss0.34478837011014596\n",
            "iteration 960 :train_loss:0.36878314367522386 val_loss0.34308788478476115\n",
            "iteration 961 :train_loss:0.3688857976093001 val_loss0.34478144344294676\n",
            "iteration 962 :train_loss:0.3687578444496465 val_loss0.34306951330877444\n",
            "iteration 963 :train_loss:0.36881323466607546 val_loss0.3447207747726076\n",
            "iteration 964 :train_loss:0.36869128068652557 val_loss0.34301148461853465\n",
            "iteration 965 :train_loss:0.36880279929819043 val_loss0.3447251626828797\n",
            "iteration 966 :train_loss:0.36867718187268367 val_loss0.3430104794748669\n",
            "iteration 967 :train_loss:0.3687446461111223 val_loss0.344678446022468\n",
            "iteration 968 :train_loss:0.3686150583873829 val_loss0.34295864416858385\n",
            "iteration 969 :train_loss:0.3687382630571525 val_loss0.34468872263116385\n",
            "iteration 970 :train_loss:0.3686197164639041 val_loss0.34297196012471254\n",
            "iteration 971 :train_loss:0.36869036889146556 val_loss0.344653543848394\n",
            "iteration 972 :train_loss:0.3685715125660083 val_loss0.3429269826679156\n",
            "iteration 973 :train_loss:0.36863454707877097 val_loss0.3446052609196665\n",
            "iteration 974 :train_loss:0.3685141623958779 val_loss0.3428743995351233\n",
            "iteration 975 :train_loss:0.36863099217988876 val_loss0.3446145435940688\n",
            "iteration 976 :train_loss:0.36850431108658727 val_loss0.34287057590219583\n",
            "iteration 977 :train_loss:0.36857227460653813 val_loss0.3445612898228848\n",
            "iteration 978 :train_loss:0.3684503684273484 val_loss0.3428235633025469\n",
            "iteration 979 :train_loss:0.36851870120892805 val_loss0.3445169562838686\n",
            "iteration 980 :train_loss:0.36840355088087734 val_loss0.3427809215399492\n",
            "iteration 981 :train_loss:0.36843029272970046 val_loss0.3444320168465288\n",
            "iteration 982 :train_loss:0.3683142800328212 val_loss0.34269617708369793\n",
            "iteration 983 :train_loss:0.3684013154368244 val_loss0.34441225324404\n",
            "iteration 984 :train_loss:0.36828048007695274 val_loss0.34266936256280783\n",
            "iteration 985 :train_loss:0.3683626157969603 val_loss0.34438040489208105\n",
            "iteration 986 :train_loss:0.36823858987997976 val_loss0.34263234812574206\n",
            "iteration 987 :train_loss:0.36829873589589485 val_loss0.34432047648013686\n",
            "iteration 988 :train_loss:0.3681794661197014 val_loss0.34257954780896444\n",
            "iteration 989 :train_loss:0.36824780523096456 val_loss0.344274605058953\n",
            "iteration 990 :train_loss:0.3681366229479201 val_loss0.3425421469892258\n",
            "iteration 991 :train_loss:0.36817518248856845 val_loss0.3442064534171777\n",
            "iteration 992 :train_loss:0.36806800488397556 val_loss0.3424775523266822\n",
            "iteration 993 :train_loss:0.3680945676352382 val_loss0.34412640396275723\n",
            "iteration 994 :train_loss:0.36799463125125575 val_loss0.3424104520824578\n",
            "iteration 995 :train_loss:0.36805184632451726 val_loss0.34408875252308047\n",
            "iteration 996 :train_loss:0.3679563836904419 val_loss0.342376162041687\n",
            "iteration 997 :train_loss:0.3680221102893145 val_loss0.3440626704465132\n",
            "iteration 998 :train_loss:0.36790869322539504 val_loss0.34233287577790095\n",
            "iteration 999 :train_loss:0.3679457562038442 val_loss0.3439882667735966\n",
            "iteration 1000 :train_loss:0.36785686665723544 val_loss0.3422806929312997\n",
            "iteration 1001 :train_loss:0.36789646738663756 val_loss0.3439406073826063\n",
            "iteration 1002 :train_loss:0.36780519120700267 val_loss0.34223619906706\n",
            "iteration 1003 :train_loss:0.3678575772429548 val_loss0.3439080229176222\n",
            "iteration 1004 :train_loss:0.3677755841244729 val_loss0.3422098999568466\n",
            "iteration 1005 :train_loss:0.3678357872811309 val_loss0.3438909598629573\n",
            "iteration 1006 :train_loss:0.36775509046266214 val_loss0.3421952839869918\n",
            "iteration 1007 :train_loss:0.367826400121714 val_loss0.34388685480957437\n",
            "iteration 1008 :train_loss:0.3677377063501549 val_loss0.34218212057458347\n",
            "iteration 1009 :train_loss:0.3677650308633597 val_loss0.3438295270226851\n",
            "iteration 1010 :train_loss:0.3676700018488823 val_loss0.34211627624606933\n",
            "iteration 1011 :train_loss:0.3677224790146073 val_loss0.34378952315443995\n",
            "iteration 1012 :train_loss:0.36763530371175723 val_loss0.34208720427819633\n",
            "iteration 1013 :train_loss:0.3677245733440316 val_loss0.34379846024690436\n",
            "iteration 1014 :train_loss:0.36762797809711295 val_loss0.34208444877306626\n",
            "iteration 1015 :train_loss:0.36771109886382114 val_loss0.34378920350969444\n",
            "iteration 1016 :train_loss:0.3676077497643473 val_loss0.3420696714431131\n",
            "iteration 1017 :train_loss:0.36764370897455123 val_loss0.3437222874634732\n",
            "iteration 1018 :train_loss:0.3675347017915267 val_loss0.34200254490414467\n",
            "iteration 1019 :train_loss:0.36756150032761076 val_loss0.34364480317469315\n",
            "iteration 1020 :train_loss:0.3674646585115327 val_loss0.34193191299906667\n",
            "iteration 1021 :train_loss:0.3675457788965446 val_loss0.34363483608450074\n",
            "iteration 1022 :train_loss:0.36744878722335533 val_loss0.34192236869963233\n",
            "iteration 1023 :train_loss:0.3675416557540322 val_loss0.3436382820276855\n",
            "iteration 1024 :train_loss:0.3674358833367673 val_loss0.341914588543708\n",
            "iteration 1025 :train_loss:0.3675271185772789 val_loss0.3436313446130465\n",
            "iteration 1026 :train_loss:0.3674270027628163 val_loss0.34190749869093756\n",
            "iteration 1027 :train_loss:0.36748848213510316 val_loss0.3435970375810341\n",
            "iteration 1028 :train_loss:0.367376597129781 val_loss0.3418642880614012\n",
            "iteration 1029 :train_loss:0.367416514660247 val_loss0.34352780744161326\n",
            "iteration 1030 :train_loss:0.3673264251955712 val_loss0.3418167604514648\n",
            "iteration 1031 :train_loss:0.36737286886857656 val_loss0.3434952893775578\n",
            "iteration 1032 :train_loss:0.36729321982652285 val_loss0.3417885314153853\n",
            "iteration 1033 :train_loss:0.3673670885897275 val_loss0.3435017106836966\n",
            "iteration 1034 :train_loss:0.3672837541672738 val_loss0.34178490520509347\n",
            "iteration 1035 :train_loss:0.3673468940432016 val_loss0.34348832421203485\n",
            "iteration 1036 :train_loss:0.3672644750476668 val_loss0.3417745504228883\n",
            "iteration 1037 :train_loss:0.36734184698153843 val_loss0.3434938504973111\n",
            "iteration 1038 :train_loss:0.3672633332223465 val_loss0.34177849781217884\n",
            "iteration 1039 :train_loss:0.3672915039597536 val_loss0.34344959401792186\n",
            "iteration 1040 :train_loss:0.36721073053831715 val_loss0.34173249643916304\n",
            "iteration 1041 :train_loss:0.36723320248817926 val_loss0.3433977347867348\n",
            "iteration 1042 :train_loss:0.36715493483235456 val_loss0.34168400130067134\n",
            "iteration 1043 :train_loss:0.36713393907054115 val_loss0.34330341463237546\n",
            "iteration 1044 :train_loss:0.36705700438157274 val_loss0.34159007157376087\n",
            "iteration 1045 :train_loss:0.3670501928308604 val_loss0.34322785695752855\n",
            "iteration 1046 :train_loss:0.366961725471648 val_loss0.3414991600413271\n",
            "iteration 1047 :train_loss:0.36695871832589344 val_loss0.34314043796909705\n",
            "iteration 1048 :train_loss:0.36687262135845816 val_loss0.34141673832073255\n",
            "iteration 1049 :train_loss:0.36691102045545326 val_loss0.34310063369347216\n",
            "iteration 1050 :train_loss:0.36681785094051816 val_loss0.34136892758922877\n",
            "iteration 1051 :train_loss:0.3668290915773237 val_loss0.34302639964372406\n",
            "iteration 1052 :train_loss:0.3667476324389247 val_loss0.34130482592377004\n",
            "iteration 1053 :train_loss:0.36675222949948794 val_loss0.34295774392678846\n",
            "iteration 1054 :train_loss:0.3666791849247996 val_loss0.3412421704545348\n",
            "iteration 1055 :train_loss:0.366637224708187 val_loss0.3428453931023056\n",
            "iteration 1056 :train_loss:0.3665702407281186 val_loss0.3411403810796684\n",
            "iteration 1057 :train_loss:0.3665828476348882 val_loss0.34280139187957137\n",
            "iteration 1058 :train_loss:0.3665160179463933 val_loss0.34109492752291537\n",
            "iteration 1059 :train_loss:0.366522615088178 val_loss0.34274832904918046\n",
            "iteration 1060 :train_loss:0.36646827342849786 val_loss0.3410545379729766\n",
            "iteration 1061 :train_loss:0.3665188613060511 val_loss0.34275537051773747\n",
            "iteration 1062 :train_loss:0.3664587579014093 val_loss0.3410535426668711\n",
            "iteration 1063 :train_loss:0.3664856707380075 val_loss0.34273002078806286\n",
            "iteration 1064 :train_loss:0.3664220400024559 val_loss0.3410251081407944\n",
            "iteration 1065 :train_loss:0.36646056676495276 val_loss0.3427155323507929\n",
            "iteration 1066 :train_loss:0.36640222449676796 val_loss0.34101269921842464\n",
            "iteration 1067 :train_loss:0.3663882244292998 val_loss0.3426455332497121\n",
            "iteration 1068 :train_loss:0.3663245447489687 val_loss0.34093963039872877\n",
            "iteration 1069 :train_loss:0.3663142413800218 val_loss0.34257860473045826\n",
            "iteration 1070 :train_loss:0.36626577396634574 val_loss0.34088775937322463\n",
            "iteration 1071 :train_loss:0.3662737433484218 val_loss0.34255004247586607\n",
            "iteration 1072 :train_loss:0.36621417270319895 val_loss0.34084107374469225\n",
            "iteration 1073 :train_loss:0.3662516496761831 val_loss0.3425368349903338\n",
            "iteration 1074 :train_loss:0.3662042217635659 val_loss0.340839187106709\n",
            "iteration 1075 :train_loss:0.3662590680582021 val_loss0.34255721808606066\n",
            "iteration 1076 :train_loss:0.36620049976758934 val_loss0.34084293168642094\n",
            "iteration 1077 :train_loss:0.36622697992192677 val_loss0.34253401961487584\n",
            "iteration 1078 :train_loss:0.3661672274239961 val_loss0.34081589317717625\n",
            "iteration 1079 :train_loss:0.366203594086044 val_loss0.34251751274842723\n",
            "iteration 1080 :train_loss:0.3661373338410997 val_loss0.34079009760288176\n",
            "iteration 1081 :train_loss:0.366127130528901 val_loss0.342444346565485\n",
            "iteration 1082 :train_loss:0.3660553649667007 val_loss0.34071307740039775\n",
            "iteration 1083 :train_loss:0.3660756474756772 val_loss0.34239916792248065\n",
            "iteration 1084 :train_loss:0.36600764095138427 val_loss0.3406728735570181\n",
            "iteration 1085 :train_loss:0.3660071952748753 val_loss0.34233503652902897\n",
            "iteration 1086 :train_loss:0.3659207230437532 val_loss0.3405957188075547\n",
            "iteration 1087 :train_loss:0.3658911885673381 val_loss0.3422200545250271\n",
            "iteration 1088 :train_loss:0.36580981533632556 val_loss0.3404939349533877\n",
            "iteration 1089 :train_loss:0.36582761024530414 val_loss0.3421605084069517\n",
            "iteration 1090 :train_loss:0.3657445113920888 val_loss0.34043289438490093\n",
            "iteration 1091 :train_loss:0.36575399236350953 val_loss0.3420912467253183\n",
            "iteration 1092 :train_loss:0.3656757984011727 val_loss0.34037189006389457\n",
            "iteration 1093 :train_loss:0.3656952903632246 val_loss0.342040193313561\n",
            "iteration 1094 :train_loss:0.36560583696567467 val_loss0.340309123395225\n",
            "iteration 1095 :train_loss:0.3656161609552247 val_loss0.3419657299119785\n",
            "iteration 1096 :train_loss:0.36552915912849615 val_loss0.34023661807941563\n",
            "iteration 1097 :train_loss:0.36556108078048977 val_loss0.3419200896688254\n",
            "iteration 1098 :train_loss:0.36547550851860316 val_loss0.3401901331928874\n",
            "iteration 1099 :train_loss:0.36551307008853845 val_loss0.3418790586617127\n",
            "iteration 1100 :train_loss:0.365426227314756 val_loss0.3401461984689316\n",
            "iteration 1101 :train_loss:0.36543954608150614 val_loss0.34181156497008597\n",
            "iteration 1102 :train_loss:0.36536434594822337 val_loss0.3400883984357792\n",
            "iteration 1103 :train_loss:0.36534938110746756 val_loss0.3417283529775452\n",
            "iteration 1104 :train_loss:0.3652609159166356 val_loss0.3399892338391193\n",
            "iteration 1105 :train_loss:0.3653042463437012 val_loss0.341690572768216\n",
            "iteration 1106 :train_loss:0.3652272920323461 val_loss0.33996230852275333\n",
            "iteration 1107 :train_loss:0.3652364175011863 val_loss0.34162783493349463\n",
            "iteration 1108 :train_loss:0.36516021294197953 val_loss0.3399016713613012\n",
            "iteration 1109 :train_loss:0.3652145042760341 val_loss0.34161567276267785\n",
            "iteration 1110 :train_loss:0.3651482974195196 val_loss0.3398933558004272\n",
            "iteration 1111 :train_loss:0.3652023212178519 val_loss0.3416133581141215\n",
            "iteration 1112 :train_loss:0.36511850195466466 val_loss0.3398695177389342\n",
            "iteration 1113 :train_loss:0.36519448034534513 val_loss0.34161859459113614\n",
            "iteration 1114 :train_loss:0.36511743263037144 val_loss0.33987354391043306\n",
            "iteration 1115 :train_loss:0.36517856264816156 val_loss0.34161201887013165\n",
            "iteration 1116 :train_loss:0.365099213215871 val_loss0.3398595939288777\n",
            "iteration 1117 :train_loss:0.36516224101452116 val_loss0.3416089055180409\n",
            "iteration 1118 :train_loss:0.36509901410996176 val_loss0.33986236780728296\n",
            "iteration 1119 :train_loss:0.3651778657390987 val_loss0.3416342121626183\n",
            "iteration 1120 :train_loss:0.36512108906492774 val_loss0.3398881773770968\n",
            "iteration 1121 :train_loss:0.3652050997222794 val_loss0.34167670783077014\n",
            "iteration 1122 :train_loss:0.36513546479330966 val_loss0.33990941995011564\n",
            "iteration 1123 :train_loss:0.3651672416417279 val_loss0.3416421247535244\n",
            "iteration 1124 :train_loss:0.3650945391868905 val_loss0.33987264335198564\n",
            "iteration 1125 :train_loss:0.3651592378058288 val_loss0.341647269894594\n",
            "iteration 1126 :train_loss:0.3650929972478192 val_loss0.3398784018738423\n",
            "iteration 1127 :train_loss:0.3651588303203772 val_loss0.3416571252344248\n",
            "iteration 1128 :train_loss:0.3650779347956397 val_loss0.3398698497979051\n",
            "iteration 1129 :train_loss:0.3651086067371736 val_loss0.3416115234239071\n",
            "iteration 1130 :train_loss:0.36503661369439305 val_loss0.3398361310158876\n",
            "iteration 1131 :train_loss:0.3651122874647646 val_loss0.3416308435494368\n",
            "iteration 1132 :train_loss:0.3650431758919231 val_loss0.3398512012032105\n",
            "iteration 1133 :train_loss:0.3650878403228048 val_loss0.34161573274143164\n",
            "iteration 1134 :train_loss:0.3650309606510102 val_loss0.33984659442725285\n",
            "iteration 1135 :train_loss:0.3650850283953765 val_loss0.3416252145008518\n",
            "iteration 1136 :train_loss:0.3650176621762823 val_loss0.33984343799382705\n",
            "iteration 1137 :train_loss:0.3650593837921007 val_loss0.34160928246629496\n",
            "iteration 1138 :train_loss:0.364984962453524 val_loss0.33982228122028635\n",
            "iteration 1139 :train_loss:0.3650075246936984 val_loss0.3415680227067669\n",
            "iteration 1140 :train_loss:0.36495455743458477 val_loss0.33979859682966806\n",
            "iteration 1141 :train_loss:0.36498955647729686 val_loss0.3415607872157828\n",
            "iteration 1142 :train_loss:0.36492497878435715 val_loss0.339780267498575\n",
            "iteration 1143 :train_loss:0.3649346328642572 val_loss0.34151426564488707\n",
            "iteration 1144 :train_loss:0.3648685018434818 val_loss0.33973197279096196\n",
            "iteration 1145 :train_loss:0.3648752419877755 val_loss0.34146363017066256\n",
            "iteration 1146 :train_loss:0.3648042263649563 val_loss0.3396788480529529\n",
            "iteration 1147 :train_loss:0.3647999991277873 val_loss0.3413977618422601\n",
            "iteration 1148 :train_loss:0.3647296611683271 val_loss0.33961231830555344\n",
            "iteration 1149 :train_loss:0.3647244032433079 val_loss0.3413301393607468\n",
            "iteration 1150 :train_loss:0.3646415555840174 val_loss0.3395345819007266\n",
            "iteration 1151 :train_loss:0.3646170527698194 val_loss0.3412267551316698\n",
            "iteration 1152 :train_loss:0.36453004654927507 val_loss0.33943091860020624\n",
            "iteration 1153 :train_loss:0.36451884404163903 val_loss0.341133270146891\n",
            "iteration 1154 :train_loss:0.3644261539750412 val_loss0.3393364081734169\n",
            "iteration 1155 :train_loss:0.3643863230979472 val_loss0.3409996063414218\n",
            "iteration 1156 :train_loss:0.36429869019139993 val_loss0.3392203833064429\n",
            "iteration 1157 :train_loss:0.3642167777410616 val_loss0.3408273195729322\n",
            "iteration 1158 :train_loss:0.36412755335837743 val_loss0.3390599956115813\n",
            "iteration 1159 :train_loss:0.36405707497420103 val_loss0.3406655723843195\n",
            "iteration 1160 :train_loss:0.3639682368858825 val_loss0.3389108155113897\n",
            "iteration 1161 :train_loss:0.36394974146101794 val_loss0.3405663988647879\n",
            "iteration 1162 :train_loss:0.3638641629666601 val_loss0.3388179422127097\n",
            "iteration 1163 :train_loss:0.3638366471654682 val_loss0.340458572641051\n",
            "iteration 1164 :train_loss:0.36376121277661727 val_loss0.33872895693253513\n",
            "iteration 1165 :train_loss:0.3637387579850608 val_loss0.3403683650686607\n",
            "iteration 1166 :train_loss:0.36366929939741105 val_loss0.33865073636628373\n",
            "iteration 1167 :train_loss:0.363655051259442 val_loss0.34029545779136017\n",
            "iteration 1168 :train_loss:0.36359241679588605 val_loss0.3385856904395431\n",
            "iteration 1169 :train_loss:0.3635875556949286 val_loss0.3402374743737959\n",
            "iteration 1170 :train_loss:0.3635204217070648 val_loss0.33852816077183884\n",
            "iteration 1171 :train_loss:0.3635074111941215 val_loss0.34016421616166437\n",
            "iteration 1172 :train_loss:0.36345207676115737 val_loss0.3384746504950081\n",
            "iteration 1173 :train_loss:0.3634383962919599 val_loss0.34010360073380075\n",
            "iteration 1174 :train_loss:0.36335485170919457 val_loss0.33838925189930646\n",
            "iteration 1175 :train_loss:0.3633486298804069 val_loss0.34002427086327947\n",
            "iteration 1176 :train_loss:0.3632953063585203 val_loss0.33834484240244134\n",
            "iteration 1177 :train_loss:0.36329199420965885 val_loss0.33998015587095964\n",
            "iteration 1178 :train_loss:0.36323982995355636 val_loss0.3383015791902732\n",
            "iteration 1179 :train_loss:0.36322815911616935 val_loss0.33992596441310113\n",
            "iteration 1180 :train_loss:0.3631854676721752 val_loss0.3382599896260748\n",
            "iteration 1181 :train_loss:0.36316444201683384 val_loss0.3398709724759514\n",
            "iteration 1182 :train_loss:0.3631214790371014 val_loss0.33821106939695555\n",
            "iteration 1183 :train_loss:0.36311662251824667 val_loss0.33983770206154673\n",
            "iteration 1184 :train_loss:0.36308528647405897 val_loss0.3381896920794009\n",
            "iteration 1185 :train_loss:0.36309411744100234 val_loss0.3398277294077233\n",
            "iteration 1186 :train_loss:0.3630647226847757 val_loss0.3381862256367454\n",
            "iteration 1187 :train_loss:0.36305030456753407 val_loss0.339799165967348\n",
            "iteration 1188 :train_loss:0.36302388897541366 val_loss0.3381629183876016\n",
            "iteration 1189 :train_loss:0.3630476699949078 val_loss0.33981604348924027\n",
            "iteration 1190 :train_loss:0.36302529311183934 val_loss0.3381846683888832\n",
            "iteration 1191 :train_loss:0.36301478150430855 val_loss0.33979684737934124\n",
            "iteration 1192 :train_loss:0.36299559805697024 val_loss0.3381718711280338\n",
            "iteration 1193 :train_loss:0.3630158540148851 val_loss0.3398140041868109\n",
            "iteration 1194 :train_loss:0.3629983217359733 val_loss0.33819011110342323\n",
            "iteration 1195 :train_loss:0.3630254562537513 val_loss0.3398403702372107\n",
            "iteration 1196 :train_loss:0.36301556296302656 val_loss0.33822223614900143\n",
            "iteration 1197 :train_loss:0.36304252697633405 val_loss0.3398739051752901\n",
            "iteration 1198 :train_loss:0.3630372353665963 val_loss0.3382583153247186\n",
            "iteration 1199 :train_loss:0.36307400292685355 val_loss0.33992579558842384\n",
            "iteration 1200 :train_loss:0.36305446980411765 val_loss0.3382926441952793\n",
            "iteration 1201 :train_loss:0.3630637295631012 val_loss0.33993165249879076\n",
            "iteration 1202 :train_loss:0.36303869378948506 val_loss0.33829111091848807\n",
            "iteration 1203 :train_loss:0.36304818646578063 val_loss0.3399325176306839\n",
            "iteration 1204 :train_loss:0.3630121139356769 val_loss0.33828192237068394\n",
            "iteration 1205 :train_loss:0.36303356556498534 val_loss0.3399368381267039\n",
            "iteration 1206 :train_loss:0.3629856956538594 val_loss0.3382723740760227\n",
            "iteration 1207 :train_loss:0.3629882332871506 val_loss0.33990485143864557\n",
            "iteration 1208 :train_loss:0.3629330400215391 val_loss0.3382342277883087\n",
            "iteration 1209 :train_loss:0.3629609424639321 val_loss0.33989510255305233\n",
            "iteration 1210 :train_loss:0.3629006915404023 val_loss0.33822006461853626\n",
            "iteration 1211 :train_loss:0.362901404797421 val_loss0.3398518019964213\n",
            "iteration 1212 :train_loss:0.3628381245460249 val_loss0.3381753492082672\n",
            "iteration 1213 :train_loss:0.3628457546482283 val_loss0.33981521841490353\n",
            "iteration 1214 :train_loss:0.3627681781011161 val_loss0.33812413166987854\n",
            "iteration 1215 :train_loss:0.3627682906884003 val_loss0.33975391256057413\n",
            "iteration 1216 :train_loss:0.3626838391375733 val_loss0.33805694399438285\n",
            "iteration 1217 :train_loss:0.3626772058090359 val_loss0.33967790727105035\n",
            "iteration 1218 :train_loss:0.36258965824427924 val_loss0.33798147337901996\n",
            "iteration 1219 :train_loss:0.36257559108332893 val_loss0.3395893015841143\n",
            "iteration 1220 :train_loss:0.36248206989918436 val_loss0.33789290472558947\n",
            "iteration 1221 :train_loss:0.3624739212039893 val_loss0.33950514612602223\n",
            "iteration 1222 :train_loss:0.362388030521648 val_loss0.33781800422031066\n",
            "iteration 1223 :train_loss:0.36237074377123085 val_loss0.33941648195688645\n",
            "iteration 1224 :train_loss:0.3622919588318291 val_loss0.3377376994273155\n",
            "iteration 1225 :train_loss:0.3622714368745238 val_loss0.33933033063509044\n",
            "iteration 1226 :train_loss:0.3621861788429365 val_loss0.3376492445953978\n",
            "iteration 1227 :train_loss:0.36217616907516276 val_loss0.33925146036775966\n",
            "iteration 1228 :train_loss:0.36210174412252805 val_loss0.3375788858386944\n",
            "iteration 1229 :train_loss:0.36209902752422635 val_loss0.3391898759330253\n",
            "iteration 1230 :train_loss:0.36203991593345225 val_loss0.33753415038494344\n",
            "iteration 1231 :train_loss:0.3620166430341799 val_loss0.33912147377492186\n",
            "iteration 1232 :train_loss:0.3619616288783622 val_loss0.3374710833178423\n",
            "iteration 1233 :train_loss:0.3619493137240627 val_loss0.3390688095234208\n",
            "iteration 1234 :train_loss:0.3619032312131239 val_loss0.33742888397084897\n",
            "iteration 1235 :train_loss:0.36190718993749615 val_loss0.33904336748913705\n",
            "iteration 1236 :train_loss:0.36187012526071494 val_loss0.33741230429270014\n",
            "iteration 1237 :train_loss:0.3618700945001657 val_loss0.3390198096796084\n",
            "iteration 1238 :train_loss:0.36182855963730415 val_loss0.3373843561586511\n",
            "iteration 1239 :train_loss:0.36184579347693857 val_loss0.33901012957385523\n",
            "iteration 1240 :train_loss:0.3618104894190953 val_loss0.33738403365713443\n",
            "iteration 1241 :train_loss:0.36182361250358785 val_loss0.33900295351328796\n",
            "iteration 1242 :train_loss:0.3617854451320407 val_loss0.3373763177652719\n",
            "iteration 1243 :train_loss:0.3618220515675523 val_loss0.33902053571075313\n",
            "iteration 1244 :train_loss:0.36180067034207575 val_loss0.3374103143643666\n",
            "iteration 1245 :train_loss:0.36183291261072326 val_loss0.3390512976286724\n",
            "iteration 1246 :train_loss:0.36180480722760966 val_loss0.3374336429753722\n",
            "iteration 1247 :train_loss:0.3618204572913826 val_loss0.33905266232869624\n",
            "iteration 1248 :train_loss:0.36180129092918334 val_loss0.3374468125264913\n",
            "iteration 1249 :train_loss:0.36182189462939507 val_loss0.3390705467301403\n",
            "iteration 1250 :train_loss:0.3618043379774377 val_loss0.3374660701884469\n",
            "iteration 1251 :train_loss:0.36181858924712973 val_loss0.3390787417359716\n",
            "iteration 1252 :train_loss:0.3617992806449058 val_loss0.33747635184048286\n",
            "iteration 1253 :train_loss:0.36179192346917954 val_loss0.3390641139349842\n",
            "iteration 1254 :train_loss:0.36178262823493607 val_loss0.33747387891043223\n",
            "iteration 1255 :train_loss:0.361801576819052 val_loss0.33908767440329807\n",
            "iteration 1256 :train_loss:0.36180301484854854 val_loss0.33750767323217745\n",
            "iteration 1257 :train_loss:0.3618204558155917 val_loss0.33912103536727156\n",
            "iteration 1258 :train_loss:0.3618211092877226 val_loss0.33754035634625146\n",
            "iteration 1259 :train_loss:0.3618437963160655 val_loss0.3391602201829583\n",
            "iteration 1260 :train_loss:0.3618403796905539 val_loss0.33757265296170275\n",
            "iteration 1261 :train_loss:0.3618431339504005 val_loss0.3391736684540243\n",
            "iteration 1262 :train_loss:0.36184407335054214 val_loss0.3375890083995564\n",
            "iteration 1263 :train_loss:0.36186174424799805 val_loss0.3392075280831549\n",
            "iteration 1264 :train_loss:0.3618617877429546 val_loss0.33761759275956377\n",
            "iteration 1265 :train_loss:0.361877426602614 val_loss0.33923780558529587\n",
            "iteration 1266 :train_loss:0.3618708856006964 val_loss0.33763727206358346\n",
            "iteration 1267 :train_loss:0.3618808369022243 val_loss0.3392528041748702\n",
            "iteration 1268 :train_loss:0.36187870648568504 val_loss0.3376562744943351\n",
            "iteration 1269 :train_loss:0.3618913310003594 val_loss0.33927657524912613\n",
            "iteration 1270 :train_loss:0.36190181997877646 val_loss0.33768946811710365\n",
            "iteration 1271 :train_loss:0.36189949455410153 val_loss0.3392971062163547\n",
            "iteration 1272 :train_loss:0.3619089949902079 val_loss0.337706412610104\n",
            "iteration 1273 :train_loss:0.36193292259395765 val_loss0.3393466407686763\n",
            "iteration 1274 :train_loss:0.36193863916776225 val_loss0.3377469898531955\n",
            "iteration 1275 :train_loss:0.36193537142128906 val_loss0.33935767417881224\n",
            "iteration 1276 :train_loss:0.36193733968678177 val_loss0.3377556960890571\n",
            "iteration 1277 :train_loss:0.3619499896241724 val_loss0.33938589259751895\n",
            "iteration 1278 :train_loss:0.36194402553178295 val_loss0.3377733112590892\n",
            "iteration 1279 :train_loss:0.3619328763624797 val_loss0.33937740173618286\n",
            "iteration 1280 :train_loss:0.3619183488082822 val_loss0.3377603105252288\n",
            "iteration 1281 :train_loss:0.3618841338439681 val_loss0.3393321902338492\n",
            "iteration 1282 :train_loss:0.36185163119331687 val_loss0.3377048055814784\n",
            "iteration 1283 :train_loss:0.36181204789318383 val_loss0.3392665320175387\n",
            "iteration 1284 :train_loss:0.3617823500031108 val_loss0.3376447914257386\n",
            "iteration 1285 :train_loss:0.36175000236256727 val_loss0.33921208285738214\n",
            "iteration 1286 :train_loss:0.3617239731560887 val_loss0.3375967145642535\n",
            "iteration 1287 :train_loss:0.36165486836254046 val_loss0.33912228813312323\n",
            "iteration 1288 :train_loss:0.3616381333195704 val_loss0.3375176331027762\n",
            "iteration 1289 :train_loss:0.36156613428136924 val_loss0.3390412135208465\n",
            "iteration 1290 :train_loss:0.3615394179931774 val_loss0.3374280529715511\n",
            "iteration 1291 :train_loss:0.3614606711993383 val_loss0.3389442469775329\n",
            "iteration 1292 :train_loss:0.3614111543880717 val_loss0.3373086078876473\n",
            "iteration 1293 :train_loss:0.36131986550656187 val_loss0.3388079470759077\n",
            "iteration 1294 :train_loss:0.36126670149310813 val_loss0.33717269300396\n",
            "iteration 1295 :train_loss:0.3611804723818277 val_loss0.3386718317009417\n",
            "iteration 1296 :train_loss:0.36112713732610785 val_loss0.33703995683213\n",
            "iteration 1297 :train_loss:0.36106862513299565 val_loss0.338566894987373\n",
            "iteration 1298 :train_loss:0.3610128860219355 val_loss0.33693415704571095\n",
            "iteration 1299 :train_loss:0.3609560640239367 val_loss0.3384599450284215\n",
            "iteration 1300 :train_loss:0.360918656820656 val_loss0.3368504224125599\n",
            "iteration 1301 :train_loss:0.360867540229155 val_loss0.33837883710495054\n",
            "iteration 1302 :train_loss:0.3608292155956088 val_loss0.33677187651914514\n",
            "iteration 1303 :train_loss:0.36079170822331463 val_loss0.33831115782702714\n",
            "iteration 1304 :train_loss:0.3607658132927157 val_loss0.33671615410247285\n",
            "iteration 1305 :train_loss:0.36073404808771076 val_loss0.338260730014266\n",
            "iteration 1306 :train_loss:0.36071255695599225 val_loss0.3366708344710904\n",
            "iteration 1307 :train_loss:0.360670305375737 val_loss0.33820429118392076\n",
            "iteration 1308 :train_loss:0.3606528100948048 val_loss0.33662083142741683\n",
            "iteration 1309 :train_loss:0.3606091057711268 val_loss0.33815224418317696\n",
            "iteration 1310 :train_loss:0.36059496273055625 val_loss0.3365724680579478\n",
            "iteration 1311 :train_loss:0.36056232007301264 val_loss0.33811253810686953\n",
            "iteration 1312 :train_loss:0.36053769074268227 val_loss0.3365247048798034\n",
            "iteration 1313 :train_loss:0.3604992883007764 val_loss0.33805689711109005\n",
            "iteration 1314 :train_loss:0.36049501942949647 val_loss0.336491237469505\n",
            "iteration 1315 :train_loss:0.36045915841698145 val_loss0.3380234051817976\n",
            "iteration 1316 :train_loss:0.36045815347788485 val_loss0.33646436750940745\n",
            "iteration 1317 :train_loss:0.36041142727168124 val_loss0.337981582489299\n",
            "iteration 1318 :train_loss:0.36041229929100543 val_loss0.33642609478318836\n",
            "iteration 1319 :train_loss:0.36037417348428963 val_loss0.3379505427541792\n",
            "iteration 1320 :train_loss:0.3603790074362354 val_loss0.33640146050735775\n",
            "iteration 1321 :train_loss:0.36035765567098554 val_loss0.33794369903152444\n",
            "iteration 1322 :train_loss:0.36036330180437187 val_loss0.3363963076346462\n",
            "iteration 1323 :train_loss:0.36032875627784244 val_loss0.3379236499735882\n",
            "iteration 1324 :train_loss:0.360329077780007 val_loss0.33637022187954657\n",
            "iteration 1325 :train_loss:0.3602996333367328 val_loss0.3379033669813876\n",
            "iteration 1326 :train_loss:0.36028595281976883 val_loss0.3363355949110862\n",
            "iteration 1327 :train_loss:0.36026598459888964 val_loss0.3378808598072898\n",
            "iteration 1328 :train_loss:0.36025122761532696 val_loss0.33631211451393395\n",
            "iteration 1329 :train_loss:0.3602302663509083 val_loss0.3378576294064592\n",
            "iteration 1330 :train_loss:0.3602230942735887 val_loss0.3362924810173937\n",
            "iteration 1331 :train_loss:0.3601946234055914 val_loss0.3378333017639063\n",
            "iteration 1332 :train_loss:0.36019177516295026 val_loss0.3362714400839148\n",
            "iteration 1333 :train_loss:0.3601722942071128 val_loss0.33782473569466825\n",
            "iteration 1334 :train_loss:0.36016186120679194 val_loss0.33625116364340185\n",
            "iteration 1335 :train_loss:0.36014478538271155 val_loss0.3378091487320632\n",
            "iteration 1336 :train_loss:0.360124558080632 val_loss0.33622344560982514\n",
            "iteration 1337 :train_loss:0.36008334762299987 val_loss0.33775784898083583\n",
            "iteration 1338 :train_loss:0.3600744252504947 val_loss0.3361827460792252\n",
            "iteration 1339 :train_loss:0.3600531440440184 val_loss0.33773886550615445\n",
            "iteration 1340 :train_loss:0.36003470992482184 val_loss0.33615179594136485\n",
            "iteration 1341 :train_loss:0.3600094037015634 val_loss0.3377070696059775\n",
            "iteration 1342 :train_loss:0.36000498827787347 val_loss0.3361302210524468\n",
            "iteration 1343 :train_loss:0.35997409352379117 val_loss0.3376811458008401\n",
            "iteration 1344 :train_loss:0.35995617157879556 val_loss0.3360901101191349\n",
            "iteration 1345 :train_loss:0.3599457702568438 val_loss0.33766470546062316\n",
            "iteration 1346 :train_loss:0.3599441291259679 val_loss0.33608744805348495\n",
            "iteration 1347 :train_loss:0.3599364460054485 val_loss0.337670264112452\n",
            "iteration 1348 :train_loss:0.3599452505356676 val_loss0.3360977286208978\n",
            "iteration 1349 :train_loss:0.35992726619418924 val_loss0.3376735829025182\n",
            "iteration 1350 :train_loss:0.35992403820234353 val_loss0.33608499841551065\n",
            "iteration 1351 :train_loss:0.35992658714061915 val_loss0.3376871278284236\n",
            "iteration 1352 :train_loss:0.35992518797719325 val_loss0.3360950278284975\n",
            "iteration 1353 :train_loss:0.3599056702000383 val_loss0.3376783525575381\n",
            "iteration 1354 :train_loss:0.35989803284349936 val_loss0.3360749410005642\n",
            "iteration 1355 :train_loss:0.3598889036879434 val_loss0.3376743939377987\n",
            "iteration 1356 :train_loss:0.3598826385906676 val_loss0.3360655032679087\n",
            "iteration 1357 :train_loss:0.3598775059045081 val_loss0.33767129627717085\n",
            "iteration 1358 :train_loss:0.35988925656092596 val_loss0.33607897824881805\n",
            "iteration 1359 :train_loss:0.35987985508591486 val_loss0.3376854076328453\n",
            "iteration 1360 :train_loss:0.3598822694936792 val_loss0.3360777815847512\n",
            "iteration 1361 :train_loss:0.3598561720534462 val_loss0.3376697173836209\n",
            "iteration 1362 :train_loss:0.3598677061217807 val_loss0.3360709978767232\n",
            "iteration 1363 :train_loss:0.359849957662819 val_loss0.3376718146682116\n",
            "iteration 1364 :train_loss:0.3598473923038069 val_loss0.33605687539385487\n",
            "iteration 1365 :train_loss:0.35981326179231216 val_loss0.3376423262209561\n",
            "iteration 1366 :train_loss:0.35980844048364347 val_loss0.33602558319380865\n",
            "iteration 1367 :train_loss:0.3597662619045746 val_loss0.3376034999722444\n",
            "iteration 1368 :train_loss:0.3597663864613779 val_loss0.3359894225903659\n",
            "iteration 1369 :train_loss:0.3597416309445566 val_loss0.3375891062576448\n",
            "iteration 1370 :train_loss:0.3597347145197556 val_loss0.3359687105062343\n",
            "iteration 1371 :train_loss:0.3596982158670638 val_loss0.337553185279546\n",
            "iteration 1372 :train_loss:0.35968162480089266 val_loss0.33592407987079026\n",
            "iteration 1373 :train_loss:0.35963617398974423 val_loss0.33749748985806427\n",
            "iteration 1374 :train_loss:0.35961075407906234 val_loss0.3358626578636393\n",
            "iteration 1375 :train_loss:0.3595600341566083 val_loss0.3374281559972417\n",
            "iteration 1376 :train_loss:0.3595518996777285 val_loss0.33581016778158246\n",
            "iteration 1377 :train_loss:0.3595024490252994 val_loss0.33737787429279625\n",
            "iteration 1378 :train_loss:0.3594780835280073 val_loss0.3357457065273942\n",
            "iteration 1379 :train_loss:0.35946045105272023 val_loss0.33734687123342794\n",
            "iteration 1380 :train_loss:0.3594585980505731 val_loss0.3357367618157879\n",
            "iteration 1381 :train_loss:0.35941362212381 val_loss0.3373138211315112\n",
            "iteration 1382 :train_loss:0.35940748392128036 val_loss0.33569845079669913\n",
            "iteration 1383 :train_loss:0.35937225658940414 val_loss0.3372839907411218\n",
            "iteration 1384 :train_loss:0.359371712620694 val_loss0.3356739322065429\n",
            "iteration 1385 :train_loss:0.35933768030084323 val_loss0.33725952836499173\n",
            "iteration 1386 :train_loss:0.35932548187967633 val_loss0.3356401853770797\n",
            "iteration 1387 :train_loss:0.3592891832286124 val_loss0.33722209148366966\n",
            "iteration 1388 :train_loss:0.35926366057925074 val_loss0.3355874264087578\n",
            "iteration 1389 :train_loss:0.35921719294123694 val_loss0.33715747895783277\n",
            "iteration 1390 :train_loss:0.3591992132495829 val_loss0.3355339614056788\n",
            "iteration 1391 :train_loss:0.3591703916523805 val_loss0.3371214793490791\n",
            "iteration 1392 :train_loss:0.359158070138803 val_loss0.33550229148832517\n",
            "iteration 1393 :train_loss:0.35912630540774415 val_loss0.33708770802654614\n",
            "iteration 1394 :train_loss:0.359109940087019 val_loss0.3354631706669597\n",
            "iteration 1395 :train_loss:0.35906031132427546 val_loss0.33702977466180173\n",
            "iteration 1396 :train_loss:0.3590467580076939 val_loss0.33540738543843096\n",
            "iteration 1397 :train_loss:0.3590104384786906 val_loss0.33698806771996925\n",
            "iteration 1398 :train_loss:0.35900541681046727 val_loss0.3353756024890755\n",
            "iteration 1399 :train_loss:0.35896668051871206 val_loss0.3369520891450885\n",
            "iteration 1400 :train_loss:0.35896460351994525 val_loss0.3353437863880208\n",
            "iteration 1401 :train_loss:0.3589300409133815 val_loss0.3369274993460094\n",
            "iteration 1402 :train_loss:0.35892259095294 val_loss0.3353111484527772\n",
            "iteration 1403 :train_loss:0.358892101533042 val_loss0.33689898332559026\n",
            "iteration 1404 :train_loss:0.3588984350419863 val_loss0.33529810993314224\n",
            "iteration 1405 :train_loss:0.35886296034663284 val_loss0.3368813857481668\n",
            "iteration 1406 :train_loss:0.35886641662325214 val_loss0.33527581191851613\n",
            "iteration 1407 :train_loss:0.35881955280041816 val_loss0.336847533482081\n",
            "iteration 1408 :train_loss:0.3588219570173167 val_loss0.33524278352013676\n",
            "iteration 1409 :train_loss:0.358778451489301 val_loss0.33681819643892125\n",
            "iteration 1410 :train_loss:0.3587746591845191 val_loss0.3352065377960795\n",
            "iteration 1411 :train_loss:0.3587362559714693 val_loss0.3367849374445223\n",
            "iteration 1412 :train_loss:0.3587310491843639 val_loss0.33517510590566574\n",
            "iteration 1413 :train_loss:0.3586959555778949 val_loss0.33675509250650404\n",
            "iteration 1414 :train_loss:0.35869984221104656 val_loss0.3351522520948424\n",
            "iteration 1415 :train_loss:0.3586551080100048 val_loss0.33672255335035045\n",
            "iteration 1416 :train_loss:0.35867223588082847 val_loss0.33513299192440127\n",
            "iteration 1417 :train_loss:0.3586390138811971 val_loss0.3367147808951083\n",
            "iteration 1418 :train_loss:0.35864608675530274 val_loss0.335113335320667\n",
            "iteration 1419 :train_loss:0.35861579022196455 val_loss0.336699946889828\n",
            "iteration 1420 :train_loss:0.3586055757943542 val_loss0.33508149687530686\n",
            "iteration 1421 :train_loss:0.35856067266322994 val_loss0.33664945324575946\n",
            "iteration 1422 :train_loss:0.3585469443119549 val_loss0.33502992825142036\n",
            "iteration 1423 :train_loss:0.35851090678326564 val_loss0.33660662742500386\n",
            "iteration 1424 :train_loss:0.35850664744563776 val_loss0.3350002204037321\n",
            "iteration 1425 :train_loss:0.35845219510159954 val_loss0.33655260809011117\n",
            "iteration 1426 :train_loss:0.3584637656885836 val_loss0.33496315138276805\n",
            "iteration 1427 :train_loss:0.3584279413253093 val_loss0.33653581761326373\n",
            "iteration 1428 :train_loss:0.3584377532361326 val_loss0.3349454797984628\n",
            "iteration 1429 :train_loss:0.3584126727065761 val_loss0.33652809133477185\n",
            "iteration 1430 :train_loss:0.35842833647918293 val_loss0.3349430173335411\n",
            "iteration 1431 :train_loss:0.35842227266476284 val_loss0.3365492648550533\n",
            "iteration 1432 :train_loss:0.35844534382891935 val_loss0.3349678167777944\n",
            "iteration 1433 :train_loss:0.35843226538939216 val_loss0.3365694383301052\n",
            "iteration 1434 :train_loss:0.3584371525496014 val_loss0.33496655874142933\n",
            "iteration 1435 :train_loss:0.358429584969562 val_loss0.3365749403746934\n",
            "iteration 1436 :train_loss:0.35845299042767553 val_loss0.3349881493540453\n",
            "iteration 1437 :train_loss:0.3584575976134308 val_loss0.33661304969024647\n",
            "iteration 1438 :train_loss:0.3584892204162961 val_loss0.3350315427445364\n",
            "iteration 1439 :train_loss:0.35849435302410465 val_loss0.3366615623453659\n",
            "iteration 1440 :train_loss:0.35852033118422677 val_loss0.33506704647184105\n",
            "iteration 1441 :train_loss:0.35850830816923673 val_loss0.3366827333148023\n",
            "iteration 1442 :train_loss:0.3585402035790544 val_loss0.3350944592280877\n",
            "iteration 1443 :train_loss:0.3585200737097376 val_loss0.3367035982468203\n",
            "iteration 1444 :train_loss:0.35854951793437 val_loss0.3351078868639289\n",
            "iteration 1445 :train_loss:0.3585388084141277 val_loss0.33673096285860593\n",
            "iteration 1446 :train_loss:0.3585683463426804 val_loss0.33513494633214014\n",
            "iteration 1447 :train_loss:0.35854790335802655 val_loss0.33675193428632755\n",
            "iteration 1448 :train_loss:0.35858773878291444 val_loss0.335158439027118\n",
            "iteration 1449 :train_loss:0.3585714177201129 val_loss0.33678603495608234\n",
            "iteration 1450 :train_loss:0.3586061920241716 val_loss0.33518314946205274\n",
            "iteration 1451 :train_loss:0.358577895712578 val_loss0.33680696513607755\n",
            "iteration 1452 :train_loss:0.35861695889296236 val_loss0.33519934919347205\n",
            "iteration 1453 :train_loss:0.35857255246286884 val_loss0.3368122127727144\n",
            "iteration 1454 :train_loss:0.3586126795255095 val_loss0.335199903989141\n",
            "iteration 1455 :train_loss:0.3585955247533372 val_loss0.3368498251097588\n",
            "iteration 1456 :train_loss:0.3586389614068349 val_loss0.33523264149668003\n",
            "iteration 1457 :train_loss:0.35860991378625195 val_loss0.33687365724556495\n",
            "iteration 1458 :train_loss:0.35865338781510225 val_loss0.33525111130353635\n",
            "iteration 1459 :train_loss:0.3586097204759691 val_loss0.33688345051651286\n",
            "iteration 1460 :train_loss:0.35864618395079384 val_loss0.3352461598258881\n",
            "iteration 1461 :train_loss:0.35860963889401376 val_loss0.33689051818945953\n",
            "iteration 1462 :train_loss:0.3586374255491328 val_loss0.3352434797290953\n",
            "iteration 1463 :train_loss:0.35858762270634875 val_loss0.3368778732790371\n",
            "iteration 1464 :train_loss:0.3586206476632433 val_loss0.3352301328126415\n",
            "iteration 1465 :train_loss:0.3585684503333977 val_loss0.3368673920836102\n",
            "iteration 1466 :train_loss:0.35860950633687644 val_loss0.33522307169884547\n",
            "iteration 1467 :train_loss:0.35855211128650977 val_loss0.3368588950768316\n",
            "iteration 1468 :train_loss:0.35857178450853977 val_loss0.3351883875922258\n",
            "iteration 1469 :train_loss:0.3585176031299566 val_loss0.3368309842165545\n",
            "iteration 1470 :train_loss:0.3585477381930063 val_loss0.3351698634129537\n",
            "iteration 1471 :train_loss:0.3584854365706463 val_loss0.33680703546441815\n",
            "iteration 1472 :train_loss:0.35850200898155005 val_loss0.3351294896970775\n",
            "iteration 1473 :train_loss:0.3584325467546519 val_loss0.3367617174828668\n",
            "iteration 1474 :train_loss:0.3584503791853423 val_loss0.3350858492047148\n",
            "iteration 1475 :train_loss:0.3583802426912745 val_loss0.33671549066770384\n",
            "iteration 1476 :train_loss:0.3583686058271822 val_loss0.3350121932419779\n",
            "iteration 1477 :train_loss:0.35829123696006643 val_loss0.33663473695355445\n",
            "iteration 1478 :train_loss:0.358288400812864 val_loss0.3349373629761065\n",
            "iteration 1479 :train_loss:0.3582173624621884 val_loss0.336566264346227\n",
            "iteration 1480 :train_loss:0.3582077871855501 val_loss0.3348649079007009\n",
            "iteration 1481 :train_loss:0.3581376648263227 val_loss0.33649393643334174\n",
            "iteration 1482 :train_loss:0.358135118205497 val_loss0.3348007509424075\n",
            "iteration 1483 :train_loss:0.3580568782628614 val_loss0.336418519338399\n",
            "iteration 1484 :train_loss:0.35803643245029426 val_loss0.33470772888333356\n",
            "iteration 1485 :train_loss:0.35798890147716755 val_loss0.3363600270313696\n",
            "iteration 1486 :train_loss:0.3579834304024255 val_loss0.33466275924441513\n",
            "iteration 1487 :train_loss:0.35793834024483 val_loss0.33631973091412953\n",
            "iteration 1488 :train_loss:0.3579212194026488 val_loss0.33460806549142763\n",
            "iteration 1489 :train_loss:0.35787053953001274 val_loss0.3362594971021667\n",
            "iteration 1490 :train_loss:0.3578671215315198 val_loss0.33456150908740345\n",
            "iteration 1491 :train_loss:0.35781680341284966 val_loss0.336216285028321\n",
            "iteration 1492 :train_loss:0.3578103456699971 val_loss0.3345098454452902\n",
            "iteration 1493 :train_loss:0.3577421766777727 val_loss0.33615069685549953\n",
            "iteration 1494 :train_loss:0.3577377338252051 val_loss0.3344432182779148\n",
            "iteration 1495 :train_loss:0.3576976881259726 val_loss0.3361154924762259\n",
            "iteration 1496 :train_loss:0.35768659560345584 val_loss0.3343990385736086\n",
            "iteration 1497 :train_loss:0.357640525365458 val_loss0.3360657899619487\n",
            "iteration 1498 :train_loss:0.35764129628121727 val_loss0.33435921129683466\n",
            "iteration 1499 :train_loss:0.3576090488888015 val_loss0.33604379260517375\n",
            "iteration 1500 :train_loss:0.35761625817090437 val_loss0.33433843634615507\n",
            "iteration 1501 :train_loss:0.35758154043115953 val_loss0.33602516513013075\n",
            "iteration 1502 :train_loss:0.3575719098765774 val_loss0.33430012728626496\n",
            "iteration 1503 :train_loss:0.3575388644640861 val_loss0.3359930767671369\n",
            "iteration 1504 :train_loss:0.3575383081960986 val_loss0.33427059921949787\n",
            "iteration 1505 :train_loss:0.35749601916007345 val_loss0.3359562464988785\n",
            "iteration 1506 :train_loss:0.357496703423114 val_loss0.33423141216808694\n",
            "iteration 1507 :train_loss:0.35743954984847276 val_loss0.3359077161155667\n",
            "iteration 1508 :train_loss:0.35743679019632574 val_loss0.3341799046579606\n",
            "iteration 1509 :train_loss:0.357396765916472 val_loss0.33587660168793504\n",
            "iteration 1510 :train_loss:0.35740094128536964 val_loss0.3341492046995785\n",
            "iteration 1511 :train_loss:0.35735812983403464 val_loss0.33584616152887786\n",
            "iteration 1512 :train_loss:0.35735628814362497 val_loss0.3341070981498896\n",
            "iteration 1513 :train_loss:0.35731832752904596 val_loss0.3358143755558919\n",
            "iteration 1514 :train_loss:0.357300500713085 val_loss0.33405743280405015\n",
            "iteration 1515 :train_loss:0.3572474497164942 val_loss0.33575027685850445\n",
            "iteration 1516 :train_loss:0.35723864728896004 val_loss0.3340013819114535\n",
            "iteration 1517 :train_loss:0.3572002872143511 val_loss0.3357106703787366\n",
            "iteration 1518 :train_loss:0.357188655699301 val_loss0.333956861712418\n",
            "iteration 1519 :train_loss:0.3571539770945832 val_loss0.33567327768808575\n",
            "iteration 1520 :train_loss:0.357142197329489 val_loss0.3339157779655094\n",
            "iteration 1521 :train_loss:0.3571076097731101 val_loss0.335634904698335\n",
            "iteration 1522 :train_loss:0.3571056735495268 val_loss0.33388593178627646\n",
            "iteration 1523 :train_loss:0.35708893912475986 val_loss0.3356266534369704\n",
            "iteration 1524 :train_loss:0.3571047460171141 val_loss0.3338912029121577\n",
            "iteration 1525 :train_loss:0.35708617481338745 val_loss0.33563271675638584\n",
            "iteration 1526 :train_loss:0.3571136877638281 val_loss0.33390563319358546\n",
            "iteration 1527 :train_loss:0.3570982105692784 val_loss0.3356546464621387\n",
            "iteration 1528 :train_loss:0.3571251125293164 val_loss0.33392243906812313\n",
            "iteration 1529 :train_loss:0.35712514467588735 val_loss0.33569424323222\n",
            "iteration 1530 :train_loss:0.3571607966994962 val_loss0.3339629351107509\n",
            "iteration 1531 :train_loss:0.3571350087302852 val_loss0.3357119944843153\n",
            "iteration 1532 :train_loss:0.3571530404641585 val_loss0.3339634833736513\n",
            "iteration 1533 :train_loss:0.35710951546090663 val_loss0.3356942176859639\n",
            "iteration 1534 :train_loss:0.35712448305807654 val_loss0.333942437844076\n",
            "iteration 1535 :train_loss:0.35707768456574585 val_loss0.3356680598472262\n",
            "iteration 1536 :train_loss:0.3570896266801561 val_loss0.33391549314361946\n",
            "iteration 1537 :train_loss:0.35703434612441226 val_loss0.33563029052657944\n",
            "iteration 1538 :train_loss:0.3570423416469654 val_loss0.3338776466003462\n",
            "iteration 1539 :train_loss:0.356985350964485 val_loss0.33558880812301844\n",
            "iteration 1540 :train_loss:0.3570099112844197 val_loss0.3338508548567228\n",
            "iteration 1541 :train_loss:0.3569684823920952 val_loss0.33557882670006545\n",
            "iteration 1542 :train_loss:0.3569871870095885 val_loss0.33383565057925796\n",
            "iteration 1543 :train_loss:0.3569351358383746 val_loss0.33555519111160154\n",
            "iteration 1544 :train_loss:0.3569572054657645 val_loss0.33381350766050016\n",
            "iteration 1545 :train_loss:0.35692136670089003 val_loss0.33554837365477475\n",
            "iteration 1546 :train_loss:0.3569446088727176 val_loss0.33380759997726617\n",
            "iteration 1547 :train_loss:0.3568749639638912 val_loss0.3355087280904748\n",
            "iteration 1548 :train_loss:0.3569097740878132 val_loss0.3337788814125252\n",
            "iteration 1549 :train_loss:0.35687050479101073 val_loss0.33551208738513727\n",
            "iteration 1550 :train_loss:0.3568812957438614 val_loss0.33375607862863244\n",
            "iteration 1551 :train_loss:0.3568234583283032 val_loss0.3354710851764842\n",
            "iteration 1552 :train_loss:0.35684033326612385 val_loss0.3337227020675285\n",
            "iteration 1553 :train_loss:0.35676938431471955 val_loss0.3354237306478278\n",
            "iteration 1554 :train_loss:0.35678502966575704 val_loss0.3336725910974191\n",
            "iteration 1555 :train_loss:0.35671276225125087 val_loss0.33537236034965245\n",
            "iteration 1556 :train_loss:0.356715585561537 val_loss0.33361075647471566\n",
            "iteration 1557 :train_loss:0.3566225150591619 val_loss0.33528528189650814\n",
            "iteration 1558 :train_loss:0.35662605695570065 val_loss0.33352676938946896\n",
            "iteration 1559 :train_loss:0.356535485532829 val_loss0.3352035120321194\n",
            "iteration 1560 :train_loss:0.3565452700513369 val_loss0.33345398866540255\n",
            "iteration 1561 :train_loss:0.35647004174557534 val_loss0.33514359460564297\n",
            "iteration 1562 :train_loss:0.35648669663234717 val_loss0.3333999539413185\n",
            "iteration 1563 :train_loss:0.3564279022306372 val_loss0.33510752396062127\n",
            "iteration 1564 :train_loss:0.356444447209814 val_loss0.33336510768514654\n",
            "iteration 1565 :train_loss:0.3563745867232814 val_loss0.3350621290675148\n",
            "iteration 1566 :train_loss:0.35639141786085166 val_loss0.3333192342673931\n",
            "iteration 1567 :train_loss:0.3563400124398427 val_loss0.33503676853570874\n",
            "iteration 1568 :train_loss:0.35635923247635476 val_loss0.3332926283037188\n",
            "iteration 1569 :train_loss:0.35629555114959355 val_loss0.3350001599488919\n",
            "iteration 1570 :train_loss:0.3563110888426085 val_loss0.33325398478105683\n",
            "iteration 1571 :train_loss:0.356255500262707 val_loss0.33496948673047516\n",
            "iteration 1572 :train_loss:0.356279679432989 val_loss0.33322977609311866\n",
            "iteration 1573 :train_loss:0.3562383622495853 val_loss0.3349592216304443\n",
            "iteration 1574 :train_loss:0.35626355417098826 val_loss0.3332185932471952\n",
            "iteration 1575 :train_loss:0.3562026036553658 val_loss0.33492943486189175\n",
            "iteration 1576 :train_loss:0.356240495812112 val_loss0.333201390004618\n",
            "iteration 1577 :train_loss:0.3562038734380597 val_loss0.33493996550560723\n",
            "iteration 1578 :train_loss:0.3562550897423427 val_loss0.33322464927367934\n",
            "iteration 1579 :train_loss:0.35620909489826436 val_loss0.3349583412404146\n",
            "iteration 1580 :train_loss:0.3562568780418584 val_loss0.3332338053739997\n",
            "iteration 1581 :train_loss:0.3562481786341244 val_loss0.3350087874296313\n",
            "iteration 1582 :train_loss:0.35630795556947675 val_loss0.333293962861547\n",
            "iteration 1583 :train_loss:0.35626387666873405 val_loss0.33504028011400694\n",
            "iteration 1584 :train_loss:0.3563337868895603 val_loss0.3333280858819921\n",
            "iteration 1585 :train_loss:0.35632272624011063 val_loss0.33511415230820746\n",
            "iteration 1586 :train_loss:0.35637783554755637 val_loss0.33338024663965343\n",
            "iteration 1587 :train_loss:0.356350834102779 val_loss0.335154773571255\n",
            "iteration 1588 :train_loss:0.356413803065391 val_loss0.3334277612802598\n",
            "iteration 1589 :train_loss:0.3563941969279227 val_loss0.3352135468717952\n",
            "iteration 1590 :train_loss:0.35643750065232166 val_loss0.3334574626428279\n",
            "iteration 1591 :train_loss:0.35640097217744066 val_loss0.3352303828895274\n",
            "iteration 1592 :train_loss:0.35644310291692394 val_loss0.333475681684737\n",
            "iteration 1593 :train_loss:0.3564020604242804 val_loss0.3352445782804259\n",
            "iteration 1594 :train_loss:0.35642953471587724 val_loss0.33347027458631906\n",
            "iteration 1595 :train_loss:0.35636491725604036 val_loss0.33521704018375637\n",
            "iteration 1596 :train_loss:0.35638148028461497 val_loss0.33343255282870826\n",
            "iteration 1597 :train_loss:0.3563118069081023 val_loss0.33517141169601494\n",
            "iteration 1598 :train_loss:0.35632229522889625 val_loss0.33338470442156515\n",
            "iteration 1599 :train_loss:0.35624675790395405 val_loss0.33511665206976227\n",
            "iteration 1600 :train_loss:0.35625209600704344 val_loss0.3333227028102536\n",
            "iteration 1601 :train_loss:0.35617305425598256 val_loss0.33505071778799583\n",
            "iteration 1602 :train_loss:0.356194363478491 val_loss0.3332770307697263\n",
            "iteration 1603 :train_loss:0.35612461735615497 val_loss0.3350112965826185\n",
            "iteration 1604 :train_loss:0.35614851131911673 val_loss0.33324034262905533\n",
            "iteration 1605 :train_loss:0.3560907953126406 val_loss0.3349868688157466\n",
            "iteration 1606 :train_loss:0.3561160906826139 val_loss0.33321863997619827\n",
            "iteration 1607 :train_loss:0.35603597404682436 val_loss0.3349406878968706\n",
            "iteration 1608 :train_loss:0.3560507755186242 val_loss0.33316145469231057\n",
            "iteration 1609 :train_loss:0.3559628593722896 val_loss0.334872283646902\n",
            "iteration 1610 :train_loss:0.355966312789251 val_loss0.33308847878175274\n",
            "iteration 1611 :train_loss:0.3558838586803335 val_loss0.3348019387751607\n",
            "iteration 1612 :train_loss:0.355889453515596 val_loss0.33302490808728763\n",
            "iteration 1613 :train_loss:0.3558012596415069 val_loss0.33472771760139136\n",
            "iteration 1614 :train_loss:0.35580081369877226 val_loss0.33294982457354383\n",
            "iteration 1615 :train_loss:0.3557119519739981 val_loss0.3346464445991879\n",
            "iteration 1616 :train_loss:0.3557240463119082 val_loss0.3328831534730685\n",
            "iteration 1617 :train_loss:0.3556447334416829 val_loss0.3345853845271986\n",
            "iteration 1618 :train_loss:0.35565247267861194 val_loss0.33282102510418204\n",
            "iteration 1619 :train_loss:0.35556562905268996 val_loss0.3345128651216281\n",
            "iteration 1620 :train_loss:0.3555857867183383 val_loss0.3327670016256745\n",
            "iteration 1621 :train_loss:0.3555052064464595 val_loss0.33446052930383907\n",
            "iteration 1622 :train_loss:0.355528865573214 val_loss0.33271961627534125\n",
            "iteration 1623 :train_loss:0.35546454058218074 val_loss0.334429288688353\n",
            "iteration 1624 :train_loss:0.3554949062912561 val_loss0.33269504142523865\n",
            "iteration 1625 :train_loss:0.3554307303677583 val_loss0.3344044551296032\n",
            "iteration 1626 :train_loss:0.3554772992893659 val_loss0.3326853942305003\n",
            "iteration 1627 :train_loss:0.3554076216563424 val_loss0.3343909266141443\n",
            "iteration 1628 :train_loss:0.3554571406276606 val_loss0.3326752386522393\n",
            "iteration 1629 :train_loss:0.3553954446639209 val_loss0.33438940975487036\n",
            "iteration 1630 :train_loss:0.35544493057518584 val_loss0.3326721198060185\n",
            "iteration 1631 :train_loss:0.355382569704567 val_loss0.33438469684265926\n",
            "iteration 1632 :train_loss:0.3554272918487685 val_loss0.3326653367670355\n",
            "iteration 1633 :train_loss:0.3553978108028606 val_loss0.33441201247433244\n",
            "iteration 1634 :train_loss:0.35545302436117826 val_loss0.3327004680289167\n",
            "iteration 1635 :train_loss:0.3554110927583545 val_loss0.3344344347506805\n",
            "iteration 1636 :train_loss:0.3554621231389991 val_loss0.33272012224193\n",
            "iteration 1637 :train_loss:0.355416232255881 val_loss0.3344503470346534\n",
            "iteration 1638 :train_loss:0.35546963287683175 val_loss0.3327384355689434\n",
            "iteration 1639 :train_loss:0.355426823177657 val_loss0.33447090079518427\n",
            "iteration 1640 :train_loss:0.3554784864153513 val_loss0.33275720272431736\n",
            "iteration 1641 :train_loss:0.3554361945396719 val_loss0.3344906054530267\n",
            "iteration 1642 :train_loss:0.35548274626850046 val_loss0.3327749280940243\n",
            "iteration 1643 :train_loss:0.355426233304492 val_loss0.33449154553830246\n",
            "iteration 1644 :train_loss:0.355460965784292 val_loss0.3327626672724919\n",
            "iteration 1645 :train_loss:0.3554226990419588 val_loss0.3344985586537409\n",
            "iteration 1646 :train_loss:0.35545247138533786 val_loss0.3327637958390993\n",
            "iteration 1647 :train_loss:0.35539037175800403 val_loss0.3344732573353185\n",
            "iteration 1648 :train_loss:0.3554175774303103 val_loss0.3327396620488513\n",
            "iteration 1649 :train_loss:0.3553708907015771 val_loss0.3344615970810959\n",
            "iteration 1650 :train_loss:0.35540287688309635 val_loss0.3327337021926028\n",
            "iteration 1651 :train_loss:0.35535947252826916 val_loss0.33445994592768685\n",
            "iteration 1652 :train_loss:0.35538236817713337 val_loss0.3327225644551572\n",
            "iteration 1653 :train_loss:0.35532267524593675 val_loss0.33443052844664206\n",
            "iteration 1654 :train_loss:0.3553578753637538 val_loss0.33270704073474905\n",
            "iteration 1655 :train_loss:0.3553016220086688 val_loss0.33441926203166855\n",
            "iteration 1656 :train_loss:0.3553331221581549 val_loss0.3326923891057076\n",
            "iteration 1657 :train_loss:0.35529212635826524 val_loss0.3344216503883338\n",
            "iteration 1658 :train_loss:0.3553153484363337 val_loss0.33268297812887737\n",
            "iteration 1659 :train_loss:0.3552582177072708 val_loss0.33439609451204516\n",
            "iteration 1660 :train_loss:0.35529298368429807 val_loss0.33267077658130034\n",
            "iteration 1661 :train_loss:0.35524478587993147 val_loss0.3343911642966589\n",
            "iteration 1662 :train_loss:0.355272174034283 val_loss0.3326590785997594\n",
            "iteration 1663 :train_loss:0.3552011462195184 val_loss0.33435439532528904\n",
            "iteration 1664 :train_loss:0.35522674337519444 val_loss0.33262491093666496\n",
            "iteration 1665 :train_loss:0.3551506062126485 val_loss0.334314250535879\n",
            "iteration 1666 :train_loss:0.355178322924747 val_loss0.33258599959451535\n",
            "iteration 1667 :train_loss:0.3551089830457006 val_loss0.3342797924879928\n",
            "iteration 1668 :train_loss:0.35513161869786936 val_loss0.3325498729647308\n",
            "iteration 1669 :train_loss:0.35505171498621524 val_loss0.334231790713284\n",
            "iteration 1670 :train_loss:0.3550624121342317 val_loss0.3324906870027972\n",
            "iteration 1671 :train_loss:0.3549870748672762 val_loss0.3341746657311162\n",
            "iteration 1672 :train_loss:0.3549951726376855 val_loss0.33243314809696045\n",
            "iteration 1673 :train_loss:0.35493490565324654 val_loss0.33413200467875614\n",
            "iteration 1674 :train_loss:0.3549415907665887 val_loss0.3323895714413363\n",
            "iteration 1675 :train_loss:0.3548704653796076 val_loss0.33407386063750516\n",
            "iteration 1676 :train_loss:0.35487573303158 val_loss0.3323316532607196\n",
            "iteration 1677 :train_loss:0.35481216504929797 val_loss0.3340247960450912\n",
            "iteration 1678 :train_loss:0.35481662903028793 val_loss0.3322804917928199\n",
            "iteration 1679 :train_loss:0.35475251120659695 val_loss0.3339704541627466\n",
            "iteration 1680 :train_loss:0.3547663063108212 val_loss0.3322362297580634\n",
            "iteration 1681 :train_loss:0.3547018785602712 val_loss0.3339280840174948\n",
            "iteration 1682 :train_loss:0.3547145271644446 val_loss0.33219147647289377\n",
            "iteration 1683 :train_loss:0.354649502462052 val_loss0.33388218912349077\n",
            "iteration 1684 :train_loss:0.3546627580563848 val_loss0.3321465696299338\n",
            "iteration 1685 :train_loss:0.35460380282915704 val_loss0.3338408616960421\n",
            "iteration 1686 :train_loss:0.354635624425078 val_loss0.33212472243456725\n",
            "iteration 1687 :train_loss:0.3545792250503923 val_loss0.3338249927043771\n",
            "iteration 1688 :train_loss:0.35460080149277445 val_loss0.33209702352601056\n",
            "iteration 1689 :train_loss:0.3545374520076525 val_loss0.3337904943508877\n",
            "iteration 1690 :train_loss:0.35455901445653376 val_loss0.33206438189194437\n",
            "iteration 1691 :train_loss:0.3545049515083937 val_loss0.33376658828710276\n",
            "iteration 1692 :train_loss:0.35452658977946194 val_loss0.33203913321343204\n",
            "iteration 1693 :train_loss:0.3544735713096715 val_loss0.3337449051973706\n",
            "iteration 1694 :train_loss:0.3545091978608742 val_loss0.33203159279091216\n",
            "iteration 1695 :train_loss:0.35445855319916497 val_loss0.33373884349783267\n",
            "iteration 1696 :train_loss:0.35448352796301075 val_loss0.33201453100636985\n",
            "iteration 1697 :train_loss:0.35442885793404616 val_loss0.3337164842822161\n",
            "iteration 1698 :train_loss:0.3544602676203595 val_loss0.33199839359844097\n",
            "iteration 1699 :train_loss:0.3544156541110589 val_loss0.33371292110968015\n",
            "iteration 1700 :train_loss:0.3544637079410036 val_loss0.3320094047319804\n",
            "iteration 1701 :train_loss:0.35444335815337674 val_loss0.33374959712876573\n",
            "iteration 1702 :train_loss:0.35447415097902285 val_loss0.3320268184184295\n",
            "iteration 1703 :train_loss:0.35444004650821975 val_loss0.3337540013958498\n",
            "iteration 1704 :train_loss:0.3544774417955424 val_loss0.33203734567943055\n",
            "iteration 1705 :train_loss:0.35443768218439714 val_loss0.3337604002179421\n",
            "iteration 1706 :train_loss:0.3544681658552627 val_loss0.33203324229640646\n",
            "iteration 1707 :train_loss:0.3544100778702255 val_loss0.333736392885774\n",
            "iteration 1708 :train_loss:0.3544439572314497 val_loss0.33201276644434324\n",
            "iteration 1709 :train_loss:0.3544185688673982 val_loss0.3337563390302852\n",
            "iteration 1710 :train_loss:0.35444340800758994 val_loss0.33201853678331256\n",
            "iteration 1711 :train_loss:0.35441582297555557 val_loss0.33376035249605485\n",
            "iteration 1712 :train_loss:0.3544607275485028 val_loss0.3320417559625282\n",
            "iteration 1713 :train_loss:0.35443605078608903 val_loss0.33378994247159866\n",
            "iteration 1714 :train_loss:0.3544773637611095 val_loss0.3320618735227589\n",
            "iteration 1715 :train_loss:0.35445341964266874 val_loss0.3338155721065416\n",
            "iteration 1716 :train_loss:0.35451815498956885 val_loss0.3321099586248117\n",
            "iteration 1717 :train_loss:0.3545015059030806 val_loss0.33387233315932074\n",
            "iteration 1718 :train_loss:0.3545638254470879 val_loss0.3321607573648645\n",
            "iteration 1719 :train_loss:0.3545373293278913 val_loss0.33391773353189347\n",
            "iteration 1720 :train_loss:0.35459988041392637 val_loss0.33220045819404376\n",
            "iteration 1721 :train_loss:0.35459382680905543 val_loss0.3339834426335639\n",
            "iteration 1722 :train_loss:0.3546510239730804 val_loss0.33225570263586907\n",
            "iteration 1723 :train_loss:0.3546429571851803 val_loss0.33404220885624075\n",
            "iteration 1724 :train_loss:0.35469015709191337 val_loss0.33229883484171147\n",
            "iteration 1725 :train_loss:0.3546632587972868 val_loss0.3340665657053243\n",
            "iteration 1726 :train_loss:0.3547060832828356 val_loss0.33232011931603317\n",
            "iteration 1727 :train_loss:0.35468458742907444 val_loss0.33409670886903425\n",
            "iteration 1728 :train_loss:0.35471217018988677 val_loss0.3323319905794054\n",
            "iteration 1729 :train_loss:0.354673614564155 val_loss0.33408965914560435\n",
            "iteration 1730 :train_loss:0.35469360303947867 val_loss0.3323200747254247\n",
            "iteration 1731 :train_loss:0.3546415653126296 val_loss0.33406335521340985\n",
            "iteration 1732 :train_loss:0.35464297152490426 val_loss0.33227448459177955\n",
            "iteration 1733 :train_loss:0.35458630631991633 val_loss0.33401047111382615\n",
            "iteration 1734 :train_loss:0.35459493178642265 val_loss0.3322338350302683\n",
            "iteration 1735 :train_loss:0.3545612069020569 val_loss0.33399027226306466\n",
            "iteration 1736 :train_loss:0.3545754550565545 val_loss0.3322227154596615\n",
            "iteration 1737 :train_loss:0.354515858209012 val_loss0.33394833590208434\n",
            "iteration 1738 :train_loss:0.35450800551179834 val_loss0.3321598097353953\n",
            "iteration 1739 :train_loss:0.3544624516286997 val_loss0.33390026597990713\n",
            "iteration 1740 :train_loss:0.3544507459448019 val_loss0.3321092838510039\n",
            "iteration 1741 :train_loss:0.35439618881275514 val_loss0.33383775340409405\n",
            "iteration 1742 :train_loss:0.35438352615267654 val_loss0.33204769215340335\n",
            "iteration 1743 :train_loss:0.3543440957196339 val_loss0.33379231423461675\n",
            "iteration 1744 :train_loss:0.3543404340937829 val_loss0.33201389550970367\n",
            "iteration 1745 :train_loss:0.35426950991156725 val_loss0.33372132669703014\n",
            "iteration 1746 :train_loss:0.3542409079836882 val_loss0.33192069688147113\n",
            "iteration 1747 :train_loss:0.35418872006257845 val_loss0.33364484529695954\n",
            "iteration 1748 :train_loss:0.3541603302683402 val_loss0.3318484712981971\n",
            "iteration 1749 :train_loss:0.35408953877073823 val_loss0.3335496183911993\n",
            "iteration 1750 :train_loss:0.3540721627669903 val_loss0.33176676826921636\n",
            "iteration 1751 :train_loss:0.3539913423707662 val_loss0.33345513873829447\n",
            "iteration 1752 :train_loss:0.35396615323508007 val_loss0.3316675052454162\n",
            "iteration 1753 :train_loss:0.3539066439466546 val_loss0.3333755853561988\n",
            "iteration 1754 :train_loss:0.3539009645550166 val_loss0.3316116741072746\n",
            "iteration 1755 :train_loss:0.35383812204094606 val_loss0.3333132068758177\n",
            "iteration 1756 :train_loss:0.3538419325896556 val_loss0.33156093093618877\n",
            "iteration 1757 :train_loss:0.35380865652103477 val_loss0.33329160211962305\n",
            "iteration 1758 :train_loss:0.35381191542338947 val_loss0.33154165941811264\n",
            "iteration 1759 :train_loss:0.3537449268985495 val_loss0.33323404812447355\n",
            "iteration 1760 :train_loss:0.353753176437368 val_loss0.3314923802036981\n",
            "iteration 1761 :train_loss:0.353713074267758 val_loss0.33320783703160606\n",
            "iteration 1762 :train_loss:0.3537316511481624 val_loss0.33147817342161745\n",
            "iteration 1763 :train_loss:0.35368346892917163 val_loss0.33318471018201373\n",
            "iteration 1764 :train_loss:0.35367611089506096 val_loss0.3314289337428308\n",
            "iteration 1765 :train_loss:0.3536249132530992 val_loss0.3331305372508285\n",
            "iteration 1766 :train_loss:0.35364906923992345 val_loss0.33141178292027523\n",
            "iteration 1767 :train_loss:0.353593823067972 val_loss0.3331074679420908\n",
            "iteration 1768 :train_loss:0.35359953450968357 val_loss0.33136794909398665\n",
            "iteration 1769 :train_loss:0.3535508680482922 val_loss0.33306806457913596\n",
            "iteration 1770 :train_loss:0.3535784853243782 val_loss0.33135334746720846\n",
            "iteration 1771 :train_loss:0.3535284247456876 val_loss0.3330525233763813\n",
            "iteration 1772 :train_loss:0.35356517398793796 val_loss0.33134814338365\n",
            "iteration 1773 :train_loss:0.353532264914907 val_loss0.33306630314660807\n",
            "iteration 1774 :train_loss:0.3535736274016247 val_loss0.3313650072381531\n",
            "iteration 1775 :train_loss:0.3535420682181673 val_loss0.3330861740914996\n",
            "iteration 1776 :train_loss:0.3535901490541985 val_loss0.33138797902177836\n",
            "iteration 1777 :train_loss:0.3535583229645204 val_loss0.33311114225286365\n",
            "iteration 1778 :train_loss:0.3536106565128379 val_loss0.33141341755373116\n",
            "iteration 1779 :train_loss:0.3535992283931626 val_loss0.33316042560354386\n",
            "iteration 1780 :train_loss:0.35367111637348714 val_loss0.3314794274924934\n",
            "iteration 1781 :train_loss:0.35365794973469183 val_loss0.3332324952735436\n",
            "iteration 1782 :train_loss:0.3537251592833122 val_loss0.3315392027860534\n",
            "iteration 1783 :train_loss:0.35372079827878233 val_loss0.33330588549914786\n",
            "iteration 1784 :train_loss:0.35377695665968567 val_loss0.3315969127264759\n",
            "iteration 1785 :train_loss:0.3537493570604104 val_loss0.3333443934129577\n",
            "iteration 1786 :train_loss:0.3538091276945654 val_loss0.3316336502157208\n",
            "iteration 1787 :train_loss:0.3537762372076782 val_loss0.33337813752040746\n",
            "iteration 1788 :train_loss:0.35383364479758195 val_loss0.3316631657768751\n",
            "iteration 1789 :train_loss:0.35379739983628133 val_loss0.3334087540874032\n",
            "iteration 1790 :train_loss:0.35385484317565097 val_loss0.331689413651174\n",
            "iteration 1791 :train_loss:0.35381790178916434 val_loss0.33343782572425357\n",
            "iteration 1792 :train_loss:0.3538709685300391 val_loss0.33171126551961\n",
            "iteration 1793 :train_loss:0.3537968626312947 val_loss0.33342165502056414\n",
            "iteration 1794 :train_loss:0.3538237746947661 val_loss0.33166878132430555\n",
            "iteration 1795 :train_loss:0.3537653427772701 val_loss0.33339732755084184\n",
            "iteration 1796 :train_loss:0.35380429290703913 val_loss0.33165669749304333\n",
            "iteration 1797 :train_loss:0.3537503722086266 val_loss0.33339175013483247\n",
            "iteration 1798 :train_loss:0.35379093758101865 val_loss0.3316537353491441\n",
            "iteration 1799 :train_loss:0.3537370069067159 val_loss0.33338633098124515\n",
            "iteration 1800 :train_loss:0.35376508401008483 val_loss0.3316342999571246\n",
            "iteration 1801 :train_loss:0.3537162182127658 val_loss0.3333738655877906\n",
            "iteration 1802 :train_loss:0.35372055263165975 val_loss0.3315969681765828\n",
            "iteration 1803 :train_loss:0.35368696487158513 val_loss0.3333514893833702\n",
            "iteration 1804 :train_loss:0.3537253982249391 val_loss0.3316083037877374\n",
            "iteration 1805 :train_loss:0.35367336051963905 val_loss0.3333434597682639\n",
            "iteration 1806 :train_loss:0.35370756886664884 val_loss0.33159626912852247\n",
            "iteration 1807 :train_loss:0.353675877980968 val_loss0.3333559428796986\n",
            "iteration 1808 :train_loss:0.3537103224854857 val_loss0.33160678743149913\n",
            "iteration 1809 :train_loss:0.3536724074974522 val_loss0.3333615588892979\n",
            "iteration 1810 :train_loss:0.3537048441980965 val_loss0.3316081808679851\n",
            "iteration 1811 :train_loss:0.35364677743694234 val_loss0.33334226063196054\n",
            "iteration 1812 :train_loss:0.35367535946293466 val_loss0.3315841752181239\n",
            "iteration 1813 :train_loss:0.35360745092512225 val_loss0.33331160258467446\n",
            "iteration 1814 :train_loss:0.35362898789017505 val_loss0.33154543703904266\n",
            "iteration 1815 :train_loss:0.35357316409882794 val_loss0.33328491313392156\n",
            "iteration 1816 :train_loss:0.3536039349840637 val_loss0.3315295563373638\n",
            "iteration 1817 :train_loss:0.35352566285708 val_loss0.3332470903457017\n",
            "iteration 1818 :train_loss:0.35352527093873876 val_loss0.33145748698624655\n",
            "iteration 1819 :train_loss:0.3534608862921039 val_loss0.33318632284296495\n",
            "iteration 1820 :train_loss:0.35346719594699744 val_loss0.3314094499952372\n",
            "iteration 1821 :train_loss:0.3533940535401403 val_loss0.33312871980701236\n",
            "iteration 1822 :train_loss:0.35341265988625786 val_loss0.3313639754231884\n",
            "iteration 1823 :train_loss:0.3533443261589348 val_loss0.3330871451352261\n",
            "iteration 1824 :train_loss:0.3533678074728058 val_loss0.33133136797399637\n",
            "iteration 1825 :train_loss:0.3532953164558283 val_loss0.3330476168821149\n",
            "iteration 1826 :train_loss:0.35331303400436764 val_loss0.3312853989761553\n",
            "iteration 1827 :train_loss:0.35326786927248516 val_loss0.33303251478829704\n",
            "iteration 1828 :train_loss:0.3532888125945872 val_loss0.3312722597727659\n",
            "iteration 1829 :train_loss:0.3532451675983284 val_loss0.3330238200769476\n",
            "iteration 1830 :train_loss:0.3532843852295617 val_loss0.33127926513813244\n",
            "iteration 1831 :train_loss:0.3532279959522295 val_loss0.3330204011944613\n",
            "iteration 1832 :train_loss:0.3532495110682989 val_loss0.33125738329559223\n",
            "iteration 1833 :train_loss:0.3531947175849783 val_loss0.3329999670816513\n",
            "iteration 1834 :train_loss:0.3532302079558757 val_loss0.3312481696732386\n",
            "iteration 1835 :train_loss:0.3532012898103226 val_loss0.3330201069656209\n",
            "iteration 1836 :train_loss:0.3532225526678871 val_loss0.3312532411305829\n",
            "iteration 1837 :train_loss:0.35319853463130285 val_loss0.3330309712506416\n",
            "iteration 1838 :train_loss:0.353216319571846 val_loss0.33125941547748433\n",
            "iteration 1839 :train_loss:0.35318814370830187 val_loss0.3330330413092117\n",
            "iteration 1840 :train_loss:0.3532202865823359 val_loss0.33127456800510435\n",
            "iteration 1841 :train_loss:0.3531908491360786 val_loss0.3330486895763184\n",
            "iteration 1842 :train_loss:0.35321184583226783 val_loss0.3312803813911059\n",
            "iteration 1843 :train_loss:0.3532031523191845 val_loss0.33307586237926456\n",
            "iteration 1844 :train_loss:0.3532302014697588 val_loss0.3313095704762284\n",
            "iteration 1845 :train_loss:0.3532145464505363 val_loss0.3330999508523393\n",
            "iteration 1846 :train_loss:0.35325014968257545 val_loss0.33134546770664325\n",
            "iteration 1847 :train_loss:0.3532377672664354 val_loss0.33313760155886896\n",
            "iteration 1848 :train_loss:0.353260088522687 val_loss0.3313642333638906\n",
            "iteration 1849 :train_loss:0.35324609082754826 val_loss0.3331596697136279\n",
            "iteration 1850 :train_loss:0.35325358400274054 val_loss0.3313726699552232\n",
            "iteration 1851 :train_loss:0.3532500541357266 val_loss0.33317719980428206\n",
            "iteration 1852 :train_loss:0.35326107493726905 val_loss0.3313915222454954\n",
            "iteration 1853 :train_loss:0.3532489259230952 val_loss0.3331882622982827\n",
            "iteration 1854 :train_loss:0.35324888455370057 val_loss0.33139180389320183\n",
            "iteration 1855 :train_loss:0.35322660857803406 val_loss0.3331782329518027\n",
            "iteration 1856 :train_loss:0.3532173522426462 val_loss0.3313727521968069\n",
            "iteration 1857 :train_loss:0.3531915742673353 val_loss0.33315307594058513\n",
            "iteration 1858 :train_loss:0.3531799554756404 val_loss0.3313491657297431\n",
            "iteration 1859 :train_loss:0.3531490965220382 val_loss0.333119546055955\n",
            "iteration 1860 :train_loss:0.35313849059428853 val_loss0.33131941663419573\n",
            "iteration 1861 :train_loss:0.3531072772642213 val_loss0.333090560903387\n",
            "iteration 1862 :train_loss:0.3530908757232231 val_loss0.3312879616791965\n",
            "iteration 1863 :train_loss:0.3530628075420249 val_loss0.33305716548412834\n",
            "iteration 1864 :train_loss:0.35304991812257314 val_loss0.3312594375240509\n",
            "iteration 1865 :train_loss:0.3530271068718221 val_loss0.333033464981512\n",
            "iteration 1866 :train_loss:0.3530246437622655 val_loss0.3312483249987397\n",
            "iteration 1867 :train_loss:0.35301235898429373 val_loss0.33303094603362604\n",
            "iteration 1868 :train_loss:0.35300528942899484 val_loss0.3312412589004922\n",
            "iteration 1869 :train_loss:0.3529768510400956 val_loss0.3330056019584143\n",
            "iteration 1870 :train_loss:0.35296058105635086 val_loss0.3312093074358714\n",
            "iteration 1871 :train_loss:0.35293161940657597 val_loss0.33296890214802755\n",
            "iteration 1872 :train_loss:0.35292235458476606 val_loss0.33118149843907824\n",
            "iteration 1873 :train_loss:0.3528936947716817 val_loss0.3329397634685146\n",
            "iteration 1874 :train_loss:0.3528772108428034 val_loss0.33114898235033763\n",
            "iteration 1875 :train_loss:0.35285234439121455 val_loss0.3329098662041212\n",
            "iteration 1876 :train_loss:0.352835513809056 val_loss0.33111972882703444\n",
            "iteration 1877 :train_loss:0.3528006971985911 val_loss0.33286474816850303\n",
            "iteration 1878 :train_loss:0.3527898508450385 val_loss0.3310866439654692\n",
            "iteration 1879 :train_loss:0.35276079729292154 val_loss0.3328361804712921\n",
            "iteration 1880 :train_loss:0.3527559543298109 val_loss0.33106566909376767\n",
            "iteration 1881 :train_loss:0.3527362133923188 val_loss0.33282387725047086\n",
            "iteration 1882 :train_loss:0.3527284918810422 val_loss0.33105053818716307\n",
            "iteration 1883 :train_loss:0.3527058511981661 val_loss0.33280354521164834\n",
            "iteration 1884 :train_loss:0.35269469177835305 val_loss0.3310271706374619\n",
            "iteration 1885 :train_loss:0.3526951793902226 val_loss0.3328063267250881\n",
            "iteration 1886 :train_loss:0.3526789168132665 val_loss0.3310244672598944\n",
            "iteration 1887 :train_loss:0.3526840713530164 val_loss0.3328081906004027\n",
            "iteration 1888 :train_loss:0.35267092575185965 val_loss0.33102656614098697\n",
            "iteration 1889 :train_loss:0.35268825594927466 val_loss0.3328235811983151\n",
            "iteration 1890 :train_loss:0.35268065324310416 val_loss0.33104880368782497\n",
            "iteration 1891 :train_loss:0.35269969106944465 val_loss0.3328498141221036\n",
            "iteration 1892 :train_loss:0.3526930667098083 val_loss0.33107182979445193\n",
            "iteration 1893 :train_loss:0.35272796441446674 val_loss0.3328917625373401\n",
            "iteration 1894 :train_loss:0.3527143308688877 val_loss0.33110158487411295\n",
            "iteration 1895 :train_loss:0.35273975927424656 val_loss0.33291565806574974\n",
            "iteration 1896 :train_loss:0.35273245688462557 val_loss0.33112938559850563\n",
            "iteration 1897 :train_loss:0.3527698971513699 val_loss0.33295894660130687\n",
            "iteration 1898 :train_loss:0.35274766166446914 val_loss0.3311550119999774\n",
            "iteration 1899 :train_loss:0.3527562278905242 val_loss0.33295574418407536\n",
            "iteration 1900 :train_loss:0.3527189984361674 val_loss0.3311360478566786\n",
            "iteration 1901 :train_loss:0.3527419844480612 val_loss0.33295125521341945\n",
            "iteration 1902 :train_loss:0.35271072569526263 val_loss0.3311386086411805\n",
            "iteration 1903 :train_loss:0.35274017922314593 val_loss0.33296118487029386\n",
            "iteration 1904 :train_loss:0.3526858937696253 val_loss0.3311253095625993\n",
            "iteration 1905 :train_loss:0.35269436132676846 val_loss0.3329246809775189\n",
            "iteration 1906 :train_loss:0.35264931372345726 val_loss0.3311001063247487\n",
            "iteration 1907 :train_loss:0.3526661205436264 val_loss0.33290692456992477\n",
            "iteration 1908 :train_loss:0.3526344559843562 val_loss0.331094712417412\n",
            "iteration 1909 :train_loss:0.3526479464290914 val_loss0.3328996513300872\n",
            "iteration 1910 :train_loss:0.3526168922723072 val_loss0.33108724496763214\n",
            "iteration 1911 :train_loss:0.35261316939279913 val_loss0.33287533797268637\n",
            "iteration 1912 :train_loss:0.3525798412346029 val_loss0.3310580624160233\n",
            "iteration 1913 :train_loss:0.35257711958467297 val_loss0.3328496924697053\n",
            "iteration 1914 :train_loss:0.3525387399221713 val_loss0.33102785602063256\n",
            "iteration 1915 :train_loss:0.35251981309613956 val_loss0.33280040726036514\n",
            "iteration 1916 :train_loss:0.3524713018868943 val_loss0.3309700976792518\n",
            "iteration 1917 :train_loss:0.3524616449066624 val_loss0.3327508040823439\n",
            "iteration 1918 :train_loss:0.3524132035991607 val_loss0.3309245981165516\n",
            "iteration 1919 :train_loss:0.35238286460241774 val_loss0.3326791906646099\n",
            "iteration 1920 :train_loss:0.35234238759176434 val_loss0.3308661525518559\n",
            "iteration 1921 :train_loss:0.35233219925529907 val_loss0.33263716578523594\n",
            "iteration 1922 :train_loss:0.35230875458130123 val_loss0.33084353095593794\n",
            "iteration 1923 :train_loss:0.35229861395138845 val_loss0.33261322130439563\n",
            "iteration 1924 :train_loss:0.3522652519433627 val_loss0.33081407288649667\n",
            "iteration 1925 :train_loss:0.3522642815797616 val_loss0.33259039713227\n",
            "iteration 1926 :train_loss:0.3522347553183556 val_loss0.3307959423742864\n",
            "iteration 1927 :train_loss:0.35223585534465623 val_loss0.3325738639117353\n",
            "iteration 1928 :train_loss:0.3522037350773202 val_loss0.33077830720946055\n",
            "iteration 1929 :train_loss:0.35222562660065154 val_loss0.3325777122953095\n",
            "iteration 1930 :train_loss:0.35219060561808657 val_loss0.3307757578090987\n",
            "iteration 1931 :train_loss:0.35219457211943406 val_loss0.33255727738346635\n",
            "iteration 1932 :train_loss:0.3521776273873415 val_loss0.33077386799803055\n",
            "iteration 1933 :train_loss:0.35219758471853696 val_loss0.33257329428923643\n",
            "iteration 1934 :train_loss:0.3521697268317034 val_loss0.3307769635554057\n",
            "iteration 1935 :train_loss:0.35218786483842673 val_loss0.33257440758987183\n",
            "iteration 1936 :train_loss:0.35216600039374657 val_loss0.33078353101519903\n",
            "iteration 1937 :train_loss:0.3521756148268438 val_loss0.33257489877815577\n",
            "iteration 1938 :train_loss:0.3521640202758844 val_loss0.3307898321950613\n",
            "iteration 1939 :train_loss:0.35218848856347795 val_loss0.332601667167245\n",
            "iteration 1940 :train_loss:0.3521649270254939 val_loss0.3307991171326549\n",
            "iteration 1941 :train_loss:0.3521835347055053 val_loss0.33260865423279967\n",
            "iteration 1942 :train_loss:0.35216696531102354 val_loss0.33081081136836005\n",
            "iteration 1943 :train_loss:0.3521993483130423 val_loss0.3326372045917757\n",
            "iteration 1944 :train_loss:0.35217447382298195 val_loss0.33082296637044906\n",
            "iteration 1945 :train_loss:0.3521983539989198 val_loss0.3326476997401692\n",
            "iteration 1946 :train_loss:0.35215875882880554 val_loss0.33081577848386035\n",
            "iteration 1947 :train_loss:0.35218145364182146 val_loss0.3326425814494421\n",
            "iteration 1948 :train_loss:0.35215279821867257 val_loss0.3308206376892097\n",
            "iteration 1949 :train_loss:0.35217664531774634 val_loss0.33264990813264694\n",
            "iteration 1950 :train_loss:0.35213851640535215 val_loss0.33081354811303965\n",
            "iteration 1951 :train_loss:0.3521711450124879 val_loss0.3326563427721986\n",
            "iteration 1952 :train_loss:0.352144072501945 val_loss0.33082898119465465\n",
            "iteration 1953 :train_loss:0.3521746354548646 val_loss0.3326720958204362\n",
            "iteration 1954 :train_loss:0.3521531378035555 val_loss0.33084462609297655\n",
            "iteration 1955 :train_loss:0.3521790483200138 val_loss0.332688029277828\n",
            "iteration 1956 :train_loss:0.3521513004322603 val_loss0.3308534457820256\n",
            "iteration 1957 :train_loss:0.3521624670619062 val_loss0.33268269292704244\n",
            "iteration 1958 :train_loss:0.35214256785786185 val_loss0.3308515361040778\n",
            "iteration 1959 :train_loss:0.35217543919221767 val_loss0.3327057044556064\n",
            "iteration 1960 :train_loss:0.3521441391057829 val_loss0.3308616075839328\n",
            "iteration 1961 :train_loss:0.35215700255931426 val_loss0.3327000032528589\n",
            "iteration 1962 :train_loss:0.35213621778679893 val_loss0.33086127669975135\n",
            "iteration 1963 :train_loss:0.35214739356250335 val_loss0.3327014286101705\n",
            "iteration 1964 :train_loss:0.3521225315578325 val_loss0.3308577829742526\n",
            "iteration 1965 :train_loss:0.35213794304934143 val_loss0.3327033048963815\n",
            "iteration 1966 :train_loss:0.35211051741048177 val_loss0.3308539661301879\n",
            "iteration 1967 :train_loss:0.35213561784061076 val_loss0.33271280757734983\n",
            "iteration 1968 :train_loss:0.3521075225487523 val_loss0.3308590958641513\n",
            "iteration 1969 :train_loss:0.35210416600182626 val_loss0.3326904117397356\n",
            "iteration 1970 :train_loss:0.3520703603309735 val_loss0.33083240664164376\n",
            "iteration 1971 :train_loss:0.3520733956752207 val_loss0.3326688799160662\n",
            "iteration 1972 :train_loss:0.3520340810191887 val_loss0.3308041206625371\n",
            "iteration 1973 :train_loss:0.35203229716354567 val_loss0.33263501494041503\n",
            "iteration 1974 :train_loss:0.35195439999481465 val_loss0.3307351495356805\n",
            "iteration 1975 :train_loss:0.3519401985692958 val_loss0.33254731114623887\n",
            "iteration 1976 :train_loss:0.35186274025791625 val_loss0.33065340198057774\n",
            "iteration 1977 :train_loss:0.35185682251355377 val_loss0.33246695559994216\n",
            "iteration 1978 :train_loss:0.3517675182706036 val_loss0.330564687677051\n",
            "iteration 1979 :train_loss:0.35175471108994855 val_loss0.33236956090015723\n",
            "iteration 1980 :train_loss:0.35166856963706494 val_loss0.33047332157461795\n",
            "iteration 1981 :train_loss:0.35165821102954625 val_loss0.3322759131045424\n",
            "iteration 1982 :train_loss:0.35157123364139975 val_loss0.33038217457350766\n",
            "iteration 1983 :train_loss:0.351578024611591 val_loss0.3322020788200735\n",
            "iteration 1984 :train_loss:0.35150908051458185 val_loss0.33032849586683816\n",
            "iteration 1985 :train_loss:0.3515071600126651 val_loss0.33213872084752816\n",
            "iteration 1986 :train_loss:0.35144119311889066 val_loss0.3302714683478736\n",
            "iteration 1987 :train_loss:0.3514743783836937 val_loss0.3321174142330762\n",
            "iteration 1988 :train_loss:0.35140982517543806 val_loss0.3302513329741777\n",
            "iteration 1989 :train_loss:0.3514675913329125 val_loss0.33212279908373227\n",
            "iteration 1990 :train_loss:0.3514174597712327 val_loss0.33027212532802125\n",
            "iteration 1991 :train_loss:0.3514777726224808 val_loss0.3321456581777802\n",
            "iteration 1992 :train_loss:0.3514341066381063 val_loss0.33030153992725564\n",
            "iteration 1993 :train_loss:0.35148957600847164 val_loss0.3321734752881653\n",
            "iteration 1994 :train_loss:0.3514631626151825 val_loss0.3303438298391011\n",
            "iteration 1995 :train_loss:0.3515461555979092 val_loss0.3322452768494063\n",
            "iteration 1996 :train_loss:0.35151439184512123 val_loss0.3304056285780092\n",
            "iteration 1997 :train_loss:0.3515818575149893 val_loss0.33229318835760474\n",
            "iteration 1998 :train_loss:0.3515552476933108 val_loss0.3304567370703574\n",
            "iteration 1999 :train_loss:0.35162974871496744 val_loss0.3323558052786837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qosaCbnJps_E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2a34cf58-d999-4269-dc03-a58cbaaba6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardless of the learning rate, number of iterations, or number of neurons, the loss for both lines stays around 5.0-6.0. Increasing the learning rate introduced NaN's, but decreasing it from 0.01 to 0.001 made both lines start to slightly increase near the end. Increasing the number of neurons from 50 to 128 made both lines start to decrease near the end. Increasing the number of iterations  to 3000 showed that the lines continue to decrease.  Increasing from 3000 to 5000 iterations showed a hint of flattening out. Validation loss was always lower than training loss. "
      ],
      "metadata": {
        "id": "dKgiUrLlUVeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Evaluation"
      ],
      "metadata": {
        "id": "G2e6V5FPN-UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to calculate the MAPE\n",
        "def MAPE(observedY, predictedY):\n",
        "    n=observedY.shape[1] \n",
        "    mape = (1/n) * (np.sum((abs(observedY - predictedY))/observedY))\n",
        "    return mape"
      ],
      "metadata": {
        "id": "o1sPXugtpuZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test = predict(parameters, test_norm)"
      ],
      "metadata": {
        "id": "aX9DPvgughaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7PZ7dcglsQb",
        "outputId": "e417d5e7-4bf3-419f-d626-e34a4b96c700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.94190115, 1.75359539, 2.75892627, ..., 0.98139326, 1.06674699,\n",
              "        4.88331738]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NN_MAPE = MAPE(test_Y, predicted_test)\n",
        "print(\"The MAPE of the NN model is: \", NN_MAPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrBoCLjRT3e4",
        "outputId": "e7ee29e2-fa82-4a0e-a238-cf0719fb855b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MAPE of the NN model is:  0.2265628201729831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transpose training data back\n",
        "train_norm = train_norm.T"
      ],
      "metadata": {
        "id": "0-kL0fljxdnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten target array back to 1D\n",
        "train_Y = train_Y.flatten()"
      ],
      "metadata": {
        "id": "g9kYRYYtxpmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWPGwZHXyVB4",
        "outputId": "e933e605-dfda-44fd-dab7-de6327598a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.355   2.332   1.25    ... 5.00001 0.725   1.778  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit linear regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "Linear_model=LinearRegression().fit(train_norm, train_Y)"
      ],
      "metadata": {
        "id": "9p324HR8x2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transpose test data back\n",
        "test_T = test_norm.T"
      ],
      "metadata": {
        "id": "F9bOXGZW1Fi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_test_predict = Linear_model.predict(test_T)"
      ],
      "metadata": {
        "id": "oU1S-rhJ0a_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_MAPE = MAPE(test_Y, LR_test_predict)\n",
        "print(\"The MAPE of the LR model is: \", LR_MAPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWFdTMH1XS_",
        "outputId": "c8caff0b-75bb-4d88-86fc-928b9d6e1495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MAPE of the LR model is:  0.29721536518246816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the LR model performed much better than the NN model because the MAPE was lower.#after fixing the backwards pass, the NN model performed better\n"
      ],
      "metadata": {
        "id": "jMXysdVS2AHM"
      }
    }
  ]
}